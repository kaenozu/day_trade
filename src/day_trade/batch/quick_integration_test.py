#!/usr/bin/env python3
"""
ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
Issue #376ã§å®Ÿè£…ã—ãŸã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬å‹•ä½œç¢ºèª
"""

import os

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ 
import sys
import tempfile
import time
from typing import Any, Dict

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from api_request_consolidator import RequestPriority, create_consolidator
from database_bulk_optimizer import OptimizationStrategy, create_bulk_optimizer
from parallel_batch_engine import ProcessingMode, TaskPriority, create_batch_engine


def test_api_request_consolidator() -> Dict[str, Any]:
    """APIãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    print("APIãƒªã‚¯ã‚¨ã‚¹ãƒˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹...")

    consolidator = create_consolidator(batch_size=20, max_workers=2, enable_caching=True)

    consolidator.start()

    try:
        symbols = ["7203", "8306", "9984"]
        responses_received = []

        def response_callback(response):
            responses_received.append(response)

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ•å…¥
        request_id = consolidator.submit_request(
            endpoint="test_endpoint",
            symbols=symbols,
            priority=RequestPriority.HIGH,
            callback=response_callback,
        )

        # å®Œäº†å¾…æ©Ÿ
        time.sleep(2.0)

        # çµ±è¨ˆå–å¾—
        stats = consolidator.get_stats()

        result = {
            "success": len(responses_received) > 0,
            "request_id": request_id,
            "responses_received": len(responses_received),
            "total_requests": stats.total_requests,
            "success_rate": stats.success_rate,
        }

        print(f"  âœ“ ãƒªã‚¯ã‚¨ã‚¹ãƒˆæŠ•å…¥æˆåŠŸ: {request_id}")
        print(f"  âœ“ ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡: {len(responses_received)}")
        print(f"  âœ“ çµ±è¨ˆ: requests={stats.total_requests}, success_rate={stats.success_rate:.1%}")

        return result

    finally:
        consolidator.stop()


def test_database_bulk_optimizer() -> Dict[str, Any]:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒ«ã‚¯æœ€é©åŒ–ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒ«ã‚¯æœ€é©åŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹...")

    # ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚DBãƒ•ã‚¡ã‚¤ãƒ«
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as tmp_file:
        test_db_path = tmp_file.name

    try:
        optimizer = create_bulk_optimizer(
            db_path=test_db_path,
            batch_size=100,
            max_workers=2,
            optimization_strategy=OptimizationStrategy.SPEED_OPTIMIZED,
        )

        # ãƒ†ã‚¹ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
        with optimizer.connection_pool.get_connection() as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS test_stocks (
                    symbol TEXT,
                    price REAL,
                    volume INTEGER,
                    PRIMARY KEY (symbol)
                )
            """
            )
            conn.commit()

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_data = [
            {"symbol": "7203", "price": 2500.0, "volume": 1000000},
            {"symbol": "8306", "price": 850.0, "volume": 2000000},
            {"symbol": "9984", "price": 8500.0, "volume": 500000},
        ]

        # ãƒãƒ«ã‚¯æŒ¿å…¥ãƒ†ã‚¹ãƒˆ
        insert_result = optimizer.bulk_insert(
            table_name="test_stocks", data=test_data, conflict_resolution="REPLACE"
        )

        # ãƒãƒ«ã‚¯UPSERTãƒ†ã‚¹ãƒˆ
        update_data = test_data.copy()
        for record in update_data:
            record["price"] *= 1.1

        upsert_result = optimizer.bulk_upsert(
            table_name="test_stocks", data=update_data, key_columns=["symbol"]
        )

        optimizer.close()

        result = {
            "success": insert_result.success_rate > 0.8 and upsert_result.success_rate > 0.8,
            "insert_processed": insert_result.processed_records,
            "insert_success_rate": insert_result.success_rate,
            "upsert_processed": upsert_result.processed_records,
            "upsert_success_rate": upsert_result.success_rate,
            "insert_throughput": insert_result.throughput_rps,
            "upsert_throughput": upsert_result.throughput_rps,
        }

        print(
            f"  âœ“ ãƒãƒ«ã‚¯æŒ¿å…¥: {insert_result.processed_records} records, {insert_result.success_rate:.1%} success"
        )
        print(
            f"  âœ“ ãƒãƒ«ã‚¯UPSERT: {upsert_result.processed_records} records, {upsert_result.success_rate:.1%} success"
        )
        print(
            f"  âœ“ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: insert={insert_result.throughput_rps:.1f} RPS, upsert={upsert_result.throughput_rps:.1f} RPS"
        )

        return result

    finally:
        try:
            os.unlink(test_db_path)
        except:
            pass


def test_parallel_batch_engine() -> Dict[str, Any]:
    """ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    print("ä¸¦åˆ—ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆé–‹å§‹...")

    def test_task(n: int, delay: float = 0.01) -> int:
        time.sleep(delay)
        return sum(i for i in range(n))

    def failing_task(should_fail: bool = True) -> str:
        if should_fail:
            raise ValueError("Test failure")
        return "Success"

    engine = create_batch_engine(
        workers=2,
        processing_mode=ProcessingMode.THREAD_BASED,
        enable_adaptive_scaling=False,
    )

    engine.start()

    try:
        task_ids = []

        # æˆåŠŸã‚¿ã‚¹ã‚¯æŠ•å…¥
        for i in range(5):
            task_id = engine.submit_task(
                test_task,
                50 + i * 10,
                delay=0.01,
                priority=TaskPriority.NORMAL,
                task_id=f"success_task_{i}",
            )
            task_ids.append(task_id)

        # å¤±æ•—ã‚¿ã‚¹ã‚¯æŠ•å…¥
        for i in range(2):
            task_id = engine.submit_task(
                failing_task,
                should_fail=(i == 0),  # æœ€åˆã®ã¯å¤±æ•—
                retry_count=1,
                task_id=f"fail_task_{i}",
            )
            task_ids.append(task_id)

        # å®Œäº†å¾…æ©Ÿ
        time.sleep(2.0)

        # çµæœå–å¾—
        results = engine.get_results_batch(task_ids, timeout=5.0)

        # çµ±è¨ˆå–å¾—
        stats = engine.get_stats()

        successful_tasks = [r for r in results.values() if r.success]
        failed_tasks = [r for r in results.values() if not r.success]

        result = {
            "success": len(successful_tasks) >= 5,  # æœ€ä½5ã¤ã®æˆåŠŸã‚¿ã‚¹ã‚¯ãŒå¿…è¦
            "total_tasks": len(results),
            "successful_tasks": len(successful_tasks),
            "failed_tasks": len(failed_tasks),
            "success_rate": stats.success_rate,
            "throughput_tps": stats.throughput_tps,
            "avg_execution_time": stats.average_execution_time,
        }

        print(f"  âœ“ ã‚¿ã‚¹ã‚¯å®Œäº†: {len(results)}/7 tasks")
        print(f"  âœ“ æˆåŠŸ: {len(successful_tasks)}, å¤±æ•—: {len(failed_tasks)}")
        print(
            f"  âœ“ ã‚¨ãƒ³ã‚¸ãƒ³çµ±è¨ˆ: success_rate={stats.success_rate:.1%}, throughput={stats.throughput_tps:.2f} TPS"
        )

        return result

    finally:
        engine.stop(timeout=5.0)


def run_quick_integration_test() -> Dict[str, Any]:
    """ã‚¯ã‚¤ãƒƒã‚¯çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("=== ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ ===\n")

    results = {}
    all_success = True

    # å„ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests = [
        ("api_request_consolidator", test_api_request_consolidator),
        ("database_bulk_optimizer", test_database_bulk_optimizer),
        ("parallel_batch_engine", test_parallel_batch_engine),
    ]

    for test_name, test_func in tests:
        try:
            test_result = test_func()
            results[test_name] = test_result

            if test_result["success"]:
                print(f"âœ“ {test_name} ãƒ†ã‚¹ãƒˆæˆåŠŸ\n")
            else:
                print(f"âœ— {test_name} ãƒ†ã‚¹ãƒˆå¤±æ•—\n")
                all_success = False

        except Exception as e:
            print(f"âœ— {test_name} ãƒ†ã‚¹ãƒˆä¾‹å¤–: {e}\n")
            results[test_name] = {"success": False, "error": str(e)}
            all_success = False

    # ç·åˆçµæœ
    total_tests = len(tests)
    successful_tests = len([r for r in results.values() if r.get("success", False)])

    print("=== çµ±åˆãƒ†ã‚¹ãƒˆçµæœ ===")
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    print(f"æˆåŠŸ: {successful_tests}")
    print(f"å¤±æ•—: {total_tests - successful_tests}")
    print(f"æˆåŠŸç‡: {successful_tests/total_tests:.1%}")
    print(f"å…¨ä½“æˆåŠŸ: {'âœ“' if all_success else 'âœ—'}")

    summary = {
        "overall_success": all_success,
        "total_tests": total_tests,
        "successful_tests": successful_tests,
        "success_rate": successful_tests / total_tests,
        "test_results": results,
        "timestamp": time.time(),
    }

    return summary


if __name__ == "__main__":
    summary = run_quick_integration_test()

    if summary["overall_success"]:
        print("\nğŸ‰ ã™ã¹ã¦ã®ãƒãƒƒãƒå‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
        exit(0)
    else:
        print("\nâš ï¸  ä¸€éƒ¨ã®ã‚·ã‚¹ãƒ†ãƒ ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚")
        exit(1)
