#!/usr/bin/env python3
"""
ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
Issue #366: é«˜é »åº¦å–å¼•æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ - è¶…é«˜ç²¾åº¦ç›£è¦–

ãƒŠãƒç§’ãƒ¬ãƒ™ãƒ«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼è¨ˆæ¸¬ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã€
ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã‚’å‚™ãˆãŸæ¬¡ä¸–ä»£HFTç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import os
import threading
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from enum import IntEnum
from typing import Any, Callable, Dict, List, Optional

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from ..cache.advanced_cache_system import AdvancedCacheSystem
    from ..distributed.distributed_computing_manager import DistributedComputingManager
    from ..utils.logging_config import get_context_logger, log_performance_metric
except ImportError:
    import logging

    logging.basicConfig(level=logging.INFO)

    def get_context_logger(name):
        return logging.getLogger(name)

    def log_performance_metric(*args, **kwargs):
        pass

    # ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    class DistributedComputingManager:
        def __init__(self):
            pass

        async def execute_distributed_task(self, task):
            return type("MockResult", (), {"success": True, "result": None})()

    class AdvancedCacheSystem:
        def __init__(self):
            pass


logger = get_context_logger(__name__)


class MetricType(IntEnum):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚¿ã‚¤ãƒ—"""

    LATENCY = 1
    THROUGHPUT = 2
    ERROR_RATE = 3
    QUEUE_DEPTH = 4
    CPU_USAGE = 5
    MEMORY_USAGE = 6
    NETWORK_IO = 7


class AlertSeverity(IntEnum):
    """ã‚¢ãƒ©ãƒ¼ãƒˆé‡è¦åº¦"""

    INFO = 0
    WARNING = 1
    ERROR = 2
    CRITICAL = 3


@dataclass
class LatencyMetrics:
    """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®šå€¤"""

    # åŸºæœ¬çµ±è¨ˆ
    count: int = 0
    sum_us: float = 0.0
    min_us: float = float("inf")
    max_us: float = 0.0

    # é«˜ç²¾åº¦çµ±è¨ˆ (P50, P95, P99, P99.9)
    p50_us: float = 0.0
    p95_us: float = 0.0
    p99_us: float = 0.0
    p999_us: float = 0.0

    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€æ–°Nå€‹ï¼‰
    recent_values: deque = field(default_factory=lambda: deque(maxlen=1000))

    # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
    histogram_buckets: Dict[int, int] = field(default_factory=dict)

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    last_update_ns: int = field(default_factory=time.perf_counter_ns)

    def update(self, latency_us: float):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ›´æ–°"""
        self.count += 1
        self.sum_us += latency_us
        self.min_us = min(self.min_us, latency_us)
        self.max_us = max(self.max_us, latency_us)

        self.recent_values.append(latency_us)
        self.last_update_ns = time.perf_counter_ns()

        # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ æ›´æ–°
        bucket = int(latency_us // 10) * 10  # 10Î¼sãƒã‚±ãƒƒãƒˆ
        self.histogram_buckets[bucket] = self.histogram_buckets.get(bucket, 0) + 1

    def calculate_percentiles(self):
        """ç™¾åˆ†ä½æ•°è¨ˆç®—"""
        if not self.recent_values:
            return

        values = sorted(self.recent_values)
        n = len(values)

        if n > 0:
            self.p50_us = values[int(n * 0.5)]
            self.p95_us = values[int(n * 0.95)]
            self.p99_us = values[int(n * 0.99)]
            self.p999_us = values[int(n * 0.999)] if n >= 1000 else values[-1]

    def get_average_us(self) -> float:
        """å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼å–å¾—"""
        return self.sum_us / self.count if self.count > 0 else 0.0

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "count": self.count,
            "avg_us": self.get_average_us(),
            "min_us": self.min_us if self.min_us != float("inf") else 0.0,
            "max_us": self.max_us,
            "p50_us": self.p50_us,
            "p95_us": self.p95_us,
            "p99_us": self.p99_us,
            "p999_us": self.p999_us,
            "last_update_ns": self.last_update_ns,
        }


@dataclass
class ThroughputMetrics:
    """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šå€¤"""

    total_operations: int = 0
    operations_per_second: float = 0.0
    peak_ops_per_second: float = 0.0

    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
    recent_ops: deque = field(default_factory=lambda: deque(maxlen=60))  # 60ç§’åˆ†
    recent_timestamps: deque = field(default_factory=lambda: deque(maxlen=60))

    last_update_ns: int = field(default_factory=time.perf_counter_ns)

    def update(self, operations: int = 1):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ›´æ–°"""
        now_ns = time.perf_counter_ns()

        self.total_operations += operations
        self.recent_ops.append(operations)
        self.recent_timestamps.append(now_ns)
        self.last_update_ns = now_ns

        # 1ç§’é–“ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        self._calculate_throughput()

    def _calculate_throughput(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—"""
        if len(self.recent_timestamps) < 2:
            return

        # ç›´è¿‘1ç§’ã®ãƒ‡ãƒ¼ã‚¿ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
        now_ns = self.recent_timestamps[-1]
        one_second_ago_ns = now_ns - 1_000_000_000

        ops_in_last_second = 0
        for i in range(len(self.recent_timestamps) - 1, -1, -1):
            if self.recent_timestamps[i] >= one_second_ago_ns:
                ops_in_last_second += self.recent_ops[i]
            else:
                break

        self.operations_per_second = ops_in_last_second
        self.peak_ops_per_second = max(self.peak_ops_per_second, ops_in_last_second)

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "total_operations": self.total_operations,
            "operations_per_second": self.operations_per_second,
            "peak_ops_per_second": self.peak_ops_per_second,
            "last_update_ns": self.last_update_ns,
        }


@dataclass
class SystemMetrics:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    cpu_usage_percent: float = 0.0
    memory_usage_mb: float = 0.0
    memory_usage_percent: float = 0.0

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯I/O
    network_bytes_sent: int = 0
    network_bytes_received: int = 0

    # ãƒ‡ã‚£ã‚¹ã‚¯I/O
    disk_read_bytes: int = 0
    disk_write_bytes: int = 0

    # ãƒ—ãƒ­ã‚»ã‚¹å›ºæœ‰
    process_threads: int = 0
    process_handles: int = 0

    last_update_ns: int = field(default_factory=time.perf_counter_ns)

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "cpu_usage_percent": self.cpu_usage_percent,
            "memory_usage_mb": self.memory_usage_mb,
            "memory_usage_percent": self.memory_usage_percent,
            "network_bytes_sent": self.network_bytes_sent,
            "network_bytes_received": self.network_bytes_received,
            "process_threads": self.process_threads,
            "process_handles": self.process_handles,
            "last_update_ns": self.last_update_ns,
        }


@dataclass
class AlertCondition:
    """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶"""

    metric_name: str
    threshold_value: float
    comparison: str  # 'gt', 'lt', 'eq'
    severity: AlertSeverity = AlertSeverity.WARNING
    enabled: bool = True

    # ãƒ’ã‚¹ãƒ†ãƒªã‚·ã‚¹ï¼ˆãƒãƒ£ã‚¿ãƒªãƒ³ã‚°é˜²æ­¢ï¼‰
    hysteresis_percent: float = 10.0
    consecutive_violations: int = 0
    required_violations: int = 3

    # é€šçŸ¥åˆ¶é™
    last_alert_time: float = 0.0
    alert_cooldown_seconds: float = 60.0


@dataclass
class AlertEvent:
    """ã‚¢ãƒ©ãƒ¼ãƒˆã‚¤ãƒ™ãƒ³ãƒˆ"""

    alert_id: str
    metric_name: str
    current_value: float
    threshold_value: float
    severity: AlertSeverity
    message: str
    timestamp_ns: int = field(default_factory=time.perf_counter_ns)

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "alert_id": self.alert_id,
            "metric_name": self.metric_name,
            "current_value": self.current_value,
            "threshold_value": self.threshold_value,
            "severity": self.severity.name,
            "message": self.message,
            "timestamp_ns": self.timestamp_ns,
        }


class HighResolutionTimer:
    """é«˜è§£åƒåº¦ã‚¿ã‚¤ãƒãƒ¼ï¼ˆãƒŠãƒç§’ç²¾åº¦ï¼‰"""

    def __init__(self):
        self.start_times = {}
        self.cpu_frequency = self._estimate_cpu_frequency()

    def _estimate_cpu_frequency(self) -> float:
        """CPUå‘¨æ³¢æ•°æ¨å®š"""
        try:
            if os.name == "posix":
                with open("/proc/cpuinfo") as f:
                    for line in f:
                        if "cpu MHz" in line:
                            return float(line.split(":")[1].strip()) * 1e6
        except:
            pass
        return 3.0e9  # 3GHz fallback

    def start_timing(self, operation_id: str) -> int:
        """ã‚¿ã‚¤ãƒŸãƒ³ã‚°é–‹å§‹"""
        start_time_ns = time.perf_counter_ns()
        self.start_times[operation_id] = start_time_ns
        return start_time_ns

    def end_timing(self, operation_id: str) -> Optional[float]:
        """ã‚¿ã‚¤ãƒŸãƒ³ã‚°çµ‚äº†ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ã§è¿”ã™ï¼‰"""
        end_time_ns = time.perf_counter_ns()
        start_time_ns = self.start_times.pop(operation_id, None)

        if start_time_ns is None:
            return None

        return (end_time_ns - start_time_ns) / 1000.0

    def time_operation(self, operation_id: str):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

        class TimingContext:
            def __init__(self, timer, op_id):
                self.timer = timer
                self.operation_id = op_id
                self.duration_us = None

            def __enter__(self):
                self.timer.start_timing(self.operation_id)
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                self.duration_us = self.timer.end_timing(self.operation_id)

        return TimingContext(self, operation_id)


@dataclass
class PerformanceReport:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ"""

    report_id: str
    generation_time_ns: int = field(default_factory=time.perf_counter_ns)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†ç´„
    latency_metrics: Dict[str, LatencyMetrics] = field(default_factory=dict)
    throughput_metrics: Dict[str, ThroughputMetrics] = field(default_factory=dict)
    system_metrics: SystemMetrics = field(default_factory=SystemMetrics)

    # ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ³
    active_alerts: List[AlertEvent] = field(default_factory=list)
    resolved_alerts: List[AlertEvent] = field(default_factory=list)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„
    overall_health_score: float = 100.0  # 0-100
    performance_summary: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã§å–å¾—"""
        return {
            "report_id": self.report_id,
            "generation_time_ns": self.generation_time_ns,
            "latency_metrics": {
                name: metrics.to_dict() for name, metrics in self.latency_metrics.items()
            },
            "throughput_metrics": {
                name: metrics.to_dict() for name, metrics in self.throughput_metrics.items()
            },
            "system_metrics": self.system_metrics.to_dict(),
            "active_alerts": [alert.to_dict() for alert in self.active_alerts],
            "resolved_alerts": [alert.to_dict() for alert in self.resolved_alerts],
            "overall_health_score": self.overall_health_score,
            "performance_summary": self.performance_summary,
        }


class MicrosecondMonitor:
    """
    ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

    ãƒŠãƒç§’ãƒ¬ãƒ™ãƒ«è¨ˆæ¸¬ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆåˆ†æã€
    ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½ã‚’æä¾›
    """

    def __init__(
        self,
        distributed_manager: Optional[DistributedComputingManager] = None,
        cache_system: Optional[AdvancedCacheSystem] = None,
        monitoring_interval_ms: int = 100,
        history_retention_hours: int = 24,
    ):
        """
        åˆæœŸåŒ–

        Args:
            distributed_manager: åˆ†æ•£å‡¦ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
            cache_system: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
            monitoring_interval_ms: ç›£è¦–é–“éš”ï¼ˆãƒŸãƒªç§’ï¼‰
            history_retention_hours: å±¥æ­´ä¿æŒæ™‚é–“
        """
        self.distributed_manager = distributed_manager or DistributedComputingManager()
        self.cache_system = cache_system or AdvancedCacheSystem()
        self.monitoring_interval_ms = monitoring_interval_ms
        self.history_retention_hours = history_retention_hours

        # é«˜ç²¾åº¦ã‚¿ã‚¤ãƒãƒ¼
        self.timer = HighResolutionTimer()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†ç´„
        self.latency_metrics: Dict[str, LatencyMetrics] = {}
        self.throughput_metrics: Dict[str, ThroughputMetrics] = {}
        self.system_metrics = SystemMetrics()

        # ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†
        self.alert_conditions: Dict[str, AlertCondition] = {}
        self.active_alerts: Dict[str, AlertEvent] = {}
        self.alert_history: deque = deque(maxlen=1000)
        self.alert_callbacks: List[Callable[[AlertEvent], None]] = []

        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰
        self.monitoring_thread = None
        self.running = False

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆå±¥æ­´
        self.report_history: deque = deque(maxlen=100)

        # çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.stats_cache = {}
        self.last_stats_update = 0

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶è¨­å®š
        self._setup_default_alerts()

        logger.info("MicrosecondMonitoråˆæœŸåŒ–å®Œäº†")

    def _setup_default_alerts(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶è¨­å®š"""
        # å®Ÿè¡Œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã‚¢ãƒ©ãƒ¼ãƒˆ
        self.add_alert_condition(
            AlertCondition(
                metric_name="execution_latency_avg",
                threshold_value=100.0,  # 100Î¼s
                comparison="gt",
                severity=AlertSeverity.WARNING,
                required_violations=5,
            )
        )

        self.add_alert_condition(
            AlertCondition(
                metric_name="execution_latency_p99",
                threshold_value=500.0,  # 500Î¼s
                comparison="gt",
                severity=AlertSeverity.ERROR,
                required_violations=3,
            )
        )

        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚¢ãƒ©ãƒ¼ãƒˆ
        self.add_alert_condition(
            AlertCondition(
                metric_name="execution_throughput",
                threshold_value=1000.0,  # 1000 ops/sec
                comparison="lt",
                severity=AlertSeverity.WARNING,
                required_violations=10,
            )
        )

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆ
        self.add_alert_condition(
            AlertCondition(
                metric_name="cpu_usage",
                threshold_value=90.0,  # 90%
                comparison="gt",
                severity=AlertSeverity.WARNING,
                required_violations=5,
            )
        )

        self.add_alert_condition(
            AlertCondition(
                metric_name="memory_usage",
                threshold_value=85.0,  # 85%
                comparison="gt",
                severity=AlertSeverity.ERROR,
                required_violations=3,
            )
        )

    def record_latency(self, metric_name: str, latency_us: float):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼è¨˜éŒ²"""
        if metric_name not in self.latency_metrics:
            self.latency_metrics[metric_name] = LatencyMetrics()

        self.latency_metrics[metric_name].update(latency_us)

        # ç™¾åˆ†ä½æ•°æ›´æ–°ï¼ˆé©åº¦ãªé »åº¦ã§ï¼‰
        if self.latency_metrics[metric_name].count % 100 == 0:
            self.latency_metrics[metric_name].calculate_percentiles()

        # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯
        self._check_metric_alerts(
            f"{metric_name}_avg", self.latency_metrics[metric_name].get_average_us()
        )
        self._check_metric_alerts(f"{metric_name}_max", latency_us)

    def record_throughput(self, metric_name: str, operations: int = 1):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨˜éŒ²"""
        if metric_name not in self.throughput_metrics:
            self.throughput_metrics[metric_name] = ThroughputMetrics()

        self.throughput_metrics[metric_name].update(operations)

        # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯
        self._check_metric_alerts(
            f"{metric_name}_throughput",
            self.throughput_metrics[metric_name].operations_per_second,
        )

    def time_operation(self, operation_name: str):
        """æ“ä½œæ™‚é–“æ¸¬å®šï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼‰"""

        class MonitoringContext:
            def __init__(self, monitor, op_name):
                self.monitor = monitor
                self.operation_name = op_name
                self.start_time_ns = None

            def __enter__(self):
                self.start_time_ns = time.perf_counter_ns()
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                end_time_ns = time.perf_counter_ns()
                duration_us = (end_time_ns - self.start_time_ns) / 1000.0

                self.monitor.record_latency(self.operation_name, duration_us)
                self.monitor.record_throughput(self.operation_name, 1)

        return MonitoringContext(self, operation_name)

    def add_alert_condition(self, condition: AlertCondition):
        """ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶è¿½åŠ """
        alert_id = f"{condition.metric_name}_{condition.comparison}_{condition.threshold_value}"
        self.alert_conditions[alert_id] = condition
        logger.debug(f"ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶è¿½åŠ : {alert_id}")

    def add_alert_callback(self, callback: Callable[[AlertEvent], None]):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ """
        self.alert_callbacks.append(callback)

    def _check_metric_alerts(self, metric_name: str, current_value: float):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯"""
        for alert_id, condition in self.alert_conditions.items():
            if not condition.enabled or condition.metric_name != metric_name:
                continue

            # é–¾å€¤ãƒã‚§ãƒƒã‚¯
            violation = False
            if condition.comparison == "gt" and current_value > condition.threshold_value:
                violation = True
            elif condition.comparison == "lt" and current_value < condition.threshold_value:
                violation = True
            elif (
                condition.comparison == "eq"
                and abs(current_value - condition.threshold_value) < 0.001
            ):
                violation = True

            if violation:
                condition.consecutive_violations += 1

                # é€£ç¶šé•åå›æ•°ãƒã‚§ãƒƒã‚¯
                if condition.consecutive_violations >= condition.required_violations:
                    self._trigger_alert(alert_id, condition, current_value)
            else:
                # é•åè§£æ±º
                if condition.consecutive_violations > 0:
                    condition.consecutive_violations = 0
                    self._resolve_alert(alert_id)

    def _trigger_alert(self, alert_id: str, condition: AlertCondition, current_value: float):
        """ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«"""
        now = time.time()

        # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒã‚§ãƒƒã‚¯
        if now - condition.last_alert_time < condition.alert_cooldown_seconds:
            return

        # ã‚¢ãƒ©ãƒ¼ãƒˆã‚¤ãƒ™ãƒ³ãƒˆä½œæˆ
        alert_event = AlertEvent(
            alert_id=alert_id,
            metric_name=condition.metric_name,
            current_value=current_value,
            threshold_value=condition.threshold_value,
            severity=condition.severity,
            message=f"{condition.metric_name} {condition.comparison} {condition.threshold_value}: current={current_value:.3f}",
        )

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆã«è¿½åŠ 
        self.active_alerts[alert_id] = alert_event
        self.alert_history.append(alert_event)

        condition.last_alert_time = now

        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
        for callback in self.alert_callbacks:
            try:
                callback(alert_event)
            except Exception as e:
                logger.error(f"ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

        logger.warning(f"ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç«: {alert_event.message}")

    def _resolve_alert(self, alert_id: str):
        """ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±º"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts.pop(alert_id)
            logger.info(f"ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±º: {alert.metric_name}")

    def _update_system_metrics(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""
        try:
            # CPUä½¿ç”¨ç‡ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            import psutil

            self.system_metrics.cpu_usage_percent = psutil.cpu_percent()

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            self.system_metrics.memory_usage_percent = memory.percent
            self.system_metrics.memory_usage_mb = memory.used / (1024 * 1024)

            # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
            process = psutil.Process()
            self.system_metrics.process_threads = process.num_threads()

        except ImportError:
            # psutilãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç°¡æ˜“å®Ÿè£…
            try:
                import resource

                usage = resource.getrusage(resource.RUSAGE_SELF)
                self.system_metrics.memory_usage_mb = usage.ru_maxrss / 1024  # KB to MB
            except:
                pass
        except Exception as e:
            logger.debug(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

        self.system_metrics.last_update_ns = time.perf_counter_ns()

        # ã‚¢ãƒ©ãƒ¼ãƒˆæ¡ä»¶ãƒã‚§ãƒƒã‚¯
        self._check_metric_alerts("cpu_usage", self.system_metrics.cpu_usage_percent)
        self._check_metric_alerts("memory_usage", self.system_metrics.memory_usage_percent)

    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if self.running:
            return

        self.running = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop, name="MicrosecondMonitor", daemon=True
        )
        self.monitoring_thread.start()
        logger.info("ãƒã‚¤ã‚¯ãƒ­ç§’ç›£è¦–é–‹å§‹")

    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.running = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5.0)
        logger.info("ãƒã‚¤ã‚¯ãƒ­ç§’ç›£è¦–åœæ­¢")

    def _monitoring_loop(self):
        """ç›£è¦–ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        logger.info("ãƒã‚¤ã‚¯ãƒ­ç§’ç›£è¦–ãƒ«ãƒ¼ãƒ—é–‹å§‹")

        while self.running:
            try:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
                self._update_system_metrics()

                # ç™¾åˆ†ä½æ•°å®šæœŸè¨ˆç®—
                for metrics in self.latency_metrics.values():
                    if metrics.count > 0 and len(metrics.recent_values) >= 10:
                        metrics.calculate_percentiles()

                # çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                self._update_stats_cache()

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå®šæœŸçš„ï¼‰
                if (
                    len(self.report_history) == 0
                    or time.perf_counter_ns() - self.report_history[-1].generation_time_ns
                    > 60_000_000_000
                ):  # 60ç§’
                    self._generate_performance_report()

                # ç›£è¦–é–“éš”å¾…æ©Ÿ
                time.sleep(self.monitoring_interval_ms / 1000.0)

            except Exception as e:
                logger.error(f"ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_stats_cache(self):
        """çµ±è¨ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°"""
        now_ns = time.perf_counter_ns()
        if now_ns - self.last_stats_update < 1_000_000_000:  # 1ç§’é–“éš”
            return

        self.stats_cache = {
            "latency_stats": {
                name: metrics.to_dict() for name, metrics in self.latency_metrics.items()
            },
            "throughput_stats": {
                name: metrics.to_dict() for name, metrics in self.throughput_metrics.items()
            },
            "system_stats": self.system_metrics.to_dict(),
            "active_alerts_count": len(self.active_alerts),
            "cache_update_time_ns": now_ns,
        }

        self.last_stats_update = now_ns

    def _generate_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_id = f"report_{int(time.time())}"

        report = PerformanceReport(
            report_id=report_id,
            latency_metrics=self.latency_metrics.copy(),
            throughput_metrics=self.throughput_metrics.copy(),
            system_metrics=self.system_metrics,
            active_alerts=list(self.active_alerts.values()),
            resolved_alerts=[],  # æœ€è¿‘è§£æ±ºã•ã‚ŒãŸã‚¢ãƒ©ãƒ¼ãƒˆ
        )

        # å¥å…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        report.overall_health_score = self._calculate_health_score()

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„
        report.performance_summary = self._create_performance_summary()

        self.report_history.append(report)

        logger.info(
            f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_id}, ãƒ˜ãƒ«ã‚¹ã‚¹ã‚³ã‚¢: {report.overall_health_score:.1f}"
        )

    def _calculate_health_score(self) -> float:
        """å¥å…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰"""
        score = 100.0

        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆæ•°ã«ã‚ˆã‚‹æ¸›ç‚¹
        score -= len(self.active_alerts) * 5.0

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ç›®æ¨™é”æˆåº¦
        for name, metrics in self.latency_metrics.items():
            if metrics.count > 0:
                avg_latency = metrics.get_average_us()
                if avg_latency > 100:  # 100Î¼sç›®æ¨™
                    score -= min(20.0, (avg_latency - 100) / 10)

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡
        if self.system_metrics.cpu_usage_percent > 80:
            score -= (self.system_metrics.cpu_usage_percent - 80) * 0.5

        if self.system_metrics.memory_usage_percent > 80:
            score -= (self.system_metrics.memory_usage_percent - 80) * 0.5

        return max(0.0, min(100.0, score))

    def _create_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ä½œæˆ"""
        summary = {}

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼è¦ç´„
        if self.latency_metrics:
            total_ops = sum(m.count for m in self.latency_metrics.values())
            avg_latency = (
                sum(m.sum_us for m in self.latency_metrics.values()) / total_ops
                if total_ops > 0
                else 0
            )

            summary["latency_summary"] = {
                "total_operations": total_ops,
                "average_latency_us": avg_latency,
                "sub50us_operations": sum(
                    sum(1 for v in m.recent_values if v < 50) for m in self.latency_metrics.values()
                ),
            }

        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¦ç´„
        if self.throughput_metrics:
            total_throughput = sum(
                m.operations_per_second for m in self.throughput_metrics.values()
            )
            peak_throughput = max(m.peak_ops_per_second for m in self.throughput_metrics.values())

            summary["throughput_summary"] = {
                "current_total_ops_per_sec": total_throughput,
                "peak_total_ops_per_sec": peak_throughput,
            }

        return summary

    def get_current_stats(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®çµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.stats_cache.copy() if self.stats_cache else {}

    def get_latest_report(self) -> Optional[PerformanceReport]:
        """æœ€æ–°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""
        return self.report_history[-1] if self.report_history else None

    def get_alert_summary(self) -> Dict[str, Any]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆè¦ç´„å–å¾—"""
        severity_counts = defaultdict(int)
        for alert in self.active_alerts.values():
            severity_counts[alert.severity.name] += 1

        return {
            "active_alerts_count": len(self.active_alerts),
            "severity_breakdown": dict(severity_counts),
            "recent_alerts": [alert.to_dict() for alert in list(self.alert_history)[-10:]],
        }

    def reset_metrics(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ"""
        self.latency_metrics.clear()
        self.throughput_metrics.clear()
        self.active_alerts.clear()
        self.alert_history.clear()
        logger.info("ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆå®Œäº†")

    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("MicrosecondMonitor ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")

        self.stop_monitoring()

        # ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¯ãƒªã‚¢
        self.alert_callbacks.clear()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¯ãƒªã‚¢
        self.reset_metrics()

        logger.info("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")


# Factory function
def create_microsecond_monitor(
    distributed_manager: Optional[DistributedComputingManager] = None,
    cache_system: Optional[AdvancedCacheSystem] = None,
    **config,
) -> MicrosecondMonitor:
    """MicrosecondMonitorãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    return MicrosecondMonitor(
        distributed_manager=distributed_manager, cache_system=cache_system, **config
    )


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async def main():
        print("=== Issue #366 ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆ ===")

        monitor = None
        try:
            # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            monitor = create_microsecond_monitor(monitoring_interval_ms=50)  # 50msé–“éš”

            # ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
            def alert_handler(alert: AlertEvent):
                print(f"ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ: {alert.message} (é‡è¦åº¦: {alert.severity.name})")

            monitor.add_alert_callback(alert_handler)

            # ç›£è¦–é–‹å§‹
            monitor.start_monitoring()

            print("\n1. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®šãƒ†ã‚¹ãƒˆ")
            # å„ç¨®æ“ä½œã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®š
            test_operations = [
                ("order_validation", lambda: time.sleep(0.000005)),  # 5Î¼s
                ("order_execution", lambda: time.sleep(0.000050)),  # 50Î¼s
                ("market_data_processing", lambda: time.sleep(0.000010)),  # 10Î¼s
            ]

            for op_name, operation in test_operations:
                for i in range(100):
                    with monitor.time_operation(op_name):
                        operation()

                    # ä¸€éƒ¨ã§æ„å›³çš„ã«é«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼
                    if i % 20 == 19:
                        with monitor.time_operation(op_name):
                            time.sleep(0.000200)  # 200Î¼s (é«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼)

            # çµ±è¨ˆç¢ºèªã®ãŸã‚å°‘ã—å¾…æ©Ÿ
            await asyncio.sleep(2)

            print("\n2. ç¾åœ¨ã®çµ±è¨ˆæƒ…å ±")
            stats = monitor.get_current_stats()
            if stats.get("latency_stats"):
                for metric_name, data in stats["latency_stats"].items():
                    print(f"  {metric_name}:")
                    print(f"    å¹³å‡: {data['avg_us']:.1f}Î¼s")
                    print(f"    P95: {data['p95_us']:.1f}Î¼s")
                    print(f"    P99: {data['p99_us']:.1f}Î¼s")
                    print(f"    å‡¦ç†å›æ•°: {data['count']}")

            print("\n3. ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆçµ±è¨ˆ")
            if stats.get("throughput_stats"):
                for metric_name, data in stats["throughput_stats"].items():
                    print(f"  {metric_name}: {data['operations_per_second']:.1f} ops/sec")
                    print(f"    ãƒ”ãƒ¼ã‚¯: {data['peak_ops_per_second']:.1f} ops/sec")

            print("\n4. ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
            sys_stats = stats.get("system_stats", {})
            print(f"  CPUä½¿ç”¨ç‡: {sys_stats.get('cpu_usage_percent', 0):.1f}%")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {sys_stats.get('memory_usage_percent', 0):.1f}%")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {sys_stats.get('memory_usage_mb', 0):.1f}MB")

            print("\n5. ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ³")
            alert_summary = monitor.get_alert_summary()
            print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆ: {alert_summary['active_alerts_count']}")
            if alert_summary["severity_breakdown"]:
                print("  é‡è¦åº¦åˆ¥:")
                for severity, count in alert_summary["severity_breakdown"].items():
                    print(f"    {severity}: {count}")

            print("\n6. æœ€æ–°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ")
            latest_report = monitor.get_latest_report()
            if latest_report:
                print(f"  ãƒ¬ãƒãƒ¼ãƒˆID: {latest_report.report_id}")
                print(f"  å¥å…¨æ€§ã‚¹ã‚³ã‚¢: {latest_report.overall_health_score:.1f}/100")

                if latest_report.performance_summary:
                    summary = latest_report.performance_summary
                    if "latency_summary" in summary:
                        lat_sum = summary["latency_summary"]
                        print(f"  ç·å‡¦ç†æ•°: {lat_sum['total_operations']}")
                        print(f"  å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {lat_sum['average_latency_us']:.1f}Î¼s")
                        print(f"  <50Î¼så‡¦ç†: {lat_sum['sub50us_operations']}")

            # é«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã§ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿãƒ†ã‚¹ãƒˆ
            print("\n7. ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿãƒ†ã‚¹ãƒˆï¼ˆé«˜ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ï¼‰")
            for i in range(10):
                with monitor.time_operation("test_high_latency"):
                    time.sleep(0.000150)  # 150Î¼sï¼ˆé–¾å€¤è¶…éï¼‰

            await asyncio.sleep(1)  # ã‚¢ãƒ©ãƒ¼ãƒˆå‡¦ç†å¾…æ©Ÿ

            print("\n8. æœ€çµ‚çµ±è¨ˆç¢ºèª")
            final_stats = monitor.get_current_stats()
            final_alert_summary = monitor.get_alert_summary()
            print(f"  æœ€çµ‚ã‚¢ãƒ©ãƒ¼ãƒˆæ•°: {final_alert_summary['active_alerts_count']}")

        except Exception as e:
            print(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

        finally:
            if monitor:
                await monitor.cleanup()

        print("\n=== ãƒã‚¤ã‚¯ãƒ­ç§’ç²¾åº¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆå®Œäº† ===")

    asyncio.run(main())
