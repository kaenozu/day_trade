#!/usr/bin/env python3
"""
HFTçµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ€§èƒ½æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
Issue #366: é«˜é »åº¦å–å¼•æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ - åŒ…æ‹¬çš„æ€§èƒ½æ¤œè¨¼

å…¨HFTã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€
æ€§èƒ½ç›®æ¨™é”æˆç¢ºèªã‚’è¡Œã†åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
"""

import asyncio
import json
import statistics
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from ..utils.logging_config import get_context_logger
    from .hft_orchestrator import (
        HFTConfig,
        HFTMode,
        HFTOrchestrator,
        HFTStrategy,
        create_hft_orchestrator,
    )
    from .market_data_processor import MarketUpdate, MessageType, OrderBookSide
    from .microsecond_monitor import AlertEvent
    from .ultra_fast_executor import OrderEntry, OrderSide, OrderType
except ImportError:
    import logging

    logging.basicConfig(level=logging.INFO)

    def get_context_logger(name):
        return logging.getLogger(name)

    # åŸºæœ¬çš„ãªãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    class HFTOrchestrator:
        def __init__(self, config):
            self.config = config

        async def initialize_system(self):
            return True

        async def start_trading(self):
            pass

        async def stop_trading(self):
            pass

        def add_strategy(self, strategy):
            pass

        def get_system_status(self):
            return {}

        def get_detailed_performance(self):
            return {}

        async def cleanup(self):
            pass

    class HFTConfig:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)

    class HFTStrategy:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)

    def create_hft_orchestrator(config):
        return HFTOrchestrator(config)


logger = get_context_logger(__name__)


@dataclass
class PerformanceTarget:
    """æ€§èƒ½ç›®æ¨™å®šç¾©"""

    name: str
    target_value: float
    unit: str
    comparison: str = "lt"  # lt, gt, eq
    critical: bool = True
    description: str = ""


@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆçµæœ"""

    test_name: str
    success: bool
    execution_time_ms: float
    measurements: Dict[str, float] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    performance_data: Dict[str, Any] = field(default_factory=dict)


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""

    benchmark_name: str
    total_operations: int
    total_time_ms: float
    operations_per_second: float
    average_latency_us: float
    p95_latency_us: float
    p99_latency_us: float
    success_rate: float
    target_compliance: Dict[str, bool] = field(default_factory=dict)


class HFTIntegrationTester:
    """HFTçµ±åˆãƒ†ã‚¹ã‚¿ãƒ¼"""

    def __init__(self, config_file: Optional[str] = None):
        """
        åˆæœŸåŒ–

        Args:
            config_file: ãƒ†ã‚¹ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
        """
        self.config_file = config_file

        # ãƒ†ã‚¹ãƒˆè¨­å®š
        self.test_config = self._load_test_config()

        # æ€§èƒ½ç›®æ¨™
        self.performance_targets = self._define_performance_targets()

        # ãƒ†ã‚¹ãƒˆçµæœ
        self.test_results: List[TestResult] = []
        self.benchmark_results: List[BenchmarkResult] = []

        # HFTã‚·ã‚¹ãƒ†ãƒ 
        self.orchestrator: Optional[HFTOrchestrator] = None

        logger.info("HFTçµ±åˆãƒ†ã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–å®Œäº†")

    def _load_test_config(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆè¨­å®šèª­ã¿è¾¼ã¿"""
        default_config = {
            "test_duration_seconds": 30,
            "warmup_seconds": 5,
            "market_data_rate_per_second": 1000,
            "decision_rate_per_second": 500,
            "execution_rate_per_second": 100,
            "test_symbols": ["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA"],
            "enable_stress_test": True,
            "enable_latency_test": True,
            "enable_throughput_test": True,
        }

        if self.config_file and Path(self.config_file).exists():
            try:
                with open(self.config_file, encoding="utf-8") as f:
                    loaded_config = json.load(f)
                default_config.update(loaded_config)
            except Exception as e:
                logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        return default_config

    def _define_performance_targets(self) -> List[PerformanceTarget]:
        """æ€§èƒ½ç›®æ¨™å®šç¾©"""
        return [
            # å®Ÿè¡Œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ç›®æ¨™
            PerformanceTarget(
                name="execution_latency_avg",
                target_value=50.0,
                unit="microseconds",
                comparison="lt",
                critical=True,
                description="å¹³å‡å®Ÿè¡Œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ <50Î¼s",
            ),
            PerformanceTarget(
                name="execution_latency_p95",
                target_value=100.0,
                unit="microseconds",
                comparison="lt",
                critical=True,
                description="P95å®Ÿè¡Œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ <100Î¼s",
            ),
            PerformanceTarget(
                name="execution_latency_p99",
                target_value=200.0,
                unit="microseconds",
                comparison="lt",
                critical=False,
                description="P99å®Ÿè¡Œãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ <200Î¼s",
            ),
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆç›®æ¨™
            PerformanceTarget(
                name="execution_throughput",
                target_value=1000.0,
                unit="operations_per_second",
                comparison="gt",
                critical=True,
                description="å®Ÿè¡Œã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ >1000 ops/sec",
            ),
            PerformanceTarget(
                name="decision_throughput",
                target_value=5000.0,
                unit="decisions_per_second",
                comparison="gt",
                critical=True,
                description="æ±ºå®šã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ >5000 ops/sec",
            ),
            # æˆåŠŸç‡ç›®æ¨™
            PerformanceTarget(
                name="execution_success_rate",
                target_value=0.99,
                unit="ratio",
                comparison="gt",
                critical=True,
                description="å®Ÿè¡ŒæˆåŠŸç‡ >99%",
            ),
            # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›®æ¨™
            PerformanceTarget(
                name="cpu_usage",
                target_value=80.0,
                unit="percent",
                comparison="lt",
                critical=False,
                description="CPUä½¿ç”¨ç‡ <80%",
            ),
            PerformanceTarget(
                name="memory_usage",
                target_value=85.0,
                unit="percent",
                comparison="lt",
                critical=False,
                description="ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ <85%",
            ),
        ]

    async def run_full_integration_test(self) -> Dict[str, Any]:
        """ãƒ•ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("HFTçµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")
        start_time = time.time()

        try:
            # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
            await self._test_system_initialization()

            # 2. åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
            await self._test_basic_functionality()

            # 3. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            if self.test_config.get("enable_latency_test", True):
                await self._run_latency_benchmark()

            # 4. ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            if self.test_config.get("enable_throughput_test", True):
                await self._run_throughput_benchmark()

            # 5. ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
            if self.test_config.get("enable_stress_test", True):
                await self._run_stress_test()

            # 6. æ€§èƒ½ç›®æ¨™ç¢ºèª
            compliance_report = self._check_performance_compliance()

            total_time = time.time() - start_time

            # çµ±åˆãƒ†ã‚¹ãƒˆçµæœ
            test_summary = {
                "total_execution_time_seconds": total_time,
                "total_tests": len(self.test_results),
                "passed_tests": sum(1 for r in self.test_results if r.success),
                "failed_tests": sum(1 for r in self.test_results if not r.success),
                "benchmarks_run": len(self.benchmark_results),
                "performance_compliance": compliance_report,
                "test_results": [self._test_result_to_dict(r) for r in self.test_results],
                "benchmark_results": [
                    self._benchmark_result_to_dict(r) for r in self.benchmark_results
                ],
            }

            logger.info(f"HFTçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {total_time:.2f}ç§’")
            return test_summary

        except Exception as e:
            logger.error(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise

        finally:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.orchestrator:
                await self.orchestrator.cleanup()

    async def _test_system_initialization(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        logger.info("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        errors = []

        try:
            # HFTè¨­å®šä½œæˆ
            hft_config = HFTConfig(
                target_execution_latency_us=50,
                trading_mode=HFTMode.SIMULATION,
                enable_kill_switch=True,
                enable_nanosecond_monitoring=True,
                market_data_symbols=self.test_config["test_symbols"],
            )

            # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ä½œæˆ
            self.orchestrator = create_hft_orchestrator(hft_config)

            # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
            init_success = await self.orchestrator.initialize_system()

            if not init_success:
                errors.append("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")

            # ãƒ†ã‚¹ãƒˆæˆ¦ç•¥è¿½åŠ 
            for i, symbol in enumerate(self.test_config["test_symbols"][:3]):
                strategy = HFTStrategy(
                    strategy_id=f"test_strategy_{i}",
                    strategy_name=f"Test Strategy {symbol}",
                    target_symbols=[symbol],
                    min_signal_confidence=0.7,
                )
                self.orchestrator.add_strategy(strategy)

            success = len(errors) == 0

        except Exception as e:
            errors.append(f"åˆæœŸåŒ–ä¾‹å¤–: {str(e)}")
            success = False

        execution_time = (time.time() - start_time) * 1000

        result = TestResult(
            test_name="system_initialization",
            success=success,
            execution_time_ms=execution_time,
            errors=errors,
        )

        self.test_results.append(result)
        logger.info(f"ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆå®Œäº†: {'æˆåŠŸ' if success else 'å¤±æ•—'}")

    async def _test_basic_functionality(self):
        """åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        logger.info("åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")

        start_time = time.time()
        errors = []
        measurements = {}

        try:
            if not self.orchestrator:
                errors.append("ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼æœªåˆæœŸåŒ–")
                success = False
            else:
                # å–å¼•é–‹å§‹
                await self.orchestrator.start_trading()

                # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
                status = self.orchestrator.get_system_status()
                measurements["active_strategies"] = status.get("active_strategies", 0)
                measurements["system_uptime"] = status.get("uptime_seconds", 0)

                if status.get("status") != "ACTIVE":
                    errors.append(f"äºˆæœŸã—ãªã„ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {status.get('status')}")

                # çŸ­æ™‚é–“é‹ç”¨
                await asyncio.sleep(2)

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆç¢ºèª
                detailed_perf = self.orchestrator.get_detailed_performance()
                if "microsecond_monitoring" in detailed_perf:
                    monitoring_stats = detailed_perf["microsecond_monitoring"]
                    measurements["monitoring_latency_stats"] = len(
                        monitoring_stats.get("latency_stats", {})
                    )

                # å–å¼•åœæ­¢
                await self.orchestrator.stop_trading()

                success = len(errors) == 0

        except Exception as e:
            errors.append(f"åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä¾‹å¤–: {str(e)}")
            success = False

        execution_time = (time.time() - start_time) * 1000

        result = TestResult(
            test_name="basic_functionality",
            success=success,
            execution_time_ms=execution_time,
            measurements=measurements,
            errors=errors,
        )

        self.test_results.append(result)
        logger.info(f"åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº†: {'æˆåŠŸ' if success else 'å¤±æ•—'}")

    async def _run_latency_benchmark(self):
        """ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        logger.info("ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")

        if not self.orchestrator:
            logger.error("ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼æœªåˆæœŸåŒ–")
            return

        benchmark_name = "latency_benchmark"
        start_time = time.time()

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®šãƒ‡ãƒ¼ã‚¿
        latencies = []
        successful_operations = 0
        total_operations = 0

        try:
            await self.orchestrator.start_trading()

            # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
            warmup_duration = self.test_config.get("warmup_seconds", 5)
            logger.info(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—: {warmup_duration}ç§’")
            await asyncio.sleep(warmup_duration)

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            benchmark_duration = 10  # 10ç§’é–“ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®š
            end_time = time.time() + benchmark_duration

            logger.info(f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ: {benchmark_duration}ç§’")

            while time.time() < end_time:
                operation_start = time.perf_counter_ns()

                # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸå¸‚å ´ãƒ‡ãƒ¼ã‚¿æ›´æ–°
                market_update = MarketUpdate(
                    symbol_id=1001,
                    message_type=MessageType.TRADE,
                    price=100000,  # $100.00
                    size=100000,  # 100 shares
                    exchange_timestamp_ns=operation_start,
                )

                # å‡¦ç†å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ã“ã‚Œã¯éåŒæœŸã§ç™ºç”Ÿï¼‰
                total_operations += 1

                # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                operation_end = time.perf_counter_ns()
                latency_us = (operation_end - operation_start) / 1000.0
                latencies.append(latency_us)

                if latency_us < 1000:  # 1msä»¥ä¸‹ã‚’æˆåŠŸã¨ã¿ãªã™
                    successful_operations += 1

                # ãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡
                await asyncio.sleep(0.001)  # 1msé–“éš”

            await self.orchestrator.stop_trading()

        except Exception as e:
            logger.error(f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

        # çµ±è¨ˆè¨ˆç®—
        total_time_ms = (time.time() - start_time) * 1000

        if latencies:
            avg_latency = statistics.mean(latencies)
            p95_latency = np.percentile(latencies, 95)
            p99_latency = np.percentile(latencies, 99)
        else:
            avg_latency = p95_latency = p99_latency = 0.0

        ops_per_second = total_operations / (total_time_ms / 1000) if total_time_ms > 0 else 0
        success_rate = successful_operations / total_operations if total_operations > 0 else 0.0

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        benchmark_result = BenchmarkResult(
            benchmark_name=benchmark_name,
            total_operations=total_operations,
            total_time_ms=total_time_ms,
            operations_per_second=ops_per_second,
            average_latency_us=avg_latency,
            p95_latency_us=p95_latency,
            p99_latency_us=p99_latency,
            success_rate=success_rate,
        )

        self.benchmark_results.append(benchmark_result)

        logger.info("ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†:")
        logger.info(f"  ç·æ“ä½œæ•°: {total_operations}")
        logger.info(f"  å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {avg_latency:.1f}Î¼s")
        logger.info(f"  P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {p95_latency:.1f}Î¼s")
        logger.info(f"  P99ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {p99_latency:.1f}Î¼s")
        logger.info(f"  æˆåŠŸç‡: {success_rate:.1%}")

    async def _run_throughput_benchmark(self):
        """ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        logger.info("ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")

        if not self.orchestrator:
            logger.error("ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼æœªåˆæœŸåŒ–")
            return

        benchmark_name = "throughput_benchmark"
        start_time = time.time()

        total_operations = 0
        successful_operations = 0

        try:
            await self.orchestrator.start_trading()

            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
            benchmark_duration = 15  # 15ç§’é–“ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
            end_time = time.time() + benchmark_duration

            logger.info(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ: {benchmark_duration}ç§’")

            batch_size = 10
            while time.time() < end_time:
                batch_start = time.time()

                # ãƒãƒƒãƒå‡¦ç†ï¼ˆä¸¦åˆ—å®Ÿè¡Œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
                tasks = []
                for i in range(batch_size):
                    # éåŒæœŸå‡¦ç†ã‚¿ã‚¹ã‚¯ä½œæˆ
                    task = self._simulate_trading_operation(1001 + i)
                    tasks.append(task)

                # ãƒãƒƒãƒå®Ÿè¡Œ
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # çµæœé›†è¨ˆ
                for result in results:
                    total_operations += 1
                    if not isinstance(result, Exception):
                        successful_operations += 1

                batch_time = time.time() - batch_start

                # ãƒ¬ãƒ¼ãƒˆèª¿æ•´ï¼ˆç›®æ¨™: 1000 ops/secï¼‰
                target_batch_time = batch_size / 1000  # ç§’
                if batch_time < target_batch_time:
                    await asyncio.sleep(target_batch_time - batch_time)

            await self.orchestrator.stop_trading()

        except Exception as e:
            logger.error(f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

        # çµ±è¨ˆè¨ˆç®—
        total_time_ms = (time.time() - start_time) * 1000
        ops_per_second = total_operations / (total_time_ms / 1000) if total_time_ms > 0 else 0
        success_rate = successful_operations / total_operations if total_operations > 0 else 0.0

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        benchmark_result = BenchmarkResult(
            benchmark_name=benchmark_name,
            total_operations=total_operations,
            total_time_ms=total_time_ms,
            operations_per_second=ops_per_second,
            average_latency_us=0.0,  # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®šã§ã¯è©³ç´°ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¯æ¸¬å®šã—ãªã„
            p95_latency_us=0.0,
            p99_latency_us=0.0,
            success_rate=success_rate,
        )

        self.benchmark_results.append(benchmark_result)

        logger.info("ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†:")
        logger.info(f"  ç·æ“ä½œæ•°: {total_operations}")
        logger.info(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {ops_per_second:.1f} ops/sec")
        logger.info(f"  æˆåŠŸç‡: {success_rate:.1%}")

    async def _simulate_trading_operation(self, symbol_id: int) -> bool:
        """å–å¼•æ“ä½œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        try:
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸå‡¦ç†æ™‚é–“ï¼ˆãƒã‚¤ã‚¯ãƒ­ç§’ãƒ¬ãƒ™ãƒ«ï¼‰
            processing_time = np.random.uniform(10, 100) / 1_000_000  # 10-100Î¼s
            await asyncio.sleep(processing_time)

            # 98%ã®æˆåŠŸç‡ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            return np.random.random() < 0.98

        except Exception:
            return False

    async def _run_stress_test(self):
        """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")

        if not self.orchestrator:
            logger.error("ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼æœªåˆæœŸåŒ–")
            return

        start_time = time.time()
        errors = []
        measurements = {}

        try:
            await self.orchestrator.start_trading()

            # é«˜è² è·æœŸé–“
            stress_duration = 20  # 20ç§’é–“ã®ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
            high_load_tasks = []

            logger.info(f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {stress_duration}ç§’")

            # ä¸¦åˆ—é«˜è² è·ç”Ÿæˆ
            for i in range(50):  # 50ä¸¦åˆ—ã‚¿ã‚¹ã‚¯
                task = asyncio.create_task(self._high_load_generator(i, stress_duration))
                high_load_tasks.append(task)

            # å…¨ã‚¿ã‚¹ã‚¯å®Œäº†å¾…ã¡
            results = await asyncio.gather(*high_load_tasks, return_exceptions=True)

            # çµæœé›†è¨ˆ
            successful_tasks = sum(1 for r in results if not isinstance(r, Exception))
            measurements["parallel_tasks"] = len(high_load_tasks)
            measurements["successful_tasks"] = successful_tasks
            measurements["task_success_rate"] = successful_tasks / len(high_load_tasks)

            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
            final_status = self.orchestrator.get_system_status()
            measurements["final_system_status"] = final_status.get("status")

            # è©³ç´°ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¢ºèª
            detailed_perf = self.orchestrator.get_detailed_performance()
            if "microsecond_monitoring" in detailed_perf:
                monitoring_stats = detailed_perf["microsecond_monitoring"]
                measurements["post_stress_active_alerts"] = monitoring_stats.get(
                    "active_alerts_count", 0
                )

            await self.orchestrator.stop_trading()

            success = len(errors) == 0 and measurements.get("task_success_rate", 0) > 0.9

        except Exception as e:
            errors.append(f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆä¾‹å¤–: {str(e)}")
            success = False

        execution_time = (time.time() - start_time) * 1000

        result = TestResult(
            test_name="stress_test",
            success=success,
            execution_time_ms=execution_time,
            measurements=measurements,
            errors=errors,
        )

        self.test_results.append(result)
        logger.info(f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†: {'æˆåŠŸ' if success else 'å¤±æ•—'}")

    async def _high_load_generator(self, task_id: int, duration_seconds: int):
        """é«˜è² è·ç”Ÿæˆã‚¿ã‚¹ã‚¯"""
        end_time = time.time() + duration_seconds
        operations = 0

        while time.time() < end_time:
            # é«˜é »åº¦æ“ä½œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            await self._simulate_trading_operation(1000 + task_id)
            operations += 1

            # çŸ­æ™‚é–“å¾…æ©Ÿ
            await asyncio.sleep(0.001)  # 1ms

        return operations

    def _check_performance_compliance(self) -> Dict[str, Any]:
        """æ€§èƒ½ç›®æ¨™é©åˆæ€§ç¢ºèª"""
        logger.info("æ€§èƒ½ç›®æ¨™é©åˆæ€§ç¢ºèªé–‹å§‹")

        compliance_report = {
            "total_targets": len(self.performance_targets),
            "met_targets": 0,
            "critical_failures": 0,
            "target_results": [],
            "overall_compliance": False,
        }

        # æœ€æ–°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‹ã‚‰æ€§èƒ½ãƒ‡ãƒ¼ã‚¿å–å¾—
        performance_data = self._extract_performance_data()

        for target in self.performance_targets:
            target_result = self._check_single_target(target, performance_data)
            compliance_report["target_results"].append(target_result)

            if target_result["met"]:
                compliance_report["met_targets"] += 1
            elif target.critical:
                compliance_report["critical_failures"] += 1

        # å…¨ä½“çš„ãªé©åˆæ€§åˆ¤å®š
        compliance_report["overall_compliance"] = (
            compliance_report["critical_failures"] == 0
            and compliance_report["met_targets"] / compliance_report["total_targets"] >= 0.8
        )

        logger.info(
            f"æ€§èƒ½ç›®æ¨™é©åˆæ€§: {'é©åˆ' if compliance_report['overall_compliance'] else 'éé©åˆ'}"
        )
        logger.info(
            f"  é”æˆç›®æ¨™: {compliance_report['met_targets']}/{compliance_report['total_targets']}"
        )
        logger.info(f"  é‡è¦ãªå¤±æ•—: {compliance_report['critical_failures']}")

        return compliance_report

    def _extract_performance_data(self) -> Dict[str, float]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‹ã‚‰æ€§èƒ½ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        performance_data = {}

        for benchmark in self.benchmark_results:
            if benchmark.benchmark_name == "latency_benchmark":
                performance_data["execution_latency_avg"] = benchmark.average_latency_us
                performance_data["execution_latency_p95"] = benchmark.p95_latency_us
                performance_data["execution_latency_p99"] = benchmark.p99_latency_us
                performance_data["execution_success_rate"] = benchmark.success_rate

            elif benchmark.benchmark_name == "throughput_benchmark":
                performance_data["execution_throughput"] = benchmark.operations_per_second
                performance_data["decision_throughput"] = (
                    benchmark.operations_per_second * 5
                )  # æ¨å®š

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ï¼ˆæœ€æ–°ãƒ†ã‚¹ãƒˆçµæœã‹ã‚‰ï¼‰
        for test_result in self.test_results:
            if "cpu_usage" in test_result.measurements:
                performance_data["cpu_usage"] = test_result.measurements["cpu_usage"]
            if "memory_usage" in test_result.measurements:
                performance_data["memory_usage"] = test_result.measurements["memory_usage"]

        return performance_data

    def _check_single_target(
        self, target: PerformanceTarget, performance_data: Dict[str, float]
    ) -> Dict[str, Any]:
        """å˜ä¸€ç›®æ¨™ç¢ºèª"""
        current_value = performance_data.get(target.name, 0.0)

        # æ¯”è¼ƒå®Ÿè¡Œ
        if target.comparison == "lt":
            met = current_value < target.target_value
        elif target.comparison == "gt":
            met = current_value > target.target_value
        elif target.comparison == "eq":
            met = abs(current_value - target.target_value) < 0.001
        else:
            met = False

        return {
            "name": target.name,
            "description": target.description,
            "target_value": target.target_value,
            "current_value": current_value,
            "unit": target.unit,
            "comparison": target.comparison,
            "met": met,
            "critical": target.critical,
            "deviation": (
                abs(current_value - target.target_value) / target.target_value
                if target.target_value != 0
                else 0.0
            ),
        }

    def _test_result_to_dict(self, result: TestResult) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "test_name": result.test_name,
            "success": result.success,
            "execution_time_ms": result.execution_time_ms,
            "measurements": result.measurements,
            "errors": result.errors,
        }

    def _benchmark_result_to_dict(self, result: BenchmarkResult) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "benchmark_name": result.benchmark_name,
            "total_operations": result.total_operations,
            "total_time_ms": result.total_time_ms,
            "operations_per_second": result.operations_per_second,
            "average_latency_us": result.average_latency_us,
            "p95_latency_us": result.p95_latency_us,
            "p99_latency_us": result.p99_latency_us,
            "success_rate": result.success_rate,
        }

    async def save_test_report(self, output_file: str = "hft_test_report.json"):
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_file}")

        # çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_summary = await self.run_full_integration_test()

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(test_summary, f, indent=2, ensure_ascii=False)

        logger.info(f"ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_file}")
        return test_summary


# Factory function
def create_hft_integration_tester(
    config_file: Optional[str] = None,
) -> HFTIntegrationTester:
    """HFTçµ±åˆãƒ†ã‚¹ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°"""
    return HFTIntegrationTester(config_file)


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async def main():
        print("=== Issue #366 HFTçµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ€§èƒ½æ¤œè¨¼ ===")

        tester = None
        try:
            # çµ±åˆãƒ†ã‚¹ã‚¿ãƒ¼ä½œæˆ
            tester = create_hft_integration_tester()

            print("\nçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

            # ãƒ•ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            test_summary = await tester.run_full_integration_test()

            # çµæœè¡¨ç¤º
            print("\n=== ãƒ†ã‚¹ãƒˆçµæœ ===")
            print(f"å®Ÿè¡Œæ™‚é–“: {test_summary['total_execution_time_seconds']:.2f}ç§’")
            print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {test_summary['total_tests']}")
            print(f"æˆåŠŸãƒ†ã‚¹ãƒˆ: {test_summary['passed_tests']}")
            print(f"å¤±æ•—ãƒ†ã‚¹ãƒˆ: {test_summary['failed_tests']}")
            print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {test_summary['benchmarks_run']}")

            # æ€§èƒ½é©åˆæ€§
            compliance = test_summary["performance_compliance"]
            print("\n=== æ€§èƒ½ç›®æ¨™é©åˆæ€§ ===")
            print(f"å…¨ä½“é©åˆæ€§: {'é©åˆ' if compliance['overall_compliance'] else 'éé©åˆ'}")
            print(f"é”æˆç›®æ¨™: {compliance['met_targets']}/{compliance['total_targets']}")
            print(f"é‡è¦ãªå¤±æ•—: {compliance['critical_failures']}")

            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœæ¦‚è¦
            print("\n=== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ ===")
            for benchmark in test_summary["benchmark_results"]:
                print(f"{benchmark['benchmark_name']}:")
                print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {benchmark['operations_per_second']:.1f} ops/sec")
                if benchmark["average_latency_us"] > 0:
                    print(f"  å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: {benchmark['average_latency_us']:.1f}Î¼s")
                print(f"  æˆåŠŸç‡: {benchmark['success_rate']:.1%}")

            # æ€§èƒ½ç›®æ¨™è©³ç´°
            print("\n=== æ€§èƒ½ç›®æ¨™è©³ç´° ===")
            for target_result in compliance["target_results"]:
                status = "âœ“" if target_result["met"] else "âœ—"
                critical = " (é‡è¦)" if target_result["critical"] else ""
                print(
                    f"{status} {target_result['name']}: {target_result['current_value']:.1f} {target_result['comparison']} {target_result['target_value']:.1f} {target_result['unit']}{critical}"
                )

            # å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ
            print("\n=== å€‹åˆ¥ãƒ†ã‚¹ãƒˆçµæœ ===")
            for test_result in test_summary["test_results"]:
                status = "âœ“" if test_result["success"] else "âœ—"
                print(
                    f"{status} {test_result['test_name']}: {test_result['execution_time_ms']:.1f}ms"
                )
                if test_result["errors"]:
                    for error in test_result["errors"]:
                        print(f"    ã‚¨ãƒ©ãƒ¼: {error}")

            # æœ€çµ‚åˆ¤å®š
            overall_success = test_summary["failed_tests"] == 0 and compliance["overall_compliance"]

            print("\n=== æœ€çµ‚åˆ¤å®š ===")
            if overall_success:
                print("ğŸ‰ HFTæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³: å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸãƒ»æ€§èƒ½ç›®æ¨™é”æˆ")
            else:
                print("âš ï¸ HFTæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³: ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—ã¾ãŸã¯æ€§èƒ½ç›®æ¨™æœªé”")

        except Exception as e:
            print(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

        finally:
            if tester and tester.orchestrator:
                await tester.orchestrator.cleanup()

        print("\n=== HFTçµ±åˆãƒ†ã‚¹ãƒˆãƒ»æ€§èƒ½æ¤œè¨¼å®Œäº† ===")

    asyncio.run(main())
