"""
ç°¡æ˜“çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 

äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
"""

import asyncio
import time
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import json
import logging
from dataclasses import dataclass, asdict
import statistics


@dataclass
class SimpleTestResult:
    """ç°¡æ˜“ãƒ†ã‚¹ãƒˆçµæœ"""
    test_name: str
    start_time: datetime
    end_time: datetime
    duration_seconds: float
    prediction_accuracy: float
    processing_speed: float
    memory_efficiency: float
    overall_score: float
    success: bool
    recommendations: List[str]


class SimpleIntegratedTestSystem:
    """ç°¡æ˜“çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.test_results: List[SimpleTestResult] = []
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼è¨­å®š"""
        logger = logging.getLogger(__name__)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
        return logger
    
    def create_test_data(self, size: int = 1000) -> pd.DataFrame:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        np.random.seed(42)
        
        # æ ªå¼å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        dates = pd.date_range(
            start=datetime.now() - timedelta(days=size),
            periods=size,
            freq='1min'
        )
        
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        base_price = 1000
        price_changes = np.random.normal(0, 0.01, size)
        prices = [base_price]
        
        for change in price_changes[1:]:
            new_price = prices[-1] * (1 + change)
            prices.append(max(new_price, 1))
        
        data = pd.DataFrame({
            'timestamp': dates,
            'symbol': ['TEST'] * size,
            'open': prices,
            'high': [p * 1.01 for p in prices],
            'low': [p * 0.99 for p in prices],
            'close': [p * (1 + np.random.normal(0, 0.002)) for p in prices],
            'volume': np.random.randint(1000, 100000, size)
        })
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆæ¬¡ã®ä¾¡æ ¼ãŒä¸Šæ˜‡ã™ã‚‹ã‹ï¼‰
        data['target'] = (data['close'].shift(-1) > data['close']).astype(int)
        data = data.dropna()
        
        return data
    
    def simple_prediction_test(self, data: pd.DataFrame) -> float:
        """ç°¡æ˜“äºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
        # å˜ç´”ãªç§»å‹•å¹³å‡ãƒ™ãƒ¼ã‚¹äºˆæ¸¬
        data['ma5'] = data['close'].rolling(5).mean()
        data['ma20'] = data['close'].rolling(20).mean()
        
        # äºˆæ¸¬: çŸ­æœŸç§»å‹•å¹³å‡ > é•·æœŸç§»å‹•å¹³å‡ ãªã‚‰ä¸Šæ˜‡äºˆæ¸¬
        predictions = (data['ma5'] > data['ma20']).astype(int)
        actuals = data['target']
        
        # NaNé™¤å»
        valid_idx = ~(predictions.isna() | actuals.isna())
        predictions = predictions[valid_idx]
        actuals = actuals[valid_idx]
        
        if len(predictions) == 0:
            return 0.5
        
        accuracy = (predictions == actuals).mean()
        return accuracy
    
    def performance_test(self, data: pd.DataFrame) -> Dict[str, float]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        start_time = time.time()
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
        processed_chunks = []
        chunk_size = 100
        
        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i+chunk_size]
            
            # ç°¡å˜ãªè¨ˆç®—å‡¦ç†
            chunk_processed = chunk.copy()
            chunk_processed['sma'] = chunk['close'].rolling(5).mean()
            chunk_processed['ema'] = chunk['close'].ewm(span=5).mean()
            chunk_processed['rsi'] = self._calculate_rsi(chunk['close'])
            
            processed_chunks.append(len(chunk_processed))
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        total_records = sum(processed_chunks)
        processing_speed = total_records / processing_time if processing_time > 0 else 0
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        memory_efficiency = min(100, 1000 / len(data) * 100)  # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºé€†æ¯”ä¾‹
        
        return {
            'processing_speed': processing_speed,
            'memory_efficiency': memory_efficiency,
            'processing_time': processing_time
        }
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """RSIè¨ˆç®—"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def calculate_overall_score(self, prediction_accuracy: float, 
                              performance_metrics: Dict[str, float]) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # é‡ã¿ä»˜ãã‚¹ã‚³ã‚¢
        accuracy_score = prediction_accuracy * 100  # 0-100ã‚¹ã‚±ãƒ¼ãƒ«
        speed_score = min(100, performance_metrics['processing_speed'] / 10)  # é€Ÿåº¦æ­£è¦åŒ–
        memory_score = performance_metrics['memory_efficiency']
        
        # é‡ã¿ä»˜ãå¹³å‡
        overall_score = (
            accuracy_score * 0.5 +      # äºˆæ¸¬ç²¾åº¦ 50%
            speed_score * 0.3 +         # å‡¦ç†é€Ÿåº¦ 30%
            memory_score * 0.2          # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ 20%
        )
        
        return overall_score
    
    def generate_recommendations(self, prediction_accuracy: float,
                               performance_metrics: Dict[str, float],
                               overall_score: float) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        if prediction_accuracy < 0.6:
            recommendations.append("äºˆæ¸¬ç²¾åº¦å‘ä¸Šã®ãŸã‚ã€ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’æ¨å¥¨")
        
        if performance_metrics['processing_speed'] < 100:
            recommendations.append("å‡¦ç†é€Ÿåº¦å‘ä¸Šã®ãŸã‚ã€ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…ã‚’æ¨å¥¨")
        
        if performance_metrics['memory_efficiency'] < 70:
            recommendations.append("ãƒ¡ãƒ¢ãƒªä½¿ç”¨åŠ¹ç‡åŒ–ã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®æœ€é©åŒ–ã‚’æ¨å¥¨")
        
        if overall_score < 70:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æœ€é©åŒ–ãŒå¿…è¦ã§ã™")
        elif overall_score >= 80:
            recommendations.append("ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ã«å‹•ä½œã—ã¦ã„ã¾ã™")
        
        return recommendations
    
    async def run_integrated_test(self, test_name: str = "çµ±åˆãƒ†ã‚¹ãƒˆ") -> SimpleTestResult:
        """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        self.logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹: {test_name}")
        start_time = datetime.now()
        
        try:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
            test_data = self.create_test_data(2000)
            self.logger.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(test_data)}ä»¶")
            
            # äºˆæ¸¬ç²¾åº¦ãƒ†ã‚¹ãƒˆ
            prediction_accuracy = self.simple_prediction_test(test_data)
            self.logger.info(f"äºˆæ¸¬ç²¾åº¦: {prediction_accuracy:.3f}")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            performance_metrics = self.performance_test(test_data)
            self.logger.info(f"å‡¦ç†é€Ÿåº¦: {performance_metrics['processing_speed']:.1f} records/sec")
            
            # ç·åˆè©•ä¾¡
            overall_score = self.calculate_overall_score(prediction_accuracy, performance_metrics)
            recommendations = self.generate_recommendations(
                prediction_accuracy, performance_metrics, overall_score
            )
            
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            result = SimpleTestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=end_time,
                duration_seconds=duration,
                prediction_accuracy=prediction_accuracy,
                processing_speed=performance_metrics['processing_speed'],
                memory_efficiency=performance_metrics['memory_efficiency'],
                overall_score=overall_score,
                success=True,
                recommendations=recommendations
            )
            
            self.test_results.append(result)
            self.logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {test_name} (ã‚¹ã‚³ã‚¢: {overall_score:.1f})")
            
            return result
            
        except Exception as e:
            error_msg = f"çµ±åˆãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}"
            self.logger.error(error_msg)
            
            result = SimpleTestResult(
                test_name=test_name,
                start_time=start_time,
                end_time=datetime.now(),
                duration_seconds=(datetime.now() - start_time).total_seconds(),
                prediction_accuracy=0.0,
                processing_speed=0.0,
                memory_efficiency=0.0,
                overall_score=0.0,
                success=False,
                recommendations=["ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"]
            )
            
            return result
    
    async def run_test_suite(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ"""
        self.logger.info("çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
        
        test_scenarios = [
            "åŸºæœ¬çµ±åˆãƒ†ã‚¹ãƒˆ",
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ",
            "ç²¾åº¦ãƒ†ã‚¹ãƒˆ"
        ]
        
        suite_results = []
        
        for scenario in test_scenarios:
            result = await self.run_integrated_test(scenario)
            suite_results.append(result)
            await asyncio.sleep(0.1)  # å°ã•ãªä¼‘æ†©
        
        # çµ±è¨ˆã‚µãƒãƒªãƒ¼
        successful_tests = [r for r in suite_results if r.success]
        
        summary = {
            'total_tests': len(suite_results),
            'successful_tests': len(successful_tests),
            'success_rate': len(successful_tests) / len(suite_results) * 100,
            'average_score': statistics.mean([r.overall_score for r in successful_tests]) if successful_tests else 0,
            'average_accuracy': statistics.mean([r.prediction_accuracy for r in successful_tests]) if successful_tests else 0,
            'average_speed': statistics.mean([r.processing_speed for r in successful_tests]) if successful_tests else 0,
            'test_results': [asdict(r) for r in suite_results]
        }
        
        return summary
    
    def export_results(self, filepath: str = None) -> str:
        """çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not filepath:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = f"simple_integrated_test_results_{timestamp}.json"
        
        results_data = {
            'test_summary': {
                'total_tests': len(self.test_results),
                'timestamp': datetime.now().isoformat(),
                'system_info': 'Day Trade Simple Integration Test'
            },
            'results': [asdict(r) for r in self.test_results]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ãƒ†ã‚¹ãƒˆçµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {filepath}")
        return filepath


async def demo_simple_test():
    """ç°¡æ˜“ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¢"""
    print("=== Day Trade ç°¡æ˜“çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢ ===")
    
    test_system = SimpleIntegratedTestSystem()
    
    try:
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
        print("\nçµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œä¸­...")
        summary = await test_system.run_test_suite()
        
        print(f"\n=== ãƒ†ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ ===")
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['total_tests']}")
        print(f"æˆåŠŸãƒ†ã‚¹ãƒˆæ•°: {summary['successful_tests']}")
        print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {summary['average_score']:.1f}/100")
        print(f"å¹³å‡äºˆæ¸¬ç²¾åº¦: {summary['average_accuracy']:.3f}")
        print(f"å¹³å‡å‡¦ç†é€Ÿåº¦: {summary['average_speed']:.1f} records/sec")
        
        # å€‹åˆ¥çµæœè¡¨ç¤º
        for i, result_data in enumerate(summary['test_results'], 1):
            result = SimpleTestResult(**result_data)
            print(f"\n--- ãƒ†ã‚¹ãƒˆ {i}: {result.test_name} ---")
            print(f"å®Ÿè¡Œæ™‚é–“: {result.duration_seconds:.2f}ç§’")
            print(f"æˆåŠŸ: {'âœ“' if result.success else 'âœ—'}")
            print(f"ç·åˆã‚¹ã‚³ã‚¢: {result.overall_score:.1f}/100")
            print(f"äºˆæ¸¬ç²¾åº¦: {result.prediction_accuracy:.3f}")
            print(f"å‡¦ç†é€Ÿåº¦: {result.processing_speed:.1f} records/sec")
            print(f"ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: {result.memory_efficiency:.1f}%")
            
            if result.recommendations:
                print("æ¨å¥¨äº‹é …:")
                for j, rec in enumerate(result.recommendations, 1):
                    print(f"  {j}. {rec}")
        
        # çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        print(f"\nçµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
        export_file = test_system.export_results()
        print(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {export_file}")
        
        # ç·åˆè©•ä¾¡
        if summary['average_score'] >= 80:
            print(f"\nğŸ‰ ã‚·ã‚¹ãƒ†ãƒ ã¯å„ªç§€ã«å‹•ä½œã—ã¦ã„ã¾ã™ï¼")
        elif summary['average_score'] >= 60:
            print(f"\nâœ… ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        else:
            print(f"\nâš ï¸ ã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–ãŒå¿…è¦ã§ã™ã€‚")
        
    except Exception as e:
        print(f"ãƒ‡ãƒ¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(demo_simple_test())