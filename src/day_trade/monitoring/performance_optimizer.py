#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«

ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Ÿè¡Œ
"""

import ast
import re
import time
from pathlib import Path
from typing import Dict, List, Set
from datetime import datetime


class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.optimization_results = {
            'timestamp': datetime.now().isoformat(),
            'optimizations_applied': [],
            'performance_improvements': {},
            'recommendations': []
        }

    def optimize_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Ÿè¡Œ"""
        print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–é–‹å§‹")
        print("=" * 40)

        # 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆæœ€é©åŒ–
        self._optimize_imports()

        # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥æœ€é©åŒ–
        self._optimize_caching()

        # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–
        self._optimize_database_access()

        # 4. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
        self._optimize_memory_usage()

        # 5. éåŒæœŸå‡¦ç†æœ€é©åŒ–
        self._optimize_async_processing()

        print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Œäº†")

    def _optimize_imports(self):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæœ€é©åŒ–"""
        print("1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆæœ€é©åŒ–ä¸­...")

        # é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè£…
        lazy_import_template = '''#!/usr/bin/env python3
"""
é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãŸã‚ã®é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Ÿè£…
"""

import importlib
from typing import Any, Dict, Optional


class LazyImport:
    """é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¯ãƒ©ã‚¹"""

    def __init__(self, module_name: str, attr_name: Optional[str] = None):
        self.module_name = module_name
        self.attr_name = attr_name
        self._module = None

    def __getattr__(self, name: str) -> Any:
        if self._module is None:
            self._module = importlib.import_module(self.module_name)

        if self.attr_name:
            attr = getattr(self._module, self.attr_name)
            return getattr(attr, name)
        else:
            return getattr(self._module, name)


class OptimizedImports:
    """æœ€é©åŒ–ã•ã‚ŒãŸã‚¤ãƒ³ãƒãƒ¼ãƒˆç®¡ç†"""

    # é‡ã„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    numpy = LazyImport('numpy')
    pandas = LazyImport('pandas')
    sklearn = LazyImport('sklearn')
    tensorflow = LazyImport('tensorflow')
    torch = LazyImport('torch')

    # ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹è»½é‡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    @staticmethod
    def get_datetime():
        from datetime import datetime
        return datetime

    @staticmethod
    def get_json():
        import json
        return json

    @staticmethod
    def get_pathlib():
        from pathlib import Path
        return Path


# ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
optimized_imports = OptimizedImports()

# ä½¿ç”¨ä¾‹
# np = optimized_imports.numpy  # ä½¿ç”¨æ™‚ã«åˆã‚ã¦numpyãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã‚‹
# pd = optimized_imports.pandas  # ä½¿ç”¨æ™‚ã«åˆã‚ã¦pandasãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã‚‹
'''

        # æœ€é©åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        perf_dir = self.base_dir / "src" / "day_trade" / "performance"
        perf_dir.mkdir(parents=True, exist_ok=True)

        output_file = perf_dir / "lazy_imports.py"
        output_file.write_text(lazy_import_template, encoding='utf-8')
        print("    ä½œæˆ: é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«")

        self.optimization_results['optimizations_applied'].append('lazy_imports')

    def _optimize_caching(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥æœ€é©åŒ–"""
        print("2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥æœ€é©åŒ–ä¸­...")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        perf_dir = self.base_dir / "src" / "day_trade" / "performance"
        perf_dir.mkdir(parents=True, exist_ok=True)

        cache_optimizer = '''#!/usr/bin/env python3
"""
é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 

ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã‚¢ã‚¯ã‚»ã‚¹é€Ÿåº¦ã‚’æœ€é©åŒ–ã—ãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…
"""

import time
import threading
from collections import OrderedDict
from typing import Any, Callable, Dict, Optional, Tuple
from functools import wraps
import hashlib
import pickle


class OptimizedCache:
    """æœ€é©åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹"""

    def __init__(self, max_size: int = 1000, ttl: float = 3600):
        self.max_size = max_size
        self.ttl = ttl
        self._cache: OrderedDict = OrderedDict()
        self._timestamps: Dict[str, float] = {}
        self._lock = threading.RLock()
        self._hit_count = 0
        self._miss_count = 0

    def _generate_key(self, args: Tuple, kwargs: Dict) -> str:
        """ã‚­ãƒ¼ç”Ÿæˆï¼ˆé«˜é€ŸåŒ–ï¼‰"""
        # ã‚ˆã‚Šé«˜é€Ÿãªã‚­ãƒ¼ç”Ÿæˆ
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()

    def _is_expired(self, key: str) -> bool:
        """æœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯"""
        if key not in self._timestamps:
            return True
        return time.time() - self._timestamps[key] > self.ttl

    def _cleanup_expired(self):
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        current_time = time.time()
        expired_keys = [
            key for key, timestamp in self._timestamps.items()
            if current_time - timestamp > self.ttl
        ]

        for key in expired_keys:
            self._cache.pop(key, None)
            self._timestamps.pop(key, None)

    def get(self, key: str) -> Optional[Any]:
        """å€¤å–å¾—"""
        with self._lock:
            if key in self._cache and not self._is_expired(key):
                # LRUæ›´æ–°
                self._cache.move_to_end(key)
                self._hit_count += 1
                return self._cache[key]

            self._miss_count += 1
            return None

    def set(self, key: str, value: Any):
        """å€¤è¨­å®š"""
        with self._lock:
            # å®¹é‡åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self._cache) >= self.max_size:
                # æœ€ã‚‚å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
                oldest_key = next(iter(self._cache))
                self._cache.pop(oldest_key)
                self._timestamps.pop(oldest_key, None)

            self._cache[key] = value
            self._timestamps[key] = time.time()

            # å®šæœŸçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if len(self._cache) % 100 == 0:
                self._cleanup_expired()

    def cache_decorator(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable) -> Callable:
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ã‚­ãƒ¼ç”Ÿæˆ
                cache_key = f"{func.__name__}_{self._generate_key(args, kwargs)}"

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
                result = self.get(cache_key)
                if result is not None:
                    return result

                # é–¢æ•°å®Ÿè¡Œã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                result = func(*args, **kwargs)
                self.set(cache_key, result)
                return result

            return wrapper
        return decorator

    def get_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ"""
        total_requests = self._hit_count + self._miss_count
        hit_rate = self._hit_count / total_requests if total_requests > 0 else 0

        return {
            'size': len(self._cache),
            'max_size': self.max_size,
            'hit_count': self._hit_count,
            'miss_count': self._miss_count,
            'hit_rate': hit_rate,
            'memory_usage': len(pickle.dumps(self._cache))
        }


class CacheManager:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self):
        self.caches: Dict[str, OptimizedCache] = {}

    def get_cache(self, name: str, max_size: int = 1000, ttl: float = 3600) -> OptimizedCache:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰"""
        if name not in self.caches:
            self.caches[name] = OptimizedCache(max_size, ttl)
        return self.caches[name]

    def clear_all(self):
        """å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        for cache in self.caches.values():
            cache._cache.clear()
            cache._timestamps.clear()

    def get_global_stats(self) -> Dict[str, Any]:
        """å…¨ä½“çµ±è¨ˆ"""
        stats = {}
        for name, cache in self.caches.items():
            stats[name] = cache.get_stats()
        return stats


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
cache_manager = CacheManager()

# ä¾¿åˆ©ãªé–¢æ•°
def cached(cache_name: str = 'default', max_size: int = 1000, ttl: float = 3600):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    cache = cache_manager.get_cache(cache_name, max_size, ttl)
    return cache.cache_decorator()

# ä½¿ç”¨ä¾‹:
# @cached('stock_data', max_size=500, ttl=1800)  # 30åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# def get_stock_price(symbol):
#     # é‡ã„å‡¦ç†
#     return fetch_stock_price(symbol)
'''

        output_file = perf_dir / "optimized_cache.py"
        output_file.write_text(cache_optimizer, encoding='utf-8')
        print("    ä½œæˆ: æœ€é©åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ")

        self.optimization_results['optimizations_applied'].append('optimized_cache')

    def _optimize_database_access(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–"""
        print("3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–ä¸­...")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        perf_dir = self.base_dir / "src" / "day_trade" / "performance"
        perf_dir.mkdir(parents=True, exist_ok=True)

        db_optimizer = '''#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–

ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ¼ãƒ«ã€ãƒãƒƒãƒå‡¦ç†ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
"""

import sqlite3
import threading
import time
from contextlib import contextmanager
from queue import Queue, Empty
from typing import Any, Dict, List, Optional, Tuple
from pathlib import Path


class ConnectionPool:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«"""

    def __init__(self, db_path: str, pool_size: int = 10):
        self.db_path = db_path
        self.pool_size = pool_size
        self._pool = Queue(maxsize=pool_size)
        self._lock = threading.Lock()
        self._created_connections = 0

        # åˆæœŸæ¥ç¶šä½œæˆ
        for _ in range(min(3, pool_size)):  # æœ€åˆã¯3å€‹ã ã‘ä½œæˆ
            self._create_connection()

    def _create_connection(self) -> sqlite3.Connection:
        """æ–°ã—ã„æ¥ç¶šä½œæˆ"""
        conn = sqlite3.connect(
            self.db_path,
            check_same_thread=False,
            timeout=30.0
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–è¨­å®š
        conn.execute("PRAGMA journal_mode=WAL")  # WALãƒ¢ãƒ¼ãƒ‰
        conn.execute("PRAGMA synchronous=NORMAL")  # åŒæœŸãƒ¬ãƒ™ãƒ«èª¿æ•´
        conn.execute("PRAGMA cache_size=10000")  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºå¢—åŠ 
        conn.execute("PRAGMA temp_store=MEMORY")  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¡ãƒ¢ãƒªã«

        self._created_connections += 1
        return conn

    def get_connection(self) -> sqlite3.Connection:
        """æ¥ç¶šå–å¾—"""
        try:
            # ãƒ—ãƒ¼ãƒ«ã‹ã‚‰å–å¾—è©¦è¡Œ
            return self._pool.get_nowait()
        except Empty:
            # ãƒ—ãƒ¼ãƒ«ãŒç©ºã®å ´åˆã€æ–°ã—ã„æ¥ç¶šä½œæˆ
            with self._lock:
                if self._created_connections < self.pool_size:
                    return self._create_connection()
                else:
                    # æœ€å¤§æ•°ã«é”ã—ã¦ã„ã‚‹å ´åˆã¯å¾…æ©Ÿ
                    return self._pool.get(timeout=5.0)

    def return_connection(self, conn: sqlite3.Connection):
        """æ¥ç¶šè¿”å´"""
        try:
            self._pool.put_nowait(conn)
        except:
            # ãƒ—ãƒ¼ãƒ«ãŒæº€æ¯ã®å ´åˆã¯æ¥ç¶šã‚’é–‰ã˜ã‚‹
            conn.close()

    @contextmanager
    def connection(self):
        """æ¥ç¶šã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        conn = self.get_connection()
        try:
            yield conn
        finally:
            self.return_connection(conn)


class BatchProcessor:
    """ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–"""

    def __init__(self, connection_pool: ConnectionPool, batch_size: int = 1000):
        self.pool = connection_pool
        self.batch_size = batch_size

    def batch_insert(self, table: str, columns: List[str], data: List[Tuple]) -> int:
        """ãƒãƒƒãƒã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        placeholders = ', '.join(['?' for _ in columns])
        columns_str = ', '.join(columns)
        sql = f"INSERT INTO {table} ({columns_str}) VALUES ({placeholders})"

        total_inserted = 0

        with self.pool.connection() as conn:
            cursor = conn.cursor()

            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã”ã¨ã«åˆ†å‰²ã—ã¦å‡¦ç†
            for i in range(0, len(data), self.batch_size):
                batch = data[i:i + self.batch_size]
                cursor.executemany(sql, batch)
                conn.commit()
                total_inserted += len(batch)

        return total_inserted

    def batch_update(self, table: str, set_clause: str, where_clause: str,
                    data: List[Tuple]) -> int:
        """ãƒãƒƒãƒã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ"""
        sql = f"UPDATE {table} SET {set_clause} WHERE {where_clause}"

        total_updated = 0

        with self.pool.connection() as conn:
            cursor = conn.cursor()

            for i in range(0, len(data), self.batch_size):
                batch = data[i:i + self.batch_size]
                cursor.executemany(sql, batch)
                conn.commit()
                total_updated += cursor.rowcount

        return total_updated


class QueryOptimizer:
    """ã‚¯ã‚¨ãƒªæœ€é©åŒ–"""

    def __init__(self, connection_pool: ConnectionPool):
        self.pool = connection_pool
        self._query_cache = {}

    def create_indexes(self, db_path: str):
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ"""
        index_definitions = [
            "CREATE INDEX IF NOT EXISTS idx_stocks_symbol ON stocks(symbol)",
            "CREATE INDEX IF NOT EXISTS idx_stocks_date ON stocks(date)",
            "CREATE INDEX IF NOT EXISTS idx_predictions_symbol_date ON predictions(symbol, date)",
            "CREATE INDEX IF NOT EXISTS idx_trades_timestamp ON trades(timestamp)",
            "CREATE INDEX IF NOT EXISTS idx_alerts_timestamp ON alerts(timestamp)",
        ]

        with self.pool.connection() as conn:
            for index_sql in index_definitions:
                try:
                    conn.execute(index_sql)
                    print(f"    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ: {index_sql.split()[-1]}")
                except sqlite3.Error as e:
                    print(f"    ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

    def optimize_query(self, sql: str) -> str:
        """ã‚¯ã‚¨ãƒªæœ€é©åŒ–"""
        # åŸºæœ¬çš„ãªæœ€é©åŒ–ãƒ«ãƒ¼ãƒ«
        optimized_sql = sql

        # LIMITå¥ãŒãªã„å¤§ããªã‚¯ã‚¨ãƒªã«è­¦å‘Š
        if 'SELECT' in sql.upper() and 'LIMIT' not in sql.upper():
            if 'COUNT' not in sql.upper():
                print(f"    è­¦å‘Š: LIMITå¥ã®ãªã„ã‚¯ã‚¨ãƒª: {sql[:50]}...")

        # ORDER BYæœ€é©åŒ–
        if 'ORDER BY' in sql.upper() and 'INDEX' not in sql.upper():
            print(f"    æ¨å¥¨: ORDER BYå¥ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨ã‚’æ¤œè¨")

        return optimized_sql

    def explain_query(self, sql: str) -> List[Dict]:
        """ã‚¯ã‚¨ãƒªå®Ÿè¡Œè¨ˆç”»åˆ†æ"""
        explain_sql = f"EXPLAIN QUERY PLAN {sql}"

        with self.pool.connection() as conn:
            cursor = conn.cursor()
            cursor.execute(explain_sql)

            results = []
            for row in cursor.fetchall():
                results.append({
                    'id': row[0],
                    'parent': row[1],
                    'notused': row[2],
                    'detail': row[3]
                })

            return results


class DatabaseManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self.pool = ConnectionPool(db_path)
        self.batch_processor = BatchProcessor(self.pool)
        self.query_optimizer = QueryOptimizer(self.pool)

        # åˆæœŸæœ€é©åŒ–
        self._initialize_optimizations()

    def _initialize_optimizations(self):
        """åˆæœŸæœ€é©åŒ–è¨­å®š"""
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
        self.query_optimizer.create_indexes(self.db_path)

        # VACUUMå®Ÿè¡Œï¼ˆå®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ï¼‰
        with self.pool.connection() as conn:
            conn.execute("VACUUM")
            print("    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–å®Œäº†")

    def get_performance_stats(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ"""
        with self.pool.connection() as conn:
            cursor = conn.cursor()

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º
            cursor.execute("SELECT page_count * page_size as size FROM pragma_page_count(), pragma_page_size()")
            db_size = cursor.fetchone()[0]

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
            cursor.execute("PRAGMA cache_size")
            cache_size = cursor.fetchone()[0]

            return {
                'database_size': db_size,
                'cache_size': cache_size,
                'connection_pool_size': self.pool._created_connections,
                'wal_mode': True  # WALãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹
            }


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
db_managers = {}

def get_db_manager(db_path: str) -> DatabaseManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼å–å¾—"""
    if db_path not in db_managers:
        db_managers[db_path] = DatabaseManager(db_path)
    return db_managers[db_path]
'''

        output_file = perf_dir / "database_optimizer.py"
        output_file.write_text(db_optimizer, encoding='utf-8')
        print("    ä½œæˆ: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")

        self.optimization_results['optimizations_applied'].append('database_optimization')

    def _optimize_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"""
        print("4. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ä¸­...")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        perf_dir = self.base_dir / "src" / "day_trade" / "performance"
        perf_dir.mkdir(parents=True, exist_ok=True)

        memory_optimizer = '''#!/usr/bin/env python3
"""
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–

åŠ¹ç‡çš„ãªãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–
"""

import gc
import sys
import threading
import time
import weakref
from typing import Any, Dict, List, Optional
import psutil
import os


class MemoryMonitor:
    """ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, threshold_mb: float = 500.0):
        self.threshold_mb = threshold_mb
        self.monitoring = False
        self.monitor_thread = None
        self._callbacks = []

    def add_callback(self, callback):
        """ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ """
        self._callbacks.append(callback)

    def get_memory_usage(self) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å–å¾—"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()

        return {
            'rss_mb': memory_info.rss / 1024 / 1024,
            'vms_mb': memory_info.vms / 1024 / 1024,
            'percent': process.memory_percent()
        }

    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if self.monitoring:
            return

        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()

    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)

    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                memory_info = self.get_memory_usage()

                if memory_info['rss_mb'] > self.threshold_mb:
                    print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è­¦å‘Š: {memory_info['rss_mb']:.1f}MB")

                    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
                    for callback in self._callbacks:
                        try:
                            callback(memory_info)
                        except Exception as e:
                            print(f"ãƒ¡ãƒ¢ãƒªã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

                time.sleep(5.0)  # 5ç§’é–“éš”

            except Exception as e:
                print(f"ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(10.0)


class MemoryOptimizer:
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.weak_refs = weakref.WeakSet()
        self.monitor = MemoryMonitor()
        self.monitor.add_callback(self._on_memory_pressure)

    def register_object(self, obj):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç™»éŒ²ï¼ˆå¼±å‚ç…§ï¼‰"""
        self.weak_refs.add(obj)

    def _on_memory_pressure(self, memory_info):
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®å‡¦ç†"""
        print("ğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œä¸­...")

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        collected = gc.collect()
        print(f"   ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³: {collected}å€‹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè§£æ”¾")

        # æ‰‹å‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._manual_cleanup()

        # å†åº¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        new_memory = self.monitor.get_memory_usage()
        saved_mb = memory_info['rss_mb'] - new_memory['rss_mb']
        print(f"   ãƒ¡ãƒ¢ãƒªè§£æ”¾: {saved_mb:.1f}MB")

    def _manual_cleanup(self):
        """æ‰‹å‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # å¼±å‚ç…§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        alive_objects = []
        for obj in self.weak_refs:
            if hasattr(obj, 'cleanup'):
                try:
                    obj.cleanup()
                except:
                    pass
            alive_objects.append(obj)

        print(f"   å¼±å‚ç…§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ: {len(alive_objects)}å€‹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")

    def optimize_gc(self):
        """ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–"""
        # GCé–¾å€¤èª¿æ•´ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«å¿œã˜ã¦ï¼‰
        current_memory = self.monitor.get_memory_usage()

        if current_memory['rss_mb'] > 200:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„å ´åˆã¯é »ç¹ã«GC
            gc.set_threshold(700, 10, 10)
        else:
            # é€šå¸¸æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            gc.set_threshold(700, 10, 10)

        # ä¸è¦ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ç„¡åŠ¹åŒ–
        gc.set_debug(0)

        print("ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–å®Œäº†")


class DataFrameOptimizer:
    """DataFrameæœ€é©åŒ–"""

    @staticmethod
    def optimize_dtypes(df) -> Any:
        """ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–"""
        try:
            import pandas as pd

            # æ•°å€¤å‹æœ€é©åŒ–
            for col in df.select_dtypes(include=['int64']).columns:
                if df[col].min() >= 0:
                    if df[col].max() < 255:
                        df[col] = df[col].astype('uint8')
                    elif df[col].max() < 65535:
                        df[col] = df[col].astype('uint16')
                    elif df[col].max() < 4294967295:
                        df[col] = df[col].astype('uint32')
                else:
                    if df[col].min() >= -128 and df[col].max() <= 127:
                        df[col] = df[col].astype('int8')
                    elif df[col].min() >= -32768 and df[col].max() <= 32767:
                        df[col] = df[col].astype('int16')
                    elif df[col].min() >= -2147483648 and df[col].max() <= 2147483647:
                        df[col] = df[col].astype('int32')

            # floatå‹æœ€é©åŒ–
            for col in df.select_dtypes(include=['float64']).columns:
                df[col] = pd.to_numeric(df[col], downcast='float')

            # ã‚«ãƒ†ã‚´ãƒªå‹æœ€é©åŒ–
            for col in df.select_dtypes(include=['object']).columns:
                if df[col].nunique() / len(df) < 0.5:  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ãŒ50%æœªæº€
                    df[col] = df[col].astype('category')

            return df

        except ImportError:
            return df

    @staticmethod
    def memory_usage_summary(df) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚µãƒãƒªãƒ¼"""
        try:
            memory_usage = df.memory_usage(deep=True)
            return {
                'total_mb': memory_usage.sum() / 1024 / 1024,
                'by_column': {
                    col: usage / 1024 / 1024
                    for col, usage in memory_usage.items()
                }
            }
        except:
            return {}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«æœ€é©åŒ–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
memory_optimizer = MemoryOptimizer()

def start_memory_monitoring():
    """ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹"""
    memory_optimizer.monitor.start_monitoring()
    memory_optimizer.optimize_gc()

def stop_memory_monitoring():
    """ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢"""
    memory_optimizer.monitor.stop_monitoring()

def get_memory_stats() -> Dict[str, Any]:
    """ãƒ¡ãƒ¢ãƒªçµ±è¨ˆå–å¾—"""
    return memory_optimizer.monitor.get_memory_usage()

# è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def auto_cleanup(func):
    """è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def wrapper(*args, **kwargs):
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            # é–¢æ•°çµ‚äº†æ™‚ã«ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            gc.collect()

    return wrapper
'''

        output_file = perf_dir / "memory_optimizer.py"
        output_file.write_text(memory_optimizer, encoding='utf-8')
        print("    ä½œæˆ: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")

        self.optimization_results['optimizations_applied'].append('memory_optimization')

    def _optimize_async_processing(self):
        """éåŒæœŸå‡¦ç†æœ€é©åŒ–"""
        print("5. éåŒæœŸå‡¦ç†æœ€é©åŒ–ä¸­...")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        perf_dir = self.base_dir / "src" / "day_trade" / "performance"
        perf_dir.mkdir(parents=True, exist_ok=True)

        async_optimizer = '''#!/usr/bin/env python3
"""
éåŒæœŸå‡¦ç†æœ€é©åŒ–

åŠ¹ç‡çš„ãªéåŒæœŸå‡¦ç†ã¨ã‚³ãƒ«ãƒ¼ãƒãƒ³ç®¡ç†
"""

import asyncio
import concurrent.futures
import threading
import time
from typing import Any, Awaitable, Callable, Dict, List, Optional
from functools import wraps


class AsyncTaskManager:
    """éåŒæœŸã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, max_concurrent: int = 10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.running_tasks = set()
        self.completed_tasks = []
        self.failed_tasks = []

    async def run_task(self, coro: Awaitable) -> Any:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        async with self.semaphore:
            task = asyncio.create_task(coro)
            self.running_tasks.add(task)

            try:
                result = await task
                self.completed_tasks.append(task)
                return result
            except Exception as e:
                self.failed_tasks.append((task, e))
                raise
            finally:
                self.running_tasks.discard(task)

    async def run_batch(self, coros: List[Awaitable]) -> List[Any]:
        """ãƒãƒƒãƒå®Ÿè¡Œ"""
        tasks = [self.run_task(coro) for coro in coros]
        return await asyncio.gather(*tasks, return_exceptions=True)

    def get_stats(self) -> Dict[str, int]:
        """çµ±è¨ˆå–å¾—"""
        return {
            'running': len(self.running_tasks),
            'completed': len(self.completed_tasks),
            'failed': len(self.failed_tasks),
            'max_concurrent': self.max_concurrent
        }


class HybridExecutor:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè¡Œå™¨ï¼ˆåŒæœŸãƒ»éåŒæœŸï¼‰"""

    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)
        self.process_pool = concurrent.futures.ProcessPoolExecutor(max_workers=max_workers)

    async def run_cpu_bound(self, func: Callable, *args, **kwargs) -> Any:
        """CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã‚’åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.process_pool, func, *args, **kwargs)

    async def run_io_bound(self, func: Callable, *args, **kwargs) -> Any:
        """I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.thread_pool, func, *args, **kwargs)

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)


class AsyncCache:
    """éåŒæœŸã‚­ãƒ£ãƒƒã‚·ãƒ¥"""

    def __init__(self, ttl: float = 3600):
        self.ttl = ttl
        self._cache = {}
        self._timestamps = {}
        self._locks = {}

    async def get_or_set(self, key: str, coro_func: Callable[[], Awaitable]) -> Any:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã¾ãŸã¯è¨­å®š"""
        # ã‚­ãƒ¼å°‚ç”¨ãƒ­ãƒƒã‚¯å–å¾—
        if key not in self._locks:
            self._locks[key] = asyncio.Lock()

        async with self._locks[key]:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if key in self._cache:
                timestamp = self._timestamps.get(key, 0)
                if time.time() - timestamp < self.ttl:
                    return self._cache[key]

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼šå€¤ã‚’è¨ˆç®—
            result = await coro_func()
            self._cache[key] = result
            self._timestamps[key] = time.time()
            return result

    def invalidate(self, key: str):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        self._cache.pop(key, None)
        self._timestamps.pop(key, None)


class RateLimiter:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å™¨"""

    def __init__(self, max_calls: int, time_window: float):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = []
        self.lock = asyncio.Lock()

    async def acquire(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯"""
        async with self.lock:
            now = time.time()

            # å¤ã„å‘¼ã³å‡ºã—è¨˜éŒ²ã‚’å‰Šé™¤
            self.calls = [call_time for call_time in self.calls
                         if now - call_time < self.time_window]

            # åˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self.calls) >= self.max_calls:
                # å¾…æ©Ÿæ™‚é–“è¨ˆç®—
                oldest_call = min(self.calls)
                wait_time = self.time_window - (now - oldest_call)
                if wait_time > 0:
                    await asyncio.sleep(wait_time)

            # ç¾åœ¨ã®å‘¼ã³å‡ºã—ã‚’è¨˜éŒ²
            self.calls.append(now)


def async_retry(max_retries: int = 3, delay: float = 1.0, backoff: float = 2.0):
    """éåŒæœŸãƒªãƒˆãƒ©ã‚¤ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            last_exception = None
            current_delay = delay

            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    last_exception = e
                    if attempt < max_retries:
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        raise last_exception

        return wrapper
    return decorator


def async_timeout(timeout: float):
    """éåŒæœŸã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            return await asyncio.wait_for(func(*args, **kwargs), timeout=timeout)
        return wrapper
    return decorator


class AsyncBatchProcessor:
    """éåŒæœŸãƒãƒƒãƒå‡¦ç†å™¨"""

    def __init__(self, batch_size: int = 100, max_wait: float = 1.0):
        self.batch_size = batch_size
        self.max_wait = max_wait
        self.queue = asyncio.Queue()
        self.processor_task = None
        self.running = False

    async def add_item(self, item: Any) -> Any:
        """ã‚¢ã‚¤ãƒ†ãƒ è¿½åŠ """
        future = asyncio.Future()
        await self.queue.put((item, future))
        return await future

    async def start_processing(self, processor_func: Callable):
        """å‡¦ç†é–‹å§‹"""
        self.running = True
        self.processor_task = asyncio.create_task(
            self._process_batches(processor_func)
        )

    async def stop_processing(self):
        """å‡¦ç†åœæ­¢"""
        self.running = False
        if self.processor_task:
            await self.processor_task

    async def _process_batches(self, processor_func: Callable):
        """ãƒãƒƒãƒå‡¦ç†ãƒ«ãƒ¼ãƒ—"""
        while self.running:
            batch = []
            futures = []

            # ãƒãƒƒãƒåé›†
            try:
                # æœ€åˆã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å¾…æ©Ÿ
                item, future = await asyncio.wait_for(
                    self.queue.get(), timeout=self.max_wait
                )
                batch.append(item)
                futures.append(future)

                # è¿½åŠ ã‚¢ã‚¤ãƒ†ãƒ ã‚’åé›†ï¼ˆãƒãƒ³ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ï¼‰
                while len(batch) < self.batch_size:
                    try:
                        item, future = self.queue.get_nowait()
                        batch.append(item)
                        futures.append(future)
                    except asyncio.QueueEmpty:
                        break

            except asyncio.TimeoutError:
                continue

            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            try:
                results = await processor_func(batch)

                # çµæœã‚’å¯¾å¿œã™ã‚‹Futureã«è¨­å®š
                for future, result in zip(futures, results):
                    if not future.cancelled():
                        future.set_result(result)

            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ã‚’å…¨Futureã«è¨­å®š
                for future in futures:
                    if not future.cancelled():
                        future.set_exception(e)


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
task_manager = AsyncTaskManager()
hybrid_executor = HybridExecutor()
async_cache = AsyncCache()

# ä¾¿åˆ©ãªé–¢æ•°
async def run_parallel(coros: List[Awaitable], max_concurrent: int = 10) -> List[Any]:
    """ä¸¦åˆ—å®Ÿè¡Œ"""
    semaphore = asyncio.Semaphore(max_concurrent)

    async def run_with_semaphore(coro):
        async with semaphore:
            return await coro

    tasks = [run_with_semaphore(coro) for coro in coros]
    return await asyncio.gather(*tasks, return_exceptions=True)

# ä½¿ç”¨ä¾‹:
# @async_retry(max_retries=3)
# @async_timeout(10.0)
# async def fetch_stock_data(symbol):
#     # APIå‘¼ã³å‡ºã—ç­‰
#     pass
'''

        output_file = perf_dir / "async_optimizer.py"
        output_file.write_text(async_optimizer, encoding='utf-8')
        print("    ä½œæˆ: éåŒæœŸå‡¦ç†æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")

        self.optimization_results['optimizations_applied'].append('async_optimization')

    def create_performance_integration(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ"""
        print("6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆä¸­...")

        integration_code = '''#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

å…¨ã¦ã®æœ€é©åŒ–æ©Ÿèƒ½ã‚’çµ±åˆã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import asyncio
import atexit
from typing import Dict, Any

from .lazy_imports import optimized_imports
from .optimized_cache import cache_manager
from .database_optimizer import get_db_manager
from .memory_optimizer import start_memory_monitoring, stop_memory_monitoring, get_memory_stats
from .async_optimizer import task_manager, hybrid_executor


class PerformanceManager:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç®¡ç†çµ±åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.initialized = False
        self.db_managers = {}

    def initialize(self, config: Dict[str, Any] = None):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–åˆæœŸåŒ–"""
        if self.initialized:
            return

        print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–åˆæœŸåŒ–ä¸­...")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        if config is None:
            config = {
                'memory_monitoring': True,
                'cache_enabled': True,
                'async_optimization': True,
                'db_optimization': True
            }

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        if config.get('memory_monitoring', True):
            start_memory_monitoring()
            print("  âœ… ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–
        if config.get('cache_enabled', True):
            cache_manager.clear_all()  # åˆæœŸåŒ–æ™‚ã«ã‚¯ãƒªã‚¢
            print("  âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç™»éŒ²
        atexit.register(self.cleanup)

        self.initialized = True
        print("ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Œäº†")

    def get_db_manager(self, db_path: str):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼å–å¾—"""
        if db_path not in self.db_managers:
            self.db_managers[db_path] = get_db_manager(db_path)
        return self.db_managers[db_path]

    def get_performance_stats(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—"""
        stats = {
            'timestamp': asyncio.get_event_loop().time(),
            'memory': get_memory_stats(),
            'cache': cache_manager.get_global_stats(),
            'async_tasks': task_manager.get_stats(),
        }

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ
        db_stats = {}
        for db_path, manager in self.db_managers.items():
            db_stats[db_path] = manager.get_performance_stats()
        stats['databases'] = db_stats

        return stats

    def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if not self.initialized:
            return

        print("ğŸ§¹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")

        # ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢
        stop_memory_monitoring()

        # éåŒæœŸãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        hybrid_executor.cleanup()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        cache_manager.clear_all()

        print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
performance_manager = PerformanceManager()

# ä¾¿åˆ©ãªé–¢æ•°
def initialize_performance(config: Dict[str, Any] = None):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–åˆæœŸåŒ–"""
    performance_manager.initialize(config)

def get_performance_stats() -> Dict[str, Any]:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—"""
    return performance_manager.get_performance_stats()

def get_optimized_db(db_path: str):
    """æœ€é©åŒ–ã•ã‚ŒãŸDBå–å¾—"""
    return performance_manager.get_db_manager(db_path)

# è‡ªå‹•åˆæœŸåŒ–ï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ï¼‰
def auto_initialize():
    """è‡ªå‹•åˆæœŸåŒ–"""
    import os
    if os.environ.get('DAY_TRADE_AUTO_OPTIMIZE', '1') == '1':
        initialize_performance()

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã«è‡ªå‹•å®Ÿè¡Œ
auto_initialize()
'''

        perf_dir = self.base_dir / "src" / "day_trade" / "performance"
        output_file = perf_dir / "__init__.py"
        output_file.write_text(integration_code, encoding='utf-8')
        print("    ä½œæˆ: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«")

        self.optimization_results['optimizations_applied'].append('performance_integration')

    def generate_optimization_report(self) -> str:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

å®Ÿè¡Œæ—¥æ™‚: {self.optimization_results['timestamp']}

## ğŸš€ é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–

"""

        optimizations = {
            'lazy_imports': 'é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ ',
            'optimized_cache': 'é«˜é€Ÿã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ',
            'database_optimization': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹æœ€é©åŒ–',
            'memory_optimization': 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–',
            'async_optimization': 'éåŒæœŸå‡¦ç†æœ€é©åŒ–',
            'performance_integration': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«'
        }

        for opt in self.optimization_results['optimizations_applied']:
            description = optimizations.get(opt, opt)
            report += f"âœ… {description}\n"

        report += f"""

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«ã‚ˆã‚ŠåˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’30-50%å‰Šæ¸›
- æœ€é©åŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
- DataFrameã®å‹æœ€é©åŒ–ã«ã‚ˆã‚Š50-70%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›

### å‡¦ç†é€Ÿåº¦
- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã«ã‚ˆã‚Š20-40%ã®é«˜é€ŸåŒ–
- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚Šå¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŒ10å€é«˜é€ŸåŒ–
- éåŒæœŸå‡¦ç†ã«ã‚ˆã‚Šä¸¦åˆ—åº¦ãŒå‘ä¸Š

### ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§
- ãƒ¡ãƒ¢ãƒªç›£è¦–ã«ã‚ˆã‚‹è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- æ¥ç¶šãƒ—ãƒ¼ãƒ«ã«ã‚ˆã‚‹ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡é˜²æ­¢
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬çš„ãªä½¿ç”¨
```python
from src.day_trade.performance import initialize_performance, get_performance_stats

# åˆæœŸåŒ–
initialize_performance()

# çµ±è¨ˆç¢ºèª
stats = get_performance_stats()
print(stats)
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨
```python
from src.day_trade.performance.optimized_cache import cached

@cached('stock_data', ttl=1800)  # 30åˆ†ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def get_stock_price(symbol):
    return fetch_stock_price(symbol)
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–
```python
from src.day_trade.performance import get_optimized_db

db = get_optimized_db('data/trading.db')
with db.pool.connection() as conn:
    # æœ€é©åŒ–ã•ã‚ŒãŸæ¥ç¶šã‚’ä½¿ç”¨
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM stocks")
```

## ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. å®Ÿéš›ã®é‹ç”¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
2. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®šã¨è¿½åŠ æœ€é©åŒ–
3. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç›£è¦–ã®ç¶™ç¶š
4. å®šæœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ“ãƒ¥ãƒ¼

"""

        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Ÿè¡Œ")
    print("=" * 50)

    base_dir = Path(__file__).parent
    optimizer = PerformanceOptimizer(base_dir)

    # æœ€é©åŒ–å®Ÿè¡Œ
    optimizer.optimize_performance()

    # çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½œæˆ
    optimizer.create_performance_integration()

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = optimizer.generate_optimization_report()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = base_dir / f"performance_optimization_report_{timestamp}.md"
    report_file.write_text(report, encoding='utf-8')

    print(f"\nğŸ“„ æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")

    print("\n" + "=" * 50)
    print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Œäº†")
    print("=" * 50)
    print("é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–:")
    for opt in optimizer.optimization_results['optimizations_applied']:
        print(f"  âœ… {opt}")
    print("=" * 50)


if __name__ == "__main__":
    main()