#!/usr/bin/env python3
"""
APIçµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
Issue #331: APIãƒ»å¤–éƒ¨çµ±åˆã‚·ã‚¹ãƒ†ãƒ  - Phase 3

RESTfulãƒ»WebSocket APIçµ±åˆç®¡ç†ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±ä¸€ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- ãƒãƒ«ãƒã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿çµ±åˆ
- è‡ªå‹•ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼
- çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
"""

import asyncio
import hashlib
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

try:
    from ..utils.logging_config import get_context_logger
    from .external_api_client import APIProvider as RestAPIProvider
    from .external_api_client import APIResponse, ExternalAPIClient
    from .websocket_streaming_client import (
        StreamMessage,
        StreamProvider,
        StreamType,
        WebSocketStreamingClient,
    )
except ImportError:
    # ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚„ä¸€éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    ExternalAPIClient = None
    RestAPIProvider = None
    APIResponse = None
    WebSocketStreamingClient = None
    StreamProvider = None
    StreamMessage = None
    StreamType = None

    import logging

    def get_context_logger(name):
        return logging.getLogger(name)


logger = get_context_logger(__name__)


class DataSource(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹"""

    REST_API = "rest_api"
    WEBSOCKET_STREAM = "websocket_stream"
    CACHE = "cache"
    FALLBACK = "fallback"


class DataQuality(Enum):
    """ãƒ‡ãƒ¼ã‚¿å“è³ª"""

    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    UNKNOWN = "unknown"


class DataFreshness(Enum):
    """ãƒ‡ãƒ¼ã‚¿é®®åº¦"""

    REAL_TIME = "real_time"  # <5ç§’
    FRESH = "fresh"  # <1åˆ†
    RECENT = "recent"  # <5åˆ†
    STALE = "stale"  # <15åˆ†
    EXPIRED = "expired"  # >15åˆ†
    UNKNOWN = "unknown"  # ä¸æ˜


@dataclass
class UnifiedMarketData:
    """çµ±åˆå¸‚å ´ãƒ‡ãƒ¼ã‚¿"""

    symbol: str
    data_type: str
    timestamp: datetime

    # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
    open_price: Optional[float] = None
    high_price: Optional[float] = None
    low_price: Optional[float] = None
    close_price: Optional[float] = None
    volume: Optional[int] = None

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    source: DataSource = DataSource.REST_API
    provider: str = "unknown"
    quality: DataQuality = DataQuality.UNKNOWN
    freshness: DataFreshness = DataFreshness.UNKNOWN
    confidence_score: float = 0.0

    # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿
    additional_data: Dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼å¤‰æ›"""
        return {
            "symbol": self.symbol,
            "data_type": self.data_type,
            "timestamp": self.timestamp.isoformat(),
            "open_price": self.open_price,
            "high_price": self.high_price,
            "low_price": self.low_price,
            "close_price": self.close_price,
            "volume": self.volume,
            "source": self.source.value,
            "provider": self.provider,
            "quality": self.quality.value,
            "freshness": self.freshness.value,
            "confidence_score": self.confidence_score,
            **self.additional_data,
        }

    def to_pandas_row(self) -> Dict[str, Any]:
        """Pandasè¡Œå½¢å¼å¤‰æ›"""
        return {
            "Symbol": self.symbol,
            "Timestamp": self.timestamp,
            "Open": self.open_price,
            "High": self.high_price,
            "Low": self.low_price,
            "Close": self.close_price,
            "Volume": self.volume,
            "Source": self.source.value,
            "Provider": self.provider,
            "Quality": self.quality.value,
            "Confidence": self.confidence_score,
        }


@dataclass
class DataSourceConfig:
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š"""

    source_type: DataSource
    provider: str
    enabled: bool = True
    priority: int = 1  # 1=æœ€é«˜å„ªå…ˆåº¦

    # å“è³ªè¨­å®š
    min_confidence_score: float = 0.5
    max_staleness_minutes: int = 15

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
    timeout_seconds: int = 30
    retry_attempts: int = 3

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
    fallback_providers: List[str] = field(default_factory=list)


@dataclass
class CacheEntry:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒª"""

    key: str
    data: UnifiedMarketData
    created_at: datetime
    ttl_seconds: int
    access_count: int = 0
    last_access: Optional[datetime] = None

    def is_expired(self) -> bool:
        """æœŸé™åˆ‡ã‚Œåˆ¤å®š"""
        return (datetime.now() - self.created_at).total_seconds() > self.ttl_seconds

    def access(self) -> None:
        """ã‚¢ã‚¯ã‚»ã‚¹è¨˜éŒ²"""
        self.access_count += 1
        self.last_access = datetime.now()


@dataclass
class IntegrationConfig:
    """çµ±åˆè¨­å®š"""

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å„ªå…ˆåº¦
    default_source_priority: List[DataSource] = field(
        default_factory=lambda: [
            DataSource.WEBSOCKET_STREAM,
            DataSource.REST_API,
            DataSource.CACHE,
            DataSource.FALLBACK,
        ]
    )

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
    enable_intelligent_caching: bool = True
    cache_size_limit: int = 10000
    default_cache_ttl_seconds: int = 300  # 5åˆ†

    # å“è³ªç®¡ç†è¨­å®š
    enable_data_quality_scoring: bool = True
    min_acceptable_quality: DataQuality = DataQuality.ACCEPTABLE
    enable_data_validation: bool = True

    # ãƒ•ã‚§ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼è¨­å®š
    enable_automatic_fallback: bool = True
    max_fallback_attempts: int = 3
    fallback_delay_seconds: float = 1.0

    # çµ±åˆè¨­å®š
    enable_data_fusion: bool = True  # è¤‡æ•°ã‚½ãƒ¼ã‚¹çµ±åˆ
    fusion_confidence_threshold: float = 0.8
    max_concurrent_requests: int = 10


class APIIntegrationManager:
    """APIçµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, config: Optional[IntegrationConfig] = None):
        self.config = config or IntegrationConfig()

        # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        self.rest_client: Optional[ExternalAPIClient] = None
        self.websocket_client: Optional[WebSocketStreamingClient] = None

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
        self.data_sources: Dict[str, DataSourceConfig] = {}

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.cache: Dict[str, CacheEntry] = {}
        self.cache_stats = {"hits": 0, "misses": 0, "evictions": 0, "total_size": 0}

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
        self.streaming_subscriptions: Dict[str, str] = {}  # symbol -> subscription_id
        self.real_time_data: Dict[str, UnifiedMarketData] = {}

        # çµ±è¨ˆæƒ…å ±
        self.integration_stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "fallback_used": 0,
            "data_quality_issues": 0,
            "fusion_operations": 0,
        }

        # åˆ¶å¾¡ãƒ•ãƒ©ã‚°
        self._initialized = False
        self.semaphore = asyncio.Semaphore(self.config.max_concurrent_requests)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self._setup_default_data_sources()

    def _setup_default_data_sources(self) -> None:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š"""
        default_sources = [
            DataSourceConfig(
                source_type=DataSource.WEBSOCKET_STREAM,
                provider="mock_stream",
                priority=1,
                max_staleness_minutes=1,
                timeout_seconds=10,
            ),
            DataSourceConfig(
                source_type=DataSource.REST_API,
                provider="mock_provider",
                priority=2,
                max_staleness_minutes=5,
                timeout_seconds=30,
                fallback_providers=["yahoo_finance", "alpha_vantage"],
            ),
            DataSourceConfig(
                source_type=DataSource.CACHE,
                provider="intelligent_cache",
                priority=3,
                max_staleness_minutes=15,
            ),
        ]

        for source in default_sources:
            self.register_data_source(source)

    def register_data_source(self, source_config: DataSourceConfig) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç™»éŒ²"""
        source_key = f"{source_config.source_type.value}_{source_config.provider}"
        self.data_sources[source_key] = source_config
        logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç™»éŒ²: {source_key}")

    async def initialize(self) -> None:
        """çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–"""
        if self._initialized:
            return

        try:
            # RESTã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.rest_client = ExternalAPIClient()
            await self.rest_client.initialize()

            # WebSocketã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            self.websocket_client = WebSocketStreamingClient()
            await self.websocket_client.start_streaming()

            self._initialized = True
            logger.info("APIçµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–å®Œäº†")

        except Exception as e:
            logger.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def cleanup(self) -> None:
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.rest_client:
            await self.rest_client.cleanup()

        if self.websocket_client:
            await self.websocket_client.stop_streaming()

        self._initialized = False
        logger.info("APIçµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

    async def get_market_data(
        self,
        symbol: str,
        data_type: str = "stock_price",
        prefer_real_time: bool = True,
        max_staleness_minutes: int = 5,
        **kwargs,
    ) -> Optional[UnifiedMarketData]:
        """çµ±åˆå¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if not self._initialized:
            await self.initialize()

        async with self.semaphore:
            self.integration_stats["total_requests"] += 1

            try:
                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å„ªå…ˆåº¦æ±ºå®š
                sources = self._determine_data_sources(
                    symbol, data_type, prefer_real_time, max_staleness_minutes
                )

                # å„ªå…ˆåº¦é †ã«ãƒ‡ãƒ¼ã‚¿å–å¾—è©¦è¡Œ
                for source_info in sources:
                    data = await self._fetch_from_source(
                        symbol, data_type, source_info, **kwargs
                    )

                    if data and self._validate_data_quality(data):
                        # æˆåŠŸæ™‚ã®å‡¦ç†
                        self.integration_stats["successful_requests"] += 1

                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
                        if self.config.enable_intelligent_caching:
                            await self._cache_data(data)

                        return data

                # å…¨ã‚½ãƒ¼ã‚¹å¤±æ•—æ™‚ã®å‡¦ç†
                self.integration_stats["failed_requests"] += 1
                logger.warning(f"å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—å¤±æ•—: {symbol}")

                return None

            except Exception as e:
                self.integration_stats["failed_requests"] += 1
                logger.error(f"çµ±åˆãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
                return None

    def _determine_data_sources(
        self,
        symbol: str,
        data_type: str,
        prefer_real_time: bool,
        max_staleness_minutes: int,
    ) -> List[Tuple[DataSource, DataSourceConfig]]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å„ªå…ˆåº¦æ±ºå®š"""

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        real_time_available = symbol in self.real_time_data

        sources = []

        if prefer_real_time and real_time_available:
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å„ªå…ˆ
            for source_key, config in self.data_sources.items():
                if (
                    config.source_type == DataSource.WEBSOCKET_STREAM
                    and config.enabled
                    and config.max_staleness_minutes <= max_staleness_minutes
                ):
                    sources.append((config.source_type, config))

        # æ¨™æº–å„ªå…ˆåº¦é †è¿½åŠ 
        for source_type in self.config.default_source_priority:
            for source_key, config in self.data_sources.items():
                if (
                    config.source_type == source_type
                    and config.enabled
                    and config.max_staleness_minutes <= max_staleness_minutes
                    and (source_type, config) not in sources
                ):
                    sources.append((source_type, config))

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        sources.sort(key=lambda x: x[1].priority)

        return sources

    async def _fetch_from_source(
        self,
        symbol: str,
        data_type: str,
        source_info: Tuple[DataSource, DataSourceConfig],
        **kwargs,
    ) -> Optional[UnifiedMarketData]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—"""
        source_type, config = source_info

        try:
            if source_type == DataSource.WEBSOCKET_STREAM:
                return await self._fetch_from_websocket(symbol, data_type, config)
            elif source_type == DataSource.REST_API:
                return await self._fetch_from_rest_api(
                    symbol, data_type, config, **kwargs
                )
            elif source_type == DataSource.CACHE:
                return await self._fetch_from_cache(symbol, data_type, config)
            else:
                return None

        except Exception as e:
            logger.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼ {source_type.value}: {e}")
            return None

    async def _fetch_from_websocket(
        self, symbol: str, data_type: str, config: DataSourceConfig
    ) -> Optional[UnifiedMarketData]:
        """WebSocketã‹ã‚‰å–å¾—"""
        if symbol in self.real_time_data:
            real_time = self.real_time_data[symbol]

            # é®®åº¦ãƒã‚§ãƒƒã‚¯
            age_seconds = (datetime.now() - real_time.timestamp).total_seconds()
            if age_seconds <= config.max_staleness_minutes * 60:
                # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
                quality = self._evaluate_data_quality(real_time, age_seconds)
                real_time.quality = quality
                real_time.freshness = self._evaluate_data_freshness(age_seconds)
                real_time.source = DataSource.WEBSOCKET_STREAM

                return real_time

        return None

    async def _fetch_from_rest_api(
        self, symbol: str, data_type: str, config: DataSourceConfig, **kwargs
    ) -> Optional[UnifiedMarketData]:
        """REST APIã‹ã‚‰å–å¾—"""
        if not self.rest_client:
            return None

        try:
            # APIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ±ºå®š
            if config.provider == "mock_provider":
                api_provider = RestAPIProvider.MOCK_PROVIDER
            elif config.provider == "yahoo_finance":
                api_provider = RestAPIProvider.YAHOO_FINANCE
            elif config.provider == "alpha_vantage":
                api_provider = RestAPIProvider.ALPHA_VANTAGE
            else:
                api_provider = RestAPIProvider.MOCK_PROVIDER

            # APIå‘¼ã³å‡ºã—
            response = await self.rest_client.fetch_stock_data(
                symbol, api_provider, **kwargs
            )

            if response and response.success and response.normalized_data is not None:
                # çµ±åˆãƒ‡ãƒ¼ã‚¿å¤‰æ›
                unified_data = await self._convert_to_unified_data(
                    symbol, data_type, response, DataSource.REST_API, config.provider
                )

                return unified_data

        except Exception as e:
            logger.error(f"REST APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        return None

    async def _fetch_from_cache(
        self, symbol: str, data_type: str, config: DataSourceConfig
    ) -> Optional[UnifiedMarketData]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        cache_key = self._generate_cache_key(symbol, data_type)

        if cache_key in self.cache:
            entry = self.cache[cache_key]

            if not entry.is_expired():
                entry.access()
                self.cache_stats["hits"] += 1

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã®é®®åº¦ãƒ»å“è³ªæ›´æ–°
                cached_data = entry.data
                age_seconds = (datetime.now() - cached_data.timestamp).total_seconds()

                cached_data.freshness = self._evaluate_data_freshness(age_seconds)
                cached_data.source = DataSource.CACHE

                return cached_data
            else:
                # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤
                del self.cache[cache_key]

        self.cache_stats["misses"] += 1
        return None

    async def _convert_to_unified_data(
        self,
        symbol: str,
        data_type: str,
        response: APIResponse,
        source: DataSource,
        provider: str,
    ) -> UnifiedMarketData:
        """çµ±åˆãƒ‡ãƒ¼ã‚¿å¤‰æ›"""

        df = response.normalized_data

        if df is not None and not df.empty:
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            latest_row = df.iloc[-1]

            unified_data = UnifiedMarketData(
                symbol=symbol,
                data_type=data_type,
                timestamp=latest_row.get("timestamp", datetime.now()),
                open_price=latest_row.get("open"),
                high_price=latest_row.get("high"),
                low_price=latest_row.get("low"),
                close_price=latest_row.get("close"),
                volume=latest_row.get("volume"),
                source=source,
                provider=provider,
                confidence_score=0.8,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¿¡é ¼åº¦
            )

            # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
            age_seconds = (datetime.now() - unified_data.timestamp).total_seconds()
            unified_data.quality = self._evaluate_data_quality(
                unified_data, age_seconds
            )
            unified_data.freshness = self._evaluate_data_freshness(age_seconds)

            return unified_data

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
        return UnifiedMarketData(
            symbol=symbol,
            data_type=data_type,
            timestamp=datetime.now(),
            source=source,
            provider=provider,
            quality=DataQuality.POOR,
            freshness=DataFreshness.UNKNOWN,
        )

    def _evaluate_data_quality(
        self, data: UnifiedMarketData, age_seconds: float
    ) -> DataQuality:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""
        if not self.config.enable_data_quality_scoring:
            return DataQuality.UNKNOWN

        quality_score = 0.0

        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        price_completeness = (
            sum(
                [
                    1 if data.open_price is not None else 0,
                    1 if data.high_price is not None else 0,
                    1 if data.low_price is not None else 0,
                    1 if data.close_price is not None else 0,
                ]
            )
            / 4.0
        )
        quality_score += price_completeness * 0.4

        # ãƒ‡ãƒ¼ã‚¿é®®åº¦ã‚¹ã‚³ã‚¢
        if age_seconds < 60:  # 1åˆ†ä»¥å†…
            freshness_score = 1.0
        elif age_seconds < 300:  # 5åˆ†ä»¥å†…
            freshness_score = 0.8
        elif age_seconds < 900:  # 15åˆ†ä»¥å†…
            freshness_score = 0.6
        else:
            freshness_score = 0.3
        quality_score += freshness_score * 0.3

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä¿¡é ¼åº¦
        if data.source == DataSource.WEBSOCKET_STREAM:
            source_score = 1.0
        elif data.source == DataSource.REST_API:
            source_score = 0.9
        elif data.source == DataSource.CACHE:
            source_score = 0.7
        else:
            source_score = 0.5
        quality_score += source_score * 0.3

        # å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š
        if quality_score >= 0.9:
            return DataQuality.EXCELLENT
        elif quality_score >= 0.7:
            return DataQuality.GOOD
        elif quality_score >= 0.5:
            return DataQuality.ACCEPTABLE
        else:
            return DataQuality.POOR

    def _evaluate_data_freshness(self, age_seconds: float) -> DataFreshness:
        """ãƒ‡ãƒ¼ã‚¿é®®åº¦è©•ä¾¡"""
        if age_seconds < 5:
            return DataFreshness.REAL_TIME
        elif age_seconds < 60:
            return DataFreshness.FRESH
        elif age_seconds < 300:
            return DataFreshness.RECENT
        elif age_seconds < 900:
            return DataFreshness.STALE
        else:
            return DataFreshness.EXPIRED

    def _validate_data_quality(self, data: UnifiedMarketData) -> bool:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼"""
        if not self.config.enable_data_validation:
            return True

        # æœ€ä½å“è³ªè¦ä»¶ãƒã‚§ãƒƒã‚¯
        if data.quality.value < self.config.min_acceptable_quality.value:
            self.integration_stats["data_quality_issues"] += 1
            return False

        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if data.close_price is not None and data.close_price <= 0:
            return False

        if data.volume is not None and data.volume < 0:
            return False

        return True

    async def _cache_data(self, data: UnifiedMarketData) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        cache_key = self._generate_cache_key(data.symbol, data.data_type)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.cache) >= self.config.cache_size_limit:
            await self._evict_cache_entries()

        # TTLæ±ºå®šï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªã«åŸºã¥ãï¼‰
        if data.quality == DataQuality.EXCELLENT:
            ttl = self.config.default_cache_ttl_seconds
        elif data.quality == DataQuality.GOOD:
            ttl = self.config.default_cache_ttl_seconds // 2
        else:
            ttl = self.config.default_cache_ttl_seconds // 4

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
        entry = CacheEntry(
            key=cache_key, data=data, created_at=datetime.now(), ttl_seconds=ttl
        )

        self.cache[cache_key] = entry
        self.cache_stats["total_size"] = len(self.cache)

    async def _evict_cache_entries(self) -> None:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³"""
        if not self.cache:
            return

        # LRUæ–¹å¼ã§ã‚¨ãƒ“ã‚¯ã‚·ãƒ§ãƒ³
        sorted_entries = sorted(
            self.cache.items(), key=lambda x: x[1].last_access or x[1].created_at
        )

        # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’10%å‰Šé™¤
        evict_count = max(1, len(self.cache) // 10)

        for i in range(evict_count):
            cache_key, entry = sorted_entries[i]
            del self.cache[cache_key]
            self.cache_stats["evictions"] += 1

        self.cache_stats["total_size"] = len(self.cache)

    def _generate_cache_key(self, symbol: str, data_type: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_string = f"{symbol}_{data_type}"
        return hashlib.md5(key_string.encode()).hexdigest()

    async def start_real_time_streaming(self, symbols: List[str]) -> Dict[str, str]:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹"""
        if not self.websocket_client:
            await self.initialize()

        subscription_ids = {}

        for symbol in symbols:
            try:
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
                def create_callback(sym):
                    def callback(message: StreamMessage):
                        # UnifiedMarketDataã«å¤‰æ›
                        unified_data = UnifiedMarketData(
                            symbol=sym,
                            data_type="real_time_price",
                            timestamp=message.timestamp,
                            close_price=message.data.get("price"),
                            volume=message.data.get("volume"),
                            source=DataSource.WEBSOCKET_STREAM,
                            provider=message.provider.value,
                            quality=DataQuality.GOOD,
                            freshness=DataFreshness.REAL_TIME,
                            confidence_score=0.9,
                        )

                        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿æ›´æ–°
                        self.real_time_data[sym] = unified_data

                    return callback

                # è³¼èª­é–‹å§‹
                subscription_id = await self.websocket_client.subscribe(
                    provider=StreamProvider.MOCK_STREAM,
                    stream_type=StreamType.REAL_TIME_QUOTES,
                    symbols=[symbol],
                    callback=create_callback(symbol),
                )

                subscription_ids[symbol] = subscription_id
                self.streaming_subscriptions[symbol] = subscription_id

                logger.info(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹: {symbol}")

            except Exception as e:
                logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")

        return subscription_ids

    async def stop_real_time_streaming(
        self, symbols: Optional[List[str]] = None
    ) -> None:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœæ­¢"""
        if not self.websocket_client:
            return

        target_symbols = symbols or list(self.streaming_subscriptions.keys())

        for symbol in target_symbols:
            if symbol in self.streaming_subscriptions:
                subscription_id = self.streaming_subscriptions[symbol]

                try:
                    await self.websocket_client.unsubscribe(subscription_id)
                    del self.streaming_subscriptions[symbol]

                    if symbol in self.real_time_data:
                        del self.real_time_data[symbol]

                    logger.info(f"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœæ­¢: {symbol}")

                except Exception as e:
                    logger.error(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°åœæ­¢ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")

    async def get_multiple_market_data(
        self,
        symbols: List[str],
        data_type: str = "stock_price",
        prefer_real_time: bool = True,
    ) -> Dict[str, UnifiedMarketData]:
        """è¤‡æ•°éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        tasks = []
        for symbol in symbols:
            task = asyncio.create_task(
                self.get_market_data(symbol, data_type, prefer_real_time)
            )
            tasks.append((symbol, task))

        results = {}
        for symbol, task in tasks:
            try:
                data = await task
                if data:
                    results[symbol] = data
            except Exception as e:
                logger.error(f"è¤‡æ•°ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")

        return results

    def get_integration_statistics(self) -> Dict[str, Any]:
        """çµ±åˆçµ±è¨ˆæƒ…å ±å–å¾—"""
        success_rate = (
            (
                self.integration_stats["successful_requests"]
                / self.integration_stats["total_requests"]
                * 100
            )
            if self.integration_stats["total_requests"] > 0
            else 0
        )

        cache_hit_rate = (
            (
                self.cache_stats["hits"]
                / (self.cache_stats["hits"] + self.cache_stats["misses"])
                * 100
            )
            if (self.cache_stats["hits"] + self.cache_stats["misses"]) > 0
            else 0
        )

        return {
            "integration_stats": {
                **self.integration_stats,
                "success_rate_percent": success_rate,
            },
            "cache_stats": {**self.cache_stats, "hit_rate_percent": cache_hit_rate},
            "streaming_stats": {
                "active_subscriptions": len(self.streaming_subscriptions),
                "real_time_symbols": len(self.real_time_data),
            },
            "system_status": {
                "initialized": self._initialized,
                "data_sources_registered": len(self.data_sources),
                "timestamp": datetime.now().isoformat(),
            },
        }

    def get_data_quality_report(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ"""
        quality_distribution = {quality.value: 0 for quality in DataQuality}
        freshness_distribution = {freshness.value: 0 for freshness in DataFreshness}

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿åˆ†æ
        total_cached = len(self.cache)
        for entry in self.cache.values():
            quality_distribution[entry.data.quality.value] += 1
            freshness_distribution[entry.data.freshness.value] += 1

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿åˆ†æ
        total_real_time = len(self.real_time_data)
        for data in self.real_time_data.values():
            quality_distribution[data.quality.value] += 1
            freshness_distribution[data.freshness.value] += 1

        return {
            "total_data_points": total_cached + total_real_time,
            "cached_data_points": total_cached,
            "real_time_data_points": total_real_time,
            "quality_distribution": quality_distribution,
            "freshness_distribution": freshness_distribution,
            "data_quality_issues": self.integration_stats["data_quality_issues"],
            "timestamp": datetime.now().isoformat(),
        }

    async def health_check(self) -> Dict[str, Any]:
        """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        rest_health = "unknown"
        websocket_health = "unknown"

        try:
            if self.rest_client:
                rest_status = await self.rest_client.health_check()
                rest_health = (
                    "healthy" if rest_status.get("session_active") else "unhealthy"
                )
        except Exception:
            rest_health = "unhealthy"

        try:
            if self.websocket_client:
                ws_status = await self.websocket_client.health_check()
                websocket_health = ws_status.get("status", "unknown")
        except Exception:
            websocket_health = "unhealthy"

        overall_status = (
            "healthy"
            if (rest_health == "healthy" and websocket_health == "healthy")
            else "degraded"
        )

        return {
            "overall_status": overall_status,
            "rest_api_status": rest_health,
            "websocket_status": websocket_health,
            "cache_size": len(self.cache),
            "streaming_subscriptions": len(self.streaming_subscriptions),
            "initialized": self._initialized,
            "timestamp": datetime.now().isoformat(),
        }


# ä½¿ç”¨ä¾‹ãƒ»ãƒ†ã‚¹ãƒˆé–¢æ•°


async def setup_integration_manager() -> APIIntegrationManager:
    """çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    config = IntegrationConfig(
        enable_intelligent_caching=True,
        enable_data_quality_scoring=True,
        enable_automatic_fallback=True,
    )

    manager = APIIntegrationManager(config)
    await manager.initialize()

    return manager


async def test_integrated_market_data():
    """çµ±åˆå¸‚å ´ãƒ‡ãƒ¼ã‚¿ãƒ†ã‚¹ãƒˆ"""
    manager = await setup_integration_manager()

    try:
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹
        symbols = ["7203", "8306", "9984"]  # ãƒˆãƒ¨ã‚¿ã€ä¸‰è±UFJã€SBG

        print("ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é–‹å§‹...")
        await manager.start_real_time_streaming(symbols)

        # å°‘ã—å¾…æ©Ÿã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å—ä¿¡
        await asyncio.sleep(3)

        # çµ±åˆãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ†ã‚¹ãƒˆ
        print("\nçµ±åˆå¸‚å ´ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ†ã‚¹ãƒˆ:")
        for symbol in symbols:
            data = await manager.get_market_data(symbol, prefer_real_time=True)

            if data:
                print(
                    f"  {symbol}: Â¥{data.close_price:.2f} "
                    f"[{data.source.value}] {data.quality.value} {data.freshness.value}"
                )
            else:
                print(f"  {symbol}: ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—")

        # è¤‡æ•°éŠ˜æŸ„ä¸€æ‹¬å–å¾—ãƒ†ã‚¹ãƒˆ
        print("\nè¤‡æ•°éŠ˜æŸ„ä¸€æ‹¬å–å¾—ãƒ†ã‚¹ãƒˆ:")
        multiple_data = await manager.get_multiple_market_data(symbols)

        for symbol, data in multiple_data.items():
            print(
                f"  {symbol}: Â¥{data.close_price:.2f} ä¿¡é ¼åº¦{data.confidence_score:.2f}"
            )

        # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
        stats = manager.get_integration_statistics()
        print("\nğŸ“Š çµ±åˆçµ±è¨ˆ:")
        print(f"  ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {stats['integration_stats']['total_requests']}")
        print(f"  æˆåŠŸç‡: {stats['integration_stats']['success_rate_percent']:.1f}%")
        print(f"  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {stats['cache_stats']['hit_rate_percent']:.1f}%")
        print(
            f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: {stats['streaming_stats']['active_subscriptions']}"
        )

        # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
        quality_report = manager.get_data_quality_report()
        print("\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿å“è³ª:")
        print(f"  ç·ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ: {quality_report['total_data_points']}")
        print(f"  å“è³ªåˆ†å¸ƒ: {quality_report['quality_distribution']}")

    finally:
        await manager.stop_real_time_streaming()
        await manager.cleanup()


if __name__ == "__main__":
    asyncio.run(test_integrated_market_data())
