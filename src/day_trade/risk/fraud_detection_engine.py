#!/usr/bin/env python3
"""
Deep Learning Fraud Detection Engine
æ·±å±¤å­¦ç¿’ä¸æ­£æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³

LSTM + Transformer + Ensemble ã«ã‚ˆã‚‹é«˜ç²¾åº¦ä¸æ­£å–å¼•æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
å¹´é–“10å„„å††è¦æ¨¡ã®æå¤±é˜²æ­¢ã‚’ç›®æ¨™ã¨ã™ã‚‹æ¬¡ä¸–ä»£æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
"""

import pickle
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np

# æ·±å±¤å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from sklearn.ensemble import IsolationForest
    from sklearn.metrics import classification_report, roc_auc_score
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from torch.utils.data import DataLoader, TensorDataset

    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..utils.logging_config import get_context_logger

logger = get_context_logger(__name__)


@dataclass
class FraudDetectionRequest:
    """ä¸æ­£æ¤œçŸ¥ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""

    transaction_id: str
    user_id: str
    amount: float
    timestamp: datetime
    transaction_type: str
    account_balance: float
    location: str
    device_info: Dict[str, Any]
    transaction_history: List[Dict[str, Any]]
    market_conditions: Dict[str, Any]


@dataclass
class FraudDetectionResult:
    """ä¸æ­£æ¤œçŸ¥çµæœ"""

    transaction_id: str
    is_fraud: bool
    fraud_probability: float  # 0-1
    confidence: float  # 0-1
    risk_factors: Dict[str, float]
    anomaly_score: float
    explanation: str
    recommended_action: str
    models_used: List[str]
    processing_time: float
    timestamp: datetime


class LSTMFraudModel(nn.Module):
    """LSTMä¸æ­£æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«"""

    def __init__(
        self,
        input_size: int = 50,
        hidden_size: int = 128,
        num_layers: int = 3,
        dropout: float = 0.2,
    ):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±¤
        self.lstm = nn.LSTM(
            input_size,
            hidden_size,
            num_layers,
            batch_first=True,
            dropout=dropout,
            bidirectional=True,
        )

        # Attention ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
        self.attention = nn.MultiheadAttention(
            hidden_size * 2, num_heads=8, dropout=dropout
        )

        # åˆ†é¡å±¤
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 32),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(32, 2),  # Normal(0) / Fraud(1)
        )

    def forward(self, x):
        # LSTMå‡¦ç†
        lstm_out, _ = self.lstm(x)

        # Attentioné©ç”¨
        attn_out, attention_weights = self.attention(lstm_out, lstm_out, lstm_out)

        # æœ€çµ‚æ™‚åˆ»ã®å‡ºåŠ›ã‚’ä½¿ç”¨
        final_output = attn_out[:, -1, :]

        # åˆ†é¡
        logits = self.classifier(final_output)
        probabilities = F.softmax(logits, dim=1)

        return probabilities, attention_weights


class TransformerAnomalyModel(nn.Module):
    """Transformerç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«"""

    def __init__(
        self,
        input_dim: int = 50,
        model_dim: int = 256,
        num_heads: int = 8,
        num_layers: int = 6,
        dropout: float = 0.1,
    ):
        super().__init__()
        self.model_dim = model_dim

        # å…¥åŠ›åŸ‹ã‚è¾¼ã¿
        self.input_projection = nn.Linear(input_dim, model_dim)
        self.positional_encoding = self._create_positional_encoding(1000, model_dim)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=model_dim * 4,
            dropout=dropout,
            activation="relu",
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)

        # ç•°å¸¸ã‚¹ã‚³ã‚¢å‡ºåŠ›
        self.anomaly_head = nn.Sequential(
            nn.Linear(model_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 1),
            nn.Sigmoid(),
        )

    def _create_positional_encoding(self, max_len: int, model_dim: int):
        pe = torch.zeros(max_len, model_dim)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        div_term = torch.exp(
            torch.arange(0, model_dim, 2).float() * (-np.log(10000.0) / model_dim)
        )

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        return pe.unsqueeze(0)

    def forward(self, x):
        batch_size, seq_len = x.shape[:2]

        # å…¥åŠ›æŠ•å½± + ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = self.input_projection(x)
        x += self.positional_encoding[:, :seq_len, :].to(x.device)

        # Transformerå‡¦ç† (seq_len, batch, model_dim)
        x = x.transpose(0, 1)
        transformer_out = self.transformer(x)

        # ç•°å¸¸ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæœ€çµ‚æ™‚åˆ»ï¼‰
        final_output = transformer_out[-1]  # (batch, model_dim)
        anomaly_score = self.anomaly_head(final_output)

        return anomaly_score.squeeze(-1)


class EnsembleFraudDetector:
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸æ­£æ¤œçŸ¥å™¨"""

    def __init__(self):
        self.models = {"lstm": None, "transformer": None, "isolation_forest": None}
        self.weights = {"lstm": 0.4, "transformer": 0.4, "isolation_forest": 0.2}
        self.scaler = StandardScaler()
        self.is_fitted = False

    def fit(self, X_train: np.ndarray, y_train: np.ndarray):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""

        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä¸æ­£æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹")

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        X_scaled = self.scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1]))
        X_scaled = X_scaled.reshape(X_train.shape)

        # LSTMå­¦ç¿’
        if TORCH_AVAILABLE:
            self._train_lstm(X_scaled, y_train)
            self._train_transformer(X_scaled, y_train)

        # Isolation Forestå­¦ç¿’
        self._train_isolation_forest(X_scaled.reshape(len(X_scaled), -1))

        self.is_fitted = True
        logger.info("ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«å­¦ç¿’å®Œäº†")

    def _train_lstm(self, X: np.ndarray, y: np.ndarray):
        """LSTMå­¦ç¿’"""

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.LongTensor(y).to(device)

        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        input_size = X.shape[-1]
        self.models["lstm"] = LSTMFraudModel(input_size).to(device)

        optimizer = torch.optim.Adam(self.models["lstm"].parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()

        # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        self.models["lstm"].train()
        for epoch in range(50):
            total_loss = 0
            for batch_x, batch_y in dataloader:
                optimizer.zero_grad()

                probabilities, _ = self.models["lstm"](batch_x)
                loss = criterion(probabilities, batch_y)

                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if epoch % 10 == 0:
                logger.info(
                    f"LSTM Epoch {epoch}: Loss = {total_loss/len(dataloader):.4f}"
                )

        self.models["lstm"].eval()

    def _train_transformer(self, X: np.ndarray, y: np.ndarray):
        """Transformerå­¦ç¿’"""

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        X_tensor = torch.FloatTensor(X).to(device)
        y_tensor = torch.FloatTensor(y).to(device)

        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

        input_dim = X.shape[-1]
        self.models["transformer"] = TransformerAnomalyModel(input_dim).to(device)

        optimizer = torch.optim.AdamW(
            self.models["transformer"].parameters(), lr=0.0001, weight_decay=0.01
        )
        criterion = nn.BCELoss()

        # å­¦ç¿’
        self.models["transformer"].train()
        for epoch in range(30):
            total_loss = 0
            for batch_x, batch_y in dataloader:
                optimizer.zero_grad()

                anomaly_scores = self.models["transformer"](batch_x)
                loss = criterion(anomaly_scores, batch_y)

                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            if epoch % 5 == 0:
                logger.info(
                    f"Transformer Epoch {epoch}: Loss = {total_loss/len(dataloader):.4f}"
                )

        self.models["transformer"].eval()

    def _train_isolation_forest(self, X: np.ndarray):
        """Isolation Forestå­¦ç¿’"""

        self.models["isolation_forest"] = IsolationForest(
            contamination=0.1,  # 10%ã‚’ç•°å¸¸ã¨ä»®å®š
            random_state=42,
            n_estimators=200,
        )
        self.models["isolation_forest"].fit(X)

    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""

        if not self.is_fitted:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        X_scaled = self.scaler.transform(X.reshape(-1, X.shape[-1]))
        X_scaled = X_scaled.reshape(X.shape)

        predictions = {}

        # LSTMäºˆæ¸¬
        if self.models["lstm"]:
            device = next(self.models["lstm"].parameters()).device
            X_tensor = torch.FloatTensor(X_scaled).to(device)

            with torch.no_grad():
                probabilities, _ = self.models["lstm"](X_tensor)
                predictions["lstm"] = probabilities[:, 1].cpu().numpy()  # ä¸æ­£ç¢ºç‡

        # Transformeräºˆæ¸¬
        if self.models["transformer"]:
            device = next(self.models["transformer"].parameters()).device
            X_tensor = torch.FloatTensor(X_scaled).to(device)

            with torch.no_grad():
                anomaly_scores = self.models["transformer"](X_tensor)
                predictions["transformer"] = anomaly_scores.cpu().numpy()

        # Isolation Forestäºˆæ¸¬
        if self.models["isolation_forest"]:
            X_flat = X_scaled.reshape(len(X_scaled), -1)
            anomaly_scores = self.models["isolation_forest"].decision_function(X_flat)
            # æ­£è¦åŒ– (ç•°å¸¸ã»ã©é«˜ã„ã‚¹ã‚³ã‚¢)
            predictions["isolation_forest"] = 1 / (1 + np.exp(anomaly_scores))

        # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        ensemble_scores = np.zeros(len(X))
        for model_name, scores in predictions.items():
            weight = self.weights.get(model_name, 0)
            ensemble_scores += weight * scores

        return ensemble_scores, predictions


class FraudDetectionEngine:
    """ä¸æ­£æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼‰"""

    def __init__(self):
        self.ensemble_detector = EnsembleFraudDetector()
        self.feature_extractor = FeatureExtractor()
        self.models_loaded = False

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        self.stats = {
            "total_detections": 0,
            "fraud_detected": 0,
            "false_positives": 0,
            "avg_processing_time": 0.0,
            "model_accuracies": {},
        }

        logger.info("ä¸æ­£æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")

    def load_models(self, model_path: str = "models/fraud_detection/"):
        """å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""

        model_dir = Path(model_path)
        if not model_dir.exists():
            logger.warning(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {model_path}")
            return False

        try:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            ensemble_path = model_dir / "ensemble_fraud_detector.pkl"
            if ensemble_path.exists():
                with open(ensemble_path, "rb") as f:
                    self.ensemble_detector = pickle.load(f)

            self.models_loaded = True
            logger.info("ä¸æ­£æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            return True

        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def save_models(self, model_path: str = "models/fraud_detection/"):
        """ãƒ¢ãƒ‡ãƒ«ä¿å­˜"""

        model_dir = Path(model_path)
        model_dir.mkdir(parents=True, exist_ok=True)

        try:
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            with open(model_dir / "ensemble_fraud_detector.pkl", "wb") as f:
                pickle.dump(self.ensemble_detector, f)

            logger.info(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_path}")

        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    async def detect_fraud(
        self, request: FraudDetectionRequest
    ) -> FraudDetectionResult:
        """ä¸æ­£å–å¼•æ¤œçŸ¥ï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼‰"""

        start_time = time.time()

        try:
            # ç‰¹å¾´é‡æŠ½å‡º
            features = self.feature_extractor.extract_features(request)

            if not self.models_loaded:
                # ãƒ¢ãƒ‡ãƒ«ãŒæœªå­¦ç¿’ã®å ´åˆã€åŸºæœ¬çš„ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ¤œçŸ¥
                return self._rule_based_detection(request, start_time)

            # æ·±å±¤å­¦ç¿’äºˆæ¸¬
            X = np.array([features]).reshape(1, -1, len(features))
            ensemble_scores, individual_predictions = self.ensemble_detector.predict(X)

            fraud_probability = float(ensemble_scores[0])
            is_fraud = fraud_probability > 0.5

            # ãƒªã‚¹ã‚¯è¦å› åˆ†æ
            risk_factors = self._analyze_risk_factors(
                request, features, individual_predictions
            )

            # èª¬æ˜æ–‡ç”Ÿæˆ
            explanation = self._generate_explanation(fraud_probability, risk_factors)

            # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            recommended_action = self._get_recommended_action(
                fraud_probability, request
            )

            result = FraudDetectionResult(
                transaction_id=request.transaction_id,
                is_fraud=is_fraud,
                fraud_probability=fraud_probability,
                confidence=self._calculate_confidence(individual_predictions),
                risk_factors=risk_factors,
                anomaly_score=fraud_probability,
                explanation=explanation,
                recommended_action=recommended_action,
                models_used=["lstm", "transformer", "isolation_forest"],
                processing_time=time.time() - start_time,
                timestamp=datetime.now(),
            )

            # çµ±è¨ˆæ›´æ–°
            self._update_stats(result)

            return result

        except Exception as e:
            logger.error(f"ä¸æ­£æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_result(request, str(e), start_time)

    def _rule_based_detection(
        self, request: FraudDetectionRequest, start_time: float
    ) -> FraudDetectionResult:
        """ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ¤œçŸ¥ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""

        risk_score = 0.0
        risk_factors = {}

        # é«˜é¡å–å¼•ãƒã‚§ãƒƒã‚¯
        if request.amount > 1000000:  # 100ä¸‡å††ä»¥ä¸Š
            risk_score += 0.3
            risk_factors["high_amount"] = 0.3

        # æ™‚é–“å¸¯ãƒã‚§ãƒƒã‚¯
        hour = request.timestamp.hour
        if hour < 6 or hour > 23:
            risk_score += 0.2
            risk_factors["unusual_time"] = 0.2

        # å£åº§æ®‹é«˜æ¯”ãƒã‚§ãƒƒã‚¯
        if request.amount > request.account_balance * 0.8:
            risk_score += 0.4
            risk_factors["balance_ratio"] = 0.4

        # å–å¼•å±¥æ­´ãƒã‚§ãƒƒã‚¯
        if len(request.transaction_history) > 0:
            recent_transactions = [
                t
                for t in request.transaction_history
                if (request.timestamp - datetime.fromisoformat(t["timestamp"])).days
                <= 1
            ]
            if len(recent_transactions) > 10:
                risk_score += 0.3
                risk_factors["frequent_transactions"] = 0.3

        fraud_probability = min(1.0, risk_score)
        is_fraud = fraud_probability > 0.6

        return FraudDetectionResult(
            transaction_id=request.transaction_id,
            is_fraud=is_fraud,
            fraud_probability=fraud_probability,
            confidence=0.7,
            risk_factors=risk_factors,
            anomaly_score=fraud_probability,
            explanation=f"ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æ¤œçŸ¥: ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ {fraud_probability:.2f}",
            recommended_action="è¿½åŠ èªè¨¼" if is_fraud else "é€šå¸¸å‡¦ç†",
            models_used=["rule_based"],
            processing_time=time.time() - start_time,
            timestamp=datetime.now(),
        )

    def _analyze_risk_factors(
        self,
        request: FraudDetectionRequest,
        features: List[float],
        predictions: Dict[str, np.ndarray],
    ) -> Dict[str, float]:
        """ãƒªã‚¹ã‚¯è¦å› åˆ†æ"""

        risk_factors = {}

        # é‡‘é¡ãƒªã‚¹ã‚¯
        amount_percentile = min(1.0, request.amount / 10000000)  # 1000ä¸‡å††åŸºæº–
        risk_factors["amount_risk"] = amount_percentile

        # æ™‚é–“ãƒªã‚¹ã‚¯
        hour = request.timestamp.hour
        if hour < 9 or hour > 17:
            risk_factors["time_risk"] = 0.8
        else:
            risk_factors["time_risk"] = 0.2

        # æ®‹é«˜ãƒªã‚¹ã‚¯
        balance_ratio = request.amount / max(request.account_balance, 1)
        risk_factors["balance_risk"] = min(1.0, balance_ratio)

        # å–å¼•é »åº¦ãƒªã‚¹ã‚¯
        recent_count = len(
            [
                t
                for t in request.transaction_history
                if (request.timestamp - datetime.fromisoformat(t["timestamp"])).hours
                <= 24
            ]
        )
        risk_factors["frequency_risk"] = min(1.0, recent_count / 20)

        return risk_factors

    def _generate_explanation(
        self, fraud_probability: float, risk_factors: Dict[str, float]
    ) -> str:
        """èª¬æ˜æ–‡ç”Ÿæˆ"""

        if fraud_probability > 0.8:
            severity = "éå¸¸ã«é«˜ã„"
        elif fraud_probability > 0.6:
            severity = "é«˜ã„"
        elif fraud_probability > 0.4:
            severity = "ä¸­ç¨‹åº¦"
        else:
            severity = "ä½ã„"

        top_factors = sorted(risk_factors.items(), key=lambda x: x[1], reverse=True)[:3]
        factor_names = {
            "amount_risk": "å–å¼•é‡‘é¡",
            "time_risk": "å–å¼•æ™‚é–“",
            "balance_risk": "æ®‹é«˜æ¯”ç‡",
            "frequency_risk": "å–å¼•é »åº¦",
        }

        factors_text = ", ".join(
            [factor_names.get(name, name) for name, _ in top_factors]
        )

        return (
            f"ä¸æ­£å–å¼•ã®å¯èƒ½æ€§: {severity} ({fraud_probability:.2f})\n"
            f"ä¸»è¦ãƒªã‚¹ã‚¯è¦å› : {factors_text}\n"
            f"æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹é«˜ç²¾åº¦åˆ†æçµæœ"
        )

    def _get_recommended_action(
        self, fraud_probability: float, request: FraudDetectionRequest
    ) -> str:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š"""

        if fraud_probability > 0.9:
            return "å–å¼•å³åº§åœæ­¢ãƒ»ç·Šæ€¥èª¿æŸ»"
        elif fraud_probability > 0.7:
            return "å–å¼•ä¿ç•™ãƒ»è©³ç´°ç¢ºèª"
        elif fraud_probability > 0.5:
            return "è¿½åŠ èªè¨¼ãƒ»ç›£è¦–å¼·åŒ–"
        elif fraud_probability > 0.3:
            return "é€šå¸¸ç›£è¦–ç¶™ç¶š"
        else:
            return "é€šå¸¸å‡¦ç†ç¶™ç¶š"

    def _calculate_confidence(self, predictions: Dict[str, np.ndarray]) -> float:
        """ä¿¡é ¼åº¦è¨ˆç®—"""

        if not predictions:
            return 0.5

        # ãƒ¢ãƒ‡ãƒ«é–“ã®ä¸€è‡´åº¦ã‚’ä¿¡é ¼åº¦ã¨ã™ã‚‹
        scores = list(predictions.values())
        if len(scores) < 2:
            return 0.8

        variance = np.var([s[0] if isinstance(s, np.ndarray) else s for s in scores])
        confidence = max(0.3, 1.0 - variance * 2)
        return confidence

    def _create_error_result(
        self, request: FraudDetectionRequest, error: str, start_time: float
    ) -> FraudDetectionResult:
        """ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ"""

        return FraudDetectionResult(
            transaction_id=request.transaction_id,
            is_fraud=False,
            fraud_probability=0.5,
            confidence=0.3,
            risk_factors={"error": 1.0},
            anomaly_score=0.5,
            explanation=f"æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {error}",
            recommended_action="æ‰‹å‹•ç¢ºèª",
            models_used=["error_fallback"],
            processing_time=time.time() - start_time,
            timestamp=datetime.now(),
        )

    def _update_stats(self, result: FraudDetectionResult):
        """çµ±è¨ˆæ›´æ–°"""

        self.stats["total_detections"] += 1
        if result.is_fraud:
            self.stats["fraud_detected"] += 1

        # å‡¦ç†æ™‚é–“æ›´æ–°
        total = self.stats["total_detections"]
        old_avg = self.stats["avg_processing_time"]
        self.stats["avg_processing_time"] = (
            old_avg * (total - 1) + result.processing_time
        ) / total

    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆå–å¾—"""
        return {
            **self.stats,
            "fraud_rate": self.stats["fraud_detected"]
            / max(1, self.stats["total_detections"]),
            "models_loaded": self.models_loaded,
        }


class FeatureExtractor:
    """ç‰¹å¾´é‡æŠ½å‡ºå™¨"""

    def extract_features(self, request: FraudDetectionRequest) -> List[float]:
        """å–å¼•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡æŠ½å‡º"""

        features = []

        # åŸºæœ¬ç‰¹å¾´é‡
        features.extend(
            [
                request.amount,
                request.account_balance,
                request.amount / max(request.account_balance, 1),  # æ®‹é«˜æ¯”
                request.timestamp.hour,
                request.timestamp.weekday(),
                len(request.transaction_history),
            ]
        )

        # å–å¼•å±¥æ­´çµ±è¨ˆ
        if request.transaction_history:
            amounts = [t.get("amount", 0) for t in request.transaction_history]
            features.extend(
                [np.mean(amounts), np.std(amounts), np.max(amounts), np.min(amounts)]
            )
        else:
            features.extend([0, 0, 0, 0])

        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±
        device_features = self._extract_device_features(request.device_info)
        features.extend(device_features)

        # å¸‚å ´çŠ¶æ³
        market_features = self._extract_market_features(request.market_conditions)
        features.extend(market_features)

        return features

    def _extract_device_features(self, device_info: Dict[str, Any]) -> List[float]:
        """ãƒ‡ãƒã‚¤ã‚¹ç‰¹å¾´é‡æŠ½å‡º"""

        features = []

        # ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—ï¼ˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
        device_type = device_info.get("type", "unknown")
        device_encoding = {"mobile": 1, "desktop": 2, "tablet": 3}
        features.append(device_encoding.get(device_type, 0))

        # OSæƒ…å ±
        os_info = device_info.get("os", "unknown")
        os_encoding = {"ios": 1, "android": 2, "windows": 3, "macos": 4}
        features.append(os_encoding.get(os_info, 0))

        # æ–°è¦ãƒ‡ãƒã‚¤ã‚¹ãƒ•ãƒ©ã‚°
        features.append(1 if device_info.get("is_new_device", False) else 0)

        return features

    def _extract_market_features(
        self, market_conditions: Dict[str, Any]
    ) -> List[float]:
        """å¸‚å ´ç‰¹å¾´é‡æŠ½å‡º"""

        features = []

        # å¸‚å ´ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        features.append(market_conditions.get("volatility", 0.2))

        # å–å¼•é‡
        features.append(market_conditions.get("volume", 1000000))

        # å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰
        trend = market_conditions.get("trend", "neutral")
        trend_encoding = {"bullish": 1, "bearish": -1, "neutral": 0}
        features.append(trend_encoding.get(trend, 0))

        return features


# ãƒ†ã‚¹ãƒˆç”¨é–¢æ•°
async def test_fraud_detection_engine():
    """ä¸æ­£æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³ãƒ†ã‚¹ãƒˆ"""

    engine = FraudDetectionEngine()

    # ãƒ†ã‚¹ãƒˆç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆä¸æ­£ã®å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
    suspicious_request = FraudDetectionRequest(
        transaction_id="FRAUD_TEST_001",
        user_id="user_123",
        amount=5000000,  # 500ä¸‡å††ï¼ˆé«˜é¡ï¼‰
        timestamp=datetime(2025, 1, 1, 3, 0, 0),  # æ·±å¤œ
        transaction_type="transfer",
        account_balance=1000000,  # æ®‹é«˜100ä¸‡å††ï¼ˆæ¯”ç‡é«˜ã„ï¼‰
        location="foreign",
        device_info={"type": "mobile", "os": "android", "is_new_device": True},
        transaction_history=[
            {"amount": 100000, "timestamp": "2025-01-01T02:00:00"},
            {"amount": 200000, "timestamp": "2025-01-01T02:30:00"},
        ],
        market_conditions={"volatility": 0.4, "volume": 500000, "trend": "bearish"},
    )

    print("ğŸ” ä¸æ­£æ¤œçŸ¥ãƒ†ã‚¹ãƒˆé–‹å§‹...")

    result = await engine.detect_fraud(suspicious_request)

    print("âœ… æ¤œçŸ¥å®Œäº†!")
    print(f"âš ï¸ ä¸æ­£åˆ¤å®š: {'ã¯ã„' if result.is_fraud else 'ã„ã„ãˆ'}")
    print(f"ğŸ“Š ä¸æ­£ç¢ºç‡: {result.fraud_probability:.2f}")
    print(f"ğŸ¯ ä¿¡é ¼åº¦: {result.confidence:.2f}")
    print(f"ğŸ• å‡¦ç†æ™‚é–“: {result.processing_time:.3f}ç§’")
    print(f"ğŸ“ èª¬æ˜: {result.explanation}")
    print(f"ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {result.recommended_action}")

    # çµ±è¨ˆæƒ…å ±
    stats = engine.get_stats()
    print(f"ğŸ“ˆ çµ±è¨ˆ: {stats}")


if __name__ == "__main__":
    import asyncio

    asyncio.run(test_fraud_detection_engine())
