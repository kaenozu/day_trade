#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 
Issue #420: ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã¨ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å¼·åŒ–

é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ã®ãŸã‚ã®åŒ…æ‹¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ :
- ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ã‚¸ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- å‹•çš„ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
- æ¥­ç•Œæ¨™æº–æº–æ‹ ã®å“è³ªãƒã‚§ãƒƒã‚¯
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆ
- ãƒ‡ãƒ¼ã‚¿ç³»è­œç®¡ç†
- å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
"""

import asyncio
import hashlib
import json
import logging
import time
import warnings
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler

try:
    from ..utils.data_quality_manager import (
        DataIssueType,
        DataQualityIssue,
        DataQualityLevel,
        DataQualityMetrics,
        DataValidator,
    )
    from ..utils.logging_config import get_context_logger
    from ..utils.unified_cache_manager import (
        UnifiedCacheManager,
        generate_unified_cache_key,
    )
except ImportError:
    logging.basicConfig(level=logging.INFO)

    def get_context_logger(name):
        return logging.getLogger(name)

    # ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¹
    class DataQualityMetrics:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)

    class DataQualityLevel(Enum):
        EXCELLENT = "excellent"
        GOOD = "good"
        FAIR = "fair"
        POOR = "poor"
        CRITICAL = "critical"

    class UnifiedCacheManager:
        def get(self, key, default=None):
            return default

        def put(self, key, value, **kwargs):
            return True

    def generate_unified_cache_key(*args, **kwargs):
        return f"pipeline_key_{hash(str(args))}"


logger = get_context_logger(__name__)

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)


class PipelineStage(Enum):
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†æ®µéš"""

    INGESTION = "ingestion"  # ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿
    PREPROCESSING = "preprocessing"  # å‰å‡¦ç†
    VALIDATION = "validation"  # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    CLEANING = "cleaning"  # ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    ENRICHMENT = "enrichment"  # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ãƒªãƒƒãƒãƒ¡ãƒ³ãƒˆ
    QUALITY_ASSESSMENT = "quality_assessment"  # å“è³ªè©•ä¾¡
    EXPORT = "export"  # ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›


class ValidationLevel(Enum):
    """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«"""

    BASIC = "basic"  # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    STANDARD = "standard"  # æ¨™æº–ãƒã‚§ãƒƒã‚¯
    STRICT = "strict"  # å³æ ¼ãƒã‚§ãƒƒã‚¯
    ENTERPRISE = "enterprise"  # ä¼æ¥­ãƒ¬ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯


class DataLineageEvent(Enum):
    """ãƒ‡ãƒ¼ã‚¿ç³»è­œã‚¤ãƒ™ãƒ³ãƒˆ"""

    CREATED = "created"
    TRANSFORMED = "transformed"
    VALIDATED = "validated"
    CLEANED = "cleaned"
    ENRICHED = "enriched"
    EXPORTED = "exported"
    ARCHIVED = "archived"


@dataclass
class PipelineMetrics:
    """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    stage: PipelineStage
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_ms: float = 0.0
    records_processed: int = 0
    records_passed: int = 0
    records_failed: int = 0
    errors_count: int = 0
    warnings_count: int = 0
    data_size_mb: float = 0.0
    memory_usage_mb: float = 0.0
    success: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class DataLineageRecord:
    """ãƒ‡ãƒ¼ã‚¿ç³»è­œè¨˜éŒ²"""

    record_id: str
    event: DataLineageEvent
    timestamp: datetime
    stage: PipelineStage
    data_hash: str
    transformation: str
    metrics: Dict[str, Any] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ValidationRule:
    """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«å®šç¾©"""

    rule_id: str
    name: str
    description: str
    rule_type: str  # "schema", "range", "format", "business"
    severity: str  # "critical", "high", "medium", "low"
    validation_func: callable
    auto_fix_func: Optional[callable] = None
    enabled: bool = True
    metadata: Dict[str, Any] = field(default_factory=dict)


class DataProcessor(ABC):
    """æŠ½è±¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¯ãƒ©ã‚¹"""

    @abstractmethod
    async def process(
        self, data: Any, context: Dict[str, Any]
    ) -> Tuple[Any, PipelineMetrics]:
        """ãƒ‡ãƒ¼ã‚¿å‡¦ç†å®Ÿè¡Œ"""
        pass

    @abstractmethod
    def get_processor_info(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚»ãƒƒã‚µæƒ…å ±å–å¾—"""
        pass


class SchemaValidator(DataProcessor):
    """ã‚¹ã‚­ãƒ¼ãƒãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼"""

    def __init__(self, schema_rules: Dict[str, Any]):
        self.schema_rules = schema_rules

    async def process(
        self, data: Any, context: Dict[str, Any]
    ) -> Tuple[Any, PipelineMetrics]:
        """ã‚¹ã‚­ãƒ¼ãƒãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        start_time = datetime.utcnow()
        metrics = PipelineMetrics(stage=PipelineStage.VALIDATION, start_time=start_time)

        try:
            if isinstance(data, pd.DataFrame):
                metrics.records_processed = len(data)

                # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
                required_cols = self.schema_rules.get("required_columns", [])
                missing_cols = [col for col in required_cols if col not in data.columns]

                if missing_cols:
                    metrics.errors_count += len(missing_cols)
                    metrics.metadata["missing_columns"] = missing_cols

                # ãƒ‡ãƒ¼ã‚¿å‹ãƒã‚§ãƒƒã‚¯
                type_rules = self.schema_rules.get("column_types", {})
                type_errors = []

                for col, expected_type in type_rules.items():
                    if col in data.columns:
                        actual_type = data[col].dtype
                        if not self._is_compatible_type(actual_type, expected_type):
                            type_errors.append(
                                {
                                    "column": col,
                                    "expected": expected_type,
                                    "actual": str(actual_type),
                                }
                            )

                if type_errors:
                    metrics.errors_count += len(type_errors)
                    metrics.metadata["type_errors"] = type_errors

                # æˆåŠŸåˆ¤å®š
                metrics.success = metrics.errors_count == 0
                metrics.records_passed = len(data) if metrics.success else 0
                metrics.records_failed = 0 if metrics.success else len(data)

            else:
                metrics.records_processed = 1
                metrics.success = True
                metrics.records_passed = 1

            metrics.end_time = datetime.utcnow()
            metrics.duration_ms = (metrics.end_time - start_time).total_seconds() * 1000

            return data, metrics

        except Exception as e:
            logger.error(f"ã‚¹ã‚­ãƒ¼ãƒãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            metrics.end_time = datetime.utcnow()
            metrics.errors_count += 1
            metrics.success = False
            metrics.metadata["error"] = str(e)
            return data, metrics

    def _is_compatible_type(self, actual_type, expected_type: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿å‹äº’æ›æ€§ãƒã‚§ãƒƒã‚¯"""
        type_mapping = {
            "int": ["int64", "int32", "int16", "int8"],
            "float": ["float64", "float32", "int64", "int32"],
            "string": ["object", "string"],
            "datetime": ["datetime64[ns]", "object"],
            "bool": ["bool"],
        }

        compatible_types = type_mapping.get(expected_type, [])
        return str(actual_type) in compatible_types or expected_type in str(actual_type)

    def get_processor_info(self) -> Dict[str, Any]:
        return {
            "processor_type": "schema_validator",
            "schema_rules": self.schema_rules,
            "version": "1.0",
        }


class OutlierDetector(DataProcessor):
    """ç•°å¸¸å€¤æ¤œå‡ºå‡¦ç†"""

    def __init__(self, method: str = "isolation_forest", threshold: float = 0.05):
        self.method = method
        self.threshold = threshold
        self.scaler = StandardScaler()

    async def process(
        self, data: Any, context: Dict[str, Any]
    ) -> Tuple[Any, PipelineMetrics]:
        """ç•°å¸¸å€¤æ¤œå‡ºå®Ÿè¡Œ"""
        start_time = datetime.utcnow()
        metrics = PipelineMetrics(stage=PipelineStage.CLEANING, start_time=start_time)

        try:
            if isinstance(data, pd.DataFrame) and len(data) > 0:
                metrics.records_processed = len(data)

                # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿å¯¾è±¡
                numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()

                if numeric_cols:
                    # Isolation Forest ã«ã‚ˆã‚‹ç•°å¸¸å€¤æ¤œå‡º
                    X = data[numeric_cols].fillna(data[numeric_cols].median())
                    X_scaled = self.scaler.fit_transform(X)

                    if self.method == "isolation_forest":
                        detector = IsolationForest(
                            contamination=self.threshold, random_state=42
                        )
                        outlier_labels = detector.fit_predict(X_scaled)
                        outlier_mask = outlier_labels == -1

                    else:  # statistical method
                        outlier_mask = self._detect_statistical_outliers(
                            data[numeric_cols]
                        )

                    outlier_count = outlier_mask.sum()

                    # ç•°å¸¸å€¤ã«ãƒ•ãƒ©ã‚°è¨­å®š
                    data_cleaned = data.copy()
                    data_cleaned["is_outlier"] = outlier_mask

                    metrics.records_passed = len(data) - outlier_count
                    metrics.records_failed = outlier_count
                    metrics.metadata["outlier_count"] = int(outlier_count)
                    metrics.metadata["outlier_percentage"] = float(
                        outlier_count / len(data) * 100
                    )

                    metrics.success = True

                    return data_cleaned, metrics
                else:
                    metrics.warnings_count = 1
                    metrics.metadata["warning"] = "æ•°å€¤ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"

            metrics.records_passed = metrics.records_processed
            metrics.success = True
            metrics.end_time = datetime.utcnow()
            metrics.duration_ms = (metrics.end_time - start_time).total_seconds() * 1000

            return data, metrics

        except Exception as e:
            logger.error(f"ç•°å¸¸å€¤æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            metrics.end_time = datetime.utcnow()
            metrics.errors_count += 1
            metrics.success = False
            metrics.metadata["error"] = str(e)
            return data, metrics

    def _detect_statistical_outliers(self, data: pd.DataFrame) -> np.ndarray:
        """çµ±è¨ˆçš„ç•°å¸¸å€¤æ¤œå‡º (IQRæ–¹å¼)"""
        outlier_mask = np.zeros(len(data), dtype=bool)

        for col in data.columns:
            if data[col].dtype in ["int64", "float64"]:
                Q1 = data[col].quantile(0.25)
                Q3 = data[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR

                col_outliers = (data[col] < lower_bound) | (data[col] > upper_bound)
                outlier_mask |= col_outliers

        return outlier_mask

    def get_processor_info(self) -> Dict[str, Any]:
        return {
            "processor_type": "outlier_detector",
            "method": self.method,
            "threshold": self.threshold,
            "version": "1.0",
        }


class DataValidationPipeline:
    """ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""

    def __init__(
        self,
        validation_level: ValidationLevel = ValidationLevel.STANDARD,
        enable_cache: bool = True,
        enable_lineage: bool = True,
        storage_path: str = "data/pipeline",
    ):
        self.validation_level = validation_level
        self.enable_cache = enable_cache
        self.enable_lineage = enable_lineage
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        if enable_cache:
            try:
                self.cache_manager = UnifiedCacheManager(
                    l1_memory_mb=32, l2_memory_mb=128, l3_disk_mb=512
                )
                logger.info("ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
            except Exception as e:
                logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆæœŸåŒ–å¤±æ•—: {e}")
                self.cache_manager = None
        else:
            self.cache_manager = None

        # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ç®¡ç†
        self.processors: Dict[PipelineStage, List[DataProcessor]] = {
            stage: [] for stage in PipelineStage
        }

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«
        self.validation_rules: List[ValidationRule] = []

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å±¥æ­´
        self.pipeline_history: List[Dict[str, Any]] = []

        # ãƒ‡ãƒ¼ã‚¿ç³»è­œç®¡ç†
        self.lineage_records: List[DataLineageRecord] = []

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¨­å®š
        self._setup_default_processors()

        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
        logger.info(f"  - ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«: {validation_level.value}")
        logger.info(f"  - ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {'æœ‰åŠ¹' if enable_cache else 'ç„¡åŠ¹'}")
        logger.info(f"  - ãƒ‡ãƒ¼ã‚¿ç³»è­œ: {'æœ‰åŠ¹' if enable_lineage else 'ç„¡åŠ¹'}")

    def _setup_default_processors(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¨­å®š"""
        # ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ç”¨ã‚¹ã‚­ãƒ¼ãƒãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼
        price_schema = {
            "required_columns": ["Open", "High", "Low", "Close", "Volume"],
            "column_types": {
                "Open": "float",
                "High": "float",
                "Low": "float",
                "Close": "float",
                "Volume": "int",
            },
        }
        price_validator = SchemaValidator(price_schema)
        self.add_processor(PipelineStage.VALIDATION, price_validator)

        # ç•°å¸¸å€¤æ¤œå‡º
        outlier_detector = OutlierDetector(method="isolation_forest", threshold=0.05)
        self.add_processor(PipelineStage.CLEANING, outlier_detector)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«
        self._setup_default_validation_rules()

    def _setup_default_validation_rules(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«è¨­å®š"""
        # ä¾¡æ ¼é †åºãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ«
        price_order_rule = ValidationRule(
            rule_id="price_order_check",
            name="ä¾¡æ ¼é †åºãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³",
            description="Low <= Open,Close <= High ã®ç¢ºèª",
            rule_type="business",
            severity="high",
            validation_func=self._validate_price_order,
            auto_fix_func=self._fix_price_order,
        )
        self.validation_rules.append(price_order_rule)

        # ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ«
        freshness_rule = ValidationRule(
            rule_id="data_freshness",
            name="ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒã‚§ãƒƒã‚¯",
            description="ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“çš„æ–°é®®æ€§ã®ç¢ºèª",
            rule_type="timeliness",
            severity="medium",
            validation_func=self._validate_data_freshness,
        )
        self.validation_rules.append(freshness_rule)

        # å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯ãƒ«ãƒ¼ãƒ«
        completeness_rule = ValidationRule(
            rule_id="data_completeness",
            name="ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯",
            description="æ¬ æå€¤ã®ç¢ºèª",
            rule_type="completeness",
            severity="medium",
            validation_func=self._validate_data_completeness,
            auto_fix_func=self._fix_missing_values,
        )
        self.validation_rules.append(completeness_rule)

    def add_processor(self, stage: PipelineStage, processor: DataProcessor):
        """ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¿½åŠ """
        self.processors[stage].append(processor)
        logger.info(
            f"ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¿½åŠ : {stage.value} - {processor.get_processor_info()['processor_type']}"
        )

    def add_validation_rule(self, rule: ValidationRule):
        """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«è¿½åŠ """
        self.validation_rules.append(rule)
        logger.info(f"ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«è¿½åŠ : {rule.rule_id}")

    async def process_data(
        self,
        data: Any,
        data_type: str,
        symbol: str = "",
        metadata: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ"""
        pipeline_id = f"pipeline_{int(time.time())}_{hash(str(data))}"
        start_time = datetime.utcnow()

        logger.info(f"ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†é–‹å§‹: {pipeline_id} ({data_type} {symbol})")

        # å‡¦ç†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        context = {
            "pipeline_id": pipeline_id,
            "data_type": data_type,
            "symbol": symbol,
            "validation_level": self.validation_level,
            "metadata": metadata or {},
        }

        # çµæœåé›†
        results = {
            "pipeline_id": pipeline_id,
            "data_type": data_type,
            "symbol": symbol,
            "start_time": start_time.isoformat(),
            "validation_level": self.validation_level.value,
            "stages": {},
            "overall_success": True,
            "total_duration_ms": 0.0,
            "quality_metrics": None,
            "lineage_records": [],
            "processed_data": data,
            "recommendations": [],
        }

        current_data = data

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if self.enable_cache and self.cache_manager:
            cache_key = self._generate_cache_key(data, data_type, symbol)
            cached_result = self.cache_manager.get(cache_key)
            if cached_result:
                logger.info(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {pipeline_id}")
                return cached_result

        # å„æ®µéšã‚’é †æ¬¡å‡¦ç†
        for stage in PipelineStage:
            if stage in self.processors and self.processors[stage]:
                stage_start = datetime.utcnow()
                stage_results = []
                stage_success = True

                # æ®µéšå†…ã®å…¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’å®Ÿè¡Œ
                for processor in self.processors[stage]:
                    try:
                        processed_data, metrics = await processor.process(
                            current_data, context
                        )
                        stage_results.append(
                            {
                                "processor": processor.get_processor_info(),
                                "metrics": metrics,
                            }
                        )

                        if metrics.success:
                            current_data = processed_data

                            # ãƒ‡ãƒ¼ã‚¿ç³»è­œè¨˜éŒ²
                            if self.enable_lineage:
                                self._record_lineage(
                                    pipeline_id,
                                    stage,
                                    current_data,
                                    processor.get_processor_info(),
                                )
                        else:
                            stage_success = False
                            results["overall_success"] = False

                    except Exception as e:
                        logger.error(f"ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚¨ãƒ©ãƒ¼ {stage.value}: {e}")
                        stage_success = False
                        results["overall_success"] = False
                        stage_results.append(
                            {
                                "processor": processor.get_processor_info(),
                                "error": str(e),
                            }
                        )

                # æ®µéšçµæœè¨˜éŒ²
                stage_duration = (
                    datetime.utcnow() - stage_start
                ).total_seconds() * 1000
                results["stages"][stage.value] = {
                    "success": stage_success,
                    "duration_ms": stage_duration,
                    "processors": stage_results,
                }
                results["total_duration_ms"] += stage_duration

        # ã‚«ã‚¹ã‚¿ãƒ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«å®Ÿè¡Œ
        validation_results = await self._execute_validation_rules(current_data, context)
        results["validation_results"] = validation_results

        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        if hasattr(self, "_calculate_quality_metrics"):
            results["quality_metrics"] = await self._calculate_quality_metrics(
                current_data, data_type
            )

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        results["recommendations"] = self._generate_recommendations(results)

        # çµæœçµ‚äº†å‡¦ç†
        results["processed_data"] = current_data
        results["end_time"] = datetime.utcnow().isoformat()
        results["lineage_records"] = [
            {
                "record_id": record.record_id,
                "event": record.event.value,
                "timestamp": record.timestamp.isoformat(),
                "stage": record.stage.value,
                "transformation": record.transformation,
            }
            for record in self.lineage_records[-10:]  # æœ€æ–°10ä»¶
        ]

        # å±¥æ­´ä¿å­˜
        self.pipeline_history.append(results)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        if self.enable_cache and self.cache_manager:
            cache_key = self._generate_cache_key(data, data_type, symbol)
            priority = 3.0 + (2.0 if results["overall_success"] else 0.0)
            self.cache_manager.put(cache_key, results, priority=priority)

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†çµæœä¿å­˜
        await self._save_pipeline_results(results)

        logger.info(
            f"ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å®Œäº†: {pipeline_id} "
            f"({'æˆåŠŸ' if results['overall_success'] else 'å¤±æ•—'}) "
            f"({results['total_duration_ms']:.1f}ms)"
        )

        return results

    async def _execute_validation_rules(
        self, data: Any, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«å®Ÿè¡Œ"""
        validation_results = {
            "total_rules": len(self.validation_rules),
            "passed_rules": 0,
            "failed_rules": 0,
            "rule_results": [],
            "auto_fixes_applied": [],
        }

        for rule in self.validation_rules:
            if not rule.enabled:
                continue

            try:
                # ãƒ«ãƒ¼ãƒ«å®Ÿè¡Œ
                is_valid, issues = rule.validation_func(data, context)

                rule_result = {
                    "rule_id": rule.rule_id,
                    "name": rule.name,
                    "passed": is_valid,
                    "severity": rule.severity,
                    "issues": issues,
                }

                if is_valid:
                    validation_results["passed_rules"] += 1
                else:
                    validation_results["failed_rules"] += 1

                    # è‡ªå‹•ä¿®æ­£å®Ÿè¡Œ
                    if rule.auto_fix_func and not is_valid:
                        try:
                            fixed_data = rule.auto_fix_func(data, issues)
                            if fixed_data is not None:
                                data = fixed_data
                                validation_results["auto_fixes_applied"].append(
                                    rule.rule_id
                                )
                                rule_result["auto_fixed"] = True
                        except Exception as e:
                            logger.error(f"è‡ªå‹•ä¿®æ­£ã‚¨ãƒ©ãƒ¼ {rule.rule_id}: {e}")
                            rule_result["auto_fix_error"] = str(e)

                validation_results["rule_results"].append(rule_result)

            except Exception as e:
                logger.error(f"ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ {rule.rule_id}: {e}")
                validation_results["failed_rules"] += 1
                validation_results["rule_results"].append(
                    {
                        "rule_id": rule.rule_id,
                        "name": rule.name,
                        "passed": False,
                        "error": str(e),
                    }
                )

        return validation_results

    def _validate_price_order(
        self, data: Any, context: Dict[str, Any]
    ) -> Tuple[bool, List[str]]:
        """ä¾¡æ ¼é †åºãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        issues = []

        if isinstance(data, pd.DataFrame) and all(
            col in data.columns for col in ["Low", "High", "Open", "Close"]
        ):
            invalid_rows = data[
                (data["Low"] > data["High"])
                | (data["Low"] > data["Open"])
                | (data["Low"] > data["Close"])
                | (data["High"] < data["Open"])
                | (data["High"] < data["Close"])
            ]

            if len(invalid_rows) > 0:
                issues.append(f"ä¾¡æ ¼é †åºç•°å¸¸: {len(invalid_rows)}ä»¶")
                return False, issues

        return True, issues

    def _fix_price_order(self, data: Any, issues: List[str]) -> Any:
        """ä¾¡æ ¼é †åºè‡ªå‹•ä¿®æ­£"""
        if isinstance(data, pd.DataFrame) and all(
            col in data.columns for col in ["Low", "High", "Open", "Close"]
        ):
            data_fixed = data.copy()
            # High = max(Open, High, Close)
            data_fixed["High"] = data_fixed[["Open", "High", "Close"]].max(axis=1)
            # Low = min(Open, Low, Close)
            data_fixed["Low"] = data_fixed[["Open", "Low", "Close"]].min(axis=1)
            return data_fixed

        return data

    def _validate_data_freshness(
        self, data: Any, context: Dict[str, Any]
    ) -> Tuple[bool, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿æ–°é®®åº¦ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        issues = []

        if isinstance(data, pd.DataFrame) and hasattr(data.index, "max"):
            try:
                latest_date = pd.to_datetime(data.index.max())
                age_hours = (datetime.now() - latest_date).total_seconds() / 3600

                if age_hours > 48:  # 48æ™‚é–“ä»¥ä¸Šå¤ã„
                    issues.append(f"ãƒ‡ãƒ¼ã‚¿ãŒå¤ã„: {age_hours:.1f}æ™‚é–“å‰")
                    return False, issues
            except Exception:
                pass

        return True, issues

    def _validate_data_completeness(
        self, data: Any, context: Dict[str, Any]
    ) -> Tuple[bool, List[str]]:
        """ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"""
        issues = []

        if isinstance(data, pd.DataFrame):
            missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
            if missing_ratio > 0.1:  # 10%ä»¥ä¸Šã®æ¬ æ
                issues.append(f"é«˜ã„æ¬ æç‡: {missing_ratio:.2%}")
                return False, issues

        return True, issues

    def _fix_missing_values(self, data: Any, issues: List[str]) -> Any:
        """æ¬ æå€¤è‡ªå‹•ä¿®æ­£"""
        if isinstance(data, pd.DataFrame):
            data_fixed = data.copy()

            # æ•°å€¤ã‚«ãƒ©ãƒ ã¯å‰æ–¹è£œé–“
            numeric_cols = data_fixed.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                data_fixed[col] = (
                    data_fixed[col].fillna(method="ffill").fillna(method="bfill")
                )

            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚«ãƒ©ãƒ ã¯æœ€é »å€¤
            categorical_cols = data_fixed.select_dtypes(include=["object"]).columns
            for col in categorical_cols:
                mode_val = data_fixed[col].mode()
                if len(mode_val) > 0:
                    data_fixed[col] = data_fixed[col].fillna(mode_val[0])

            return data_fixed

        return data

    def _record_lineage(
        self,
        pipeline_id: str,
        stage: PipelineStage,
        data: Any,
        processor_info: Dict[str, Any],
    ):
        """ãƒ‡ãƒ¼ã‚¿ç³»è­œè¨˜éŒ²"""
        if not self.enable_lineage:
            return

        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            data_hash = self._calculate_data_hash(data)

            # ç³»è­œãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
            lineage_record = DataLineageRecord(
                record_id=f"{pipeline_id}_{stage.value}_{int(time.time())}",
                event=DataLineageEvent.TRANSFORMED,
                timestamp=datetime.utcnow(),
                stage=stage,
                data_hash=data_hash,
                transformation=processor_info.get("processor_type", "unknown"),
                metadata=processor_info,
            )

            self.lineage_records.append(lineage_record)

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ç³»è­œè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def _calculate_data_hash(self, data: Any) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        try:
            if isinstance(data, pd.DataFrame):
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ§‹é€ ã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
                structure = (
                    f"{list(data.columns)}_{data.shape}_{str(data.dtypes.to_dict())}"
                )
                sample = str(data.head().to_dict()) if len(data) > 0 else ""
                content = f"{structure}_{sample}"
            else:
                content = str(data)

            return hashlib.md5(content.encode()).hexdigest()

        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return f"hash_error_{int(time.time())}"

    def _generate_cache_key(self, data: Any, data_type: str, symbol: str) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        data_hash = self._calculate_data_hash(data)
        return generate_unified_cache_key(
            "pipeline",
            data_type,
            symbol,
            self.validation_level.value,
            data_hash,
            time_bucket_minutes=60,
        )

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
        if not results["overall_success"]:
            recommendations.append(
                "ğŸš¨ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        # å‡¦ç†æ™‚é–“ã«é–¢ã™ã‚‹æ¨å¥¨äº‹é …
        if results["total_duration_ms"] > 5000:  # 5ç§’ä»¥ä¸Š
            recommendations.append(
                "âš ï¸ å‡¦ç†æ™‚é–“ãŒé•·ã„ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚„ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼è¨­å®šã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
            )

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …
        if "validation_results" in results:
            validation = results["validation_results"]
            if validation["failed_rules"] > 0:
                recommendations.append(
                    f"ğŸ” {validation['failed_rules']}ä»¶ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚"
                )

            if validation["auto_fixes_applied"]:
                recommendations.append(
                    f"ğŸ”§ {len(validation['auto_fixes_applied'])}ä»¶ã®è‡ªå‹•ä¿®æ­£ãŒé©ç”¨ã•ã‚Œã¾ã—ãŸã€‚"
                )

        # ã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥æ¨å¥¨äº‹é …
        for stage_name, stage_result in results["stages"].items():
            if not stage_result["success"]:
                recommendations.append(
                    f"âŒ {stage_name}æ®µéšã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )

        return recommendations[:5]  # ä¸Šä½5ä»¶ã«åˆ¶é™

    async def _save_pipeline_results(self, results: Dict[str, Any]):
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä¿å­˜"""
        try:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
            pipeline_id = results["pipeline_id"]

            # è©³ç´°çµæœã‚’JSONä¿å­˜
            result_file = (
                self.storage_path / f"pipeline_result_{pipeline_id}_{timestamp}.json"
            )

            # ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢ã«å¤‰æ›
            serializable_results = self._make_serializable(results)

            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(serializable_results, f, indent=2, ensure_ascii=False)

            logger.debug(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä¿å­˜å®Œäº†: {result_file}")

        except Exception as e:
            logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _make_serializable(self, obj: Any) -> Any:
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, pd.DataFrame):
            return f"DataFrame(shape={obj.shape}, columns={list(obj.columns)})"
        elif isinstance(obj, (datetime, pd.Timestamp)):
            return obj.isoformat()
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, Enum):
            return obj.value
        elif hasattr(obj, "__dict__"):
            return {k: self._make_serializable(v) for k, v in obj.__dict__.items()}
        else:
            return (
                str(obj)
                if not isinstance(obj, (str, int, float, bool, type(None)))
                else obj
            )

    async def get_pipeline_status(self) -> Dict[str, Any]:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çŠ¶æ…‹å–å¾—"""
        return {
            "validation_level": self.validation_level.value,
            "cache_enabled": self.enable_cache,
            "lineage_enabled": self.enable_lineage,
            "processors_count": sum(
                len(processors) for processors in self.processors.values()
            ),
            "validation_rules_count": len(self.validation_rules),
            "pipeline_history_count": len(self.pipeline_history),
            "lineage_records_count": len(self.lineage_records),
            "recent_success_rate": self._calculate_recent_success_rate(),
        }

    def _calculate_recent_success_rate(self) -> float:
        """ç›´è¿‘ã®æˆåŠŸç‡è¨ˆç®—"""
        if not self.pipeline_history:
            return 0.0

        recent_results = self.pipeline_history[-20:]  # æœ€æ–°20ä»¶
        success_count = sum(
            1 for result in recent_results if result.get("overall_success", False)
        )

        return success_count / len(recent_results)

    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")

        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®åˆ¶é™
        if len(self.pipeline_history) > 1000:
            self.pipeline_history = self.pipeline_history[-1000:]

        if len(self.lineage_records) > 5000:
            self.lineage_records = self.lineage_records[-5000:]

        logger.info("ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")


# Factory functions
def create_data_validation_pipeline(
    validation_level: ValidationLevel = ValidationLevel.STANDARD,
    enable_cache: bool = True,
    enable_lineage: bool = True,
    storage_path: str = "data/pipeline",
) -> DataValidationPipeline:
    """ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
    return DataValidationPipeline(
        validation_level=validation_level,
        enable_cache=enable_cache,
        enable_lineage=enable_lineage,
        storage_path=storage_path,
    )


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    async def test_data_validation_pipeline():
        print("=== Issue #420 ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ ===")

        try:
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
            pipeline = create_data_validation_pipeline(
                validation_level=ValidationLevel.STANDARD,
                enable_cache=True,
                enable_lineage=True,
                storage_path="test_pipeline",
            )

            print("\n1. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")
            status = await pipeline.get_pipeline_status()
            print(f"   ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«: {status['validation_level']}")
            print(f"   ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼æ•°: {status['processors_count']}")
            print(f"   ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«æ•°: {status['validation_rules_count']}")

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
            print("\n2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™...")
            test_price_data = pd.DataFrame(
                {
                    "Open": [2500, 2520, np.nan, 2480, 2510],
                    "High": [2550, 2540, 2490, 2500, 2530],
                    "Low": [2480, 2500, 2460, 2470, 2495],
                    "Close": [2530, 2485, 2475, 2495, 2525],
                    "Volume": [
                        1500000,
                        1200000,
                        1800000,
                        999999999,
                        1300000,
                    ],  # ç•°å¸¸å€¤å«ã‚€
                },
                index=pd.date_range("2024-01-01", periods=5),
            )

            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ
            print("\n3. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œä¸­...")
            results = await pipeline.process_data(
                test_price_data, "price", "7203", {"source": "test_data"}
            )

            print("\n=== ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†çµæœ ===")
            print(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ID: {results['pipeline_id']}")
            print(f"å‡¦ç†æˆåŠŸ: {'âœ…' if results['overall_success'] else 'âŒ'}")
            print(f"å‡¦ç†æ™‚é–“: {results['total_duration_ms']:.1f}ms")

            print("\nã‚¹ãƒ†ãƒ¼ã‚¸åˆ¥çµæœ:")
            for stage_name, stage_result in results["stages"].items():
                status_icon = "âœ…" if stage_result["success"] else "âŒ"
                print(
                    f"  {stage_name}: {status_icon} ({stage_result['duration_ms']:.1f}ms)"
                )

            if "validation_results" in results:
                validation = results["validation_results"]
                print("\nãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³çµæœ:")
                print(
                    f"  åˆæ ¼ãƒ«ãƒ¼ãƒ«: {validation['passed_rules']}/{validation['total_rules']}"
                )
                print(f"  è‡ªå‹•ä¿®æ­£é©ç”¨: {len(validation['auto_fixes_applied'])}ä»¶")

            print("\næ¨å¥¨äº‹é …:")
            for i, rec in enumerate(results["recommendations"], 1):
                print(f"  {i}. {rec}")

            print(f"\nãƒ‡ãƒ¼ã‚¿ç³»è­œè¨˜éŒ²: {len(results['lineage_records'])}ä»¶")

            # 2å›ç›®å‡¦ç†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆï¼‰
            print("\n4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ...")
            start_time = time.time()
            cached_results = await pipeline.process_data(
                test_price_data, "price", "7203"
            )
            cache_time = (time.time() - start_time) * 1000

            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‡¦ç†æ™‚é–“: {cache_time:.1f}ms")
            print(
                f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {'âœ…' if cache_time < results['total_duration_ms'] else 'âŒ'}"
            )

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            await pipeline.cleanup()

            print(
                "\nâœ… Issue #420 ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Œäº†"
            )

        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback

            traceback.print_exc()

    asyncio.run(test_data_validation_pipeline())
