#!/usr/bin/env python3
"""
ãƒ¡ã‚¤ãƒ³ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ãŸé«˜åº¦ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒ»æ•´åˆæ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import asyncio
import json
import logging
import threading
import time
from collections import defaultdict, deque
from datetime import datetime, timezone
from typing import Any, Callable, Dict, List, Optional

from .alert_manager import AlertManager
from .dashboard import DashboardManager
from .database_operations import DatabaseOperations
from .enums import MonitoringLevel
from .freshness_checker import FreshnessChecker
from .integrity_checker import IntegrityChecker
from .models import DataSourceConfig, DashboardData, MonitoringStats
from .recovery_manager import RecoveryManager
from .sla_metrics import SLAMetricsCalculator

# ä¾å­˜ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from ...monitoring.advanced_anomaly_detection_alerts import (
        AdvancedAnomalyAlertSystem,
    )
    from ...monitoring.structured_logging_enhancement import (
        StructuredLoggingEnhancementSystem,
    )
    from ...utils.unified_cache_manager import UnifiedCacheManager
    from ..comprehensive_data_quality_system import ComprehensiveDataQualitySystem
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False


class AdvancedFreshnessMonitor:
    """é«˜åº¦ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒ»æ•´åˆæ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
    ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®é®®åº¦ã€æ•´åˆæ€§ã€å“è³ªã‚’åŒ…æ‹¬çš„ã«ç›£è¦–ã—ã€
    è‡ªå‹•å›å¾©ã€ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ã€SLAãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.logger = logging.getLogger(__name__)
        
        # è¨­å®šç®¡ç†
        self.data_sources: Dict[str, DataSourceConfig] = {}
        self.monitoring_active = False
        self.monitor_thread: Optional[threading.Thread] = None
        
        # ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.db_operations = DatabaseOperations()
        self.freshness_checker = FreshnessChecker()
        self.integrity_checker = IntegrityChecker(self.db_operations)
        self.alert_manager = AlertManager(self.db_operations)
        self.recovery_manager = RecoveryManager(self.db_operations)
        self.sla_calculator = SLAMetricsCalculator(self.db_operations)
        self.dashboard_manager = DashboardManager(self.db_operations)
        
        # ç›£è¦–ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼‰
        self.recent_checks: Dict[str, deque] = defaultdict(lambda: deque(maxlen=1000))
        self.monitoring_stats = MonitoringStats()
        
        # å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ é€£æº
        self.quality_system = None
        self.anomaly_detector = None
        self.cache_manager = None
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self._initialize_external_components()
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        if config_path:
            self.load_config(config_path)
        
        self.logger.info("é«˜åº¦ãƒ‡ãƒ¼ã‚¿é®®åº¦ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def _initialize_external_components(self):
        """å¤–éƒ¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–"""
        if not DEPENDENCIES_AVAILABLE:
            self.logger.warning("ä¸€éƒ¨ã®ä¾å­˜ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
        
        try:
            self.quality_system = ComprehensiveDataQualitySystem()
            self.anomaly_detector = AdvancedAnomalyAlertSystem()
            self.cache_manager = UnifiedCacheManager()
            self.logger.info("å¤–éƒ¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            self.logger.warning(f"å¤–éƒ¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def add_data_source(self, config: DataSourceConfig):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¿½åŠ 
        
        Args:
            config: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
        """
        self.data_sources[config.source_id] = config
        
        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹åˆæœŸåŒ–
        from .models import DataSourceState
        initial_state = DataSourceState(
            source_id=config.source_id,
            current_status="unknown",
            consecutive_failures=0,
            recovery_attempts=0,
            metadata={
                "source_type": config.source_type.value,
                "monitoring_level": config.monitoring_level.value,
                "sla_target": config.sla_target,
            }
        )
        
        asyncio.create_task(
            self.db_operations.update_source_state(
                config.source_id, initial_state
            )
        )
        
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¿½åŠ : {config.source_id}")
    
    def remove_data_source(self, source_id: str):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å‰Šé™¤
        
        Args:
            source_id: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ID
        """
        if source_id in self.data_sources:
            del self.data_sources[source_id]
            
            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è§£æ±º
            active_alerts = self.alert_manager.get_active_alerts(source_id)
            for alert in active_alerts:
                asyncio.create_task(
                    self.alert_manager.resolve_alert(
                        alert.alert_id, 
                        "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å‰Šé™¤ã«ã‚ˆã‚Šè‡ªå‹•è§£æ±º"
                    )
                )
            
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å‰Šé™¤: {source_id}")
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if self.monitoring_active:
            self.logger.warning("ç›£è¦–ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™")
            return
        
        self.monitoring_active = True
        self.monitor_thread = threading.Thread(
            target=self._monitoring_loop, 
            daemon=True
        )
        self.monitor_thread.start()
        
        self.logger.info("ğŸ” ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒ»æ•´åˆæ€§ç›£è¦–é–‹å§‹")
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring_active = False
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
        
        self.logger.info("â¹ï¸ ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒ»æ•´åˆæ€§ç›£è¦–åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰å®Ÿè¡Œï¼‰"""
        self.logger.info("ç›£è¦–ãƒ«ãƒ¼ãƒ—é–‹å§‹")
        
        while self.monitoring_active:
            try:
                # å„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                for source_config in self.data_sources.values():
                    loop.run_until_complete(
                        self._check_data_source(source_config)
                    )
                
                # SLAãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆæ¯æ™‚0åˆ†ï¼‰
                current_time = datetime.now(timezone.utc)
                if current_time.minute == 0:
                    loop.run_until_complete(
                        self.sla_calculator.calculate_hourly_sla_metrics(
                            self.data_sources
                        )
                    )
                
                loop.close()
                
                # 10ç§’å¾…æ©Ÿ
                time.sleep(10)
                
            except Exception as e:
                self.logger.error(f"ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(30)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯é•·ã‚ã«å¾…æ©Ÿ
        
        self.logger.info("ç›£è¦–ãƒ«ãƒ¼ãƒ—çµ‚äº†")
    
    async def _check_data_source(self, config: DataSourceConfig):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯
        
        Args:
            config: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
        """
        try:
            source_id = config.source_id
            current_time = datetime.now(timezone.utc)
            
            # é®®åº¦ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            freshness_result = await self.freshness_checker.check_freshness(config)
            
            # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œï¼ˆç›£è¦–ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ï¼‰
            integrity_results = []
            if config.monitoring_level in [
                MonitoringLevel.COMPREHENSIVE,
                MonitoringLevel.CRITICAL,
            ]:
                integrity_results = await self.integrity_checker.check_integrity(config)
            
            # çµæœä¿å­˜
            await self.db_operations.save_freshness_check(freshness_result)
            for integrity_result in integrity_results:
                await self.db_operations.save_integrity_check(integrity_result)
            
            # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self.recent_checks[source_id].append(freshness_result)
            self.monitoring_stats.total_checks_performed += 1
            
            # ã‚¢ãƒ©ãƒ¼ãƒˆè©•ä¾¡
            await self.alert_manager.evaluate_alerts(
                config, freshness_result, integrity_results
            )
            
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹æ›´æ–°
            await self._update_source_state(
                config, freshness_result, integrity_results
            )
            
            # å›å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³è©•ä¾¡
            await self.recovery_manager.evaluate_recovery_actions(
                config, freshness_result, integrity_results
            )
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ ({config.source_id}): {e}")
    
    async def _update_source_state(
        self,
        config: DataSourceConfig,
        freshness_result,
        integrity_results: List
    ):
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹æ›´æ–°
        
        Args:
            config: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®š
            freshness_result: é®®åº¦ãƒã‚§ãƒƒã‚¯çµæœ
            integrity_results: æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœãƒªã‚¹ãƒˆ
        """
        try:
            current_time = datetime.now(timezone.utc)
            
            # ç¾åœ¨ã®çŠ¶æ…‹å–å¾—
            source_state = self.db_operations.get_source_state(config.source_id)
            if not source_state:
                from .models import DataSourceState
                source_state = DataSourceState(
                    source_id=config.source_id,
                    consecutive_failures=0,
                    recovery_attempts=0
                )
            
            # ç·åˆå¥å…¨æ€§åˆ¤å®š
            from .enums import FreshnessStatus
            is_healthy = (
                freshness_result.status == FreshnessStatus.FRESH and
                all(check.passed for check in integrity_results)
            )
            
            # çŠ¶æ…‹æ›´æ–°
            if is_healthy:
                source_state.consecutive_failures = 0
                source_state.current_status = "healthy"
                source_state.last_success = current_time
            else:
                source_state.consecutive_failures += 1
                source_state.current_status = "unhealthy"
            
            source_state.last_check = current_time
            
            await self.db_operations.update_source_state(
                config.source_id, source_state
            )
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çŠ¶æ…‹æ›´æ–°ã‚¨ãƒ©ãƒ¼ ({config.source_id}): {e}")
    
    async def get_monitoring_dashboard(self, hours: int = 24) -> DashboardData:
        """ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—
        
        Args:
            hours: è¡¨ç¤ºå¯¾è±¡æœŸé–“ï¼ˆæ™‚é–“ï¼‰
            
        Returns:
            ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        """
        try:
            return await self.dashboard_manager.get_monitoring_dashboard(
                self.data_sources, hours
            )
        except Exception as e:
            self.logger.error(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return DashboardData(
                overview={"error": str(e)},
                source_summary=[],
                recent_alerts=[],
                sla_summary=[],
                generated_at=datetime.now(timezone.utc).isoformat(),
                time_range_hours=hours,
                error=str(e),
            )
    
    async def generate_health_report(self, period_days: int = 7) -> Dict[str, Any]:
        """å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        
        Args:
            period_days: ãƒ¬ãƒãƒ¼ãƒˆæœŸé–“ï¼ˆæ—¥æ•°ï¼‰
            
        Returns:
            å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆ
        """
        try:
            return await self.dashboard_manager.generate_health_report(
                self.data_sources, period_days
            )
        except Exception as e:
            self.logger.error(f"å¥å…¨æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "generated_at": datetime.now(timezone.utc).isoformat(),
            }
    
    def get_system_statistics(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±å–å¾—
        
        Returns:
            çµ±åˆã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±
        """
        return {
            "monitoring_stats": self.monitoring_stats.to_dict(),
            "alert_stats": self.alert_manager.get_alert_statistics(),
            "recovery_stats": self.recovery_manager.get_recovery_statistics(),
            "data_sources": {
                "total_count": len(self.data_sources),
                "by_type": self._count_sources_by_type(),
                "by_monitoring_level": self._count_sources_by_monitoring_level(),
            },
            "system_status": {
                "monitoring_active": self.monitoring_active,
                "external_components": {
                    "quality_system": self.quality_system is not None,
                    "anomaly_detector": self.anomaly_detector is not None,
                    "cache_manager": self.cache_manager is not None,
                },
            },
        }
    
    def _count_sources_by_type(self) -> Dict[str, int]:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç¨®åˆ¥åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ"""
        count = {}
        for config in self.data_sources.values():
            source_type = config.source_type.value
            count[source_type] = count.get(source_type, 0) + 1
        return count
    
    def _count_sources_by_monitoring_level(self) -> Dict[str, int]:
        """ç›£è¦–ãƒ¬ãƒ™ãƒ«åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ"""
        count = {}
        for config in self.data_sources.values():
            level = config.monitoring_level.value
            count[level] = count.get(level, 0) + 1
        return count
    
    def add_alert_callback(self, callback: Callable):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ 
        
        Args:
            callback: ã‚¢ãƒ©ãƒ¼ãƒˆç™ºç”Ÿæ™‚ã«å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        """
        self.alert_manager.add_alert_callback(callback)
    
    def add_recovery_callback(self, action, callback: Callable):
        """å›å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¿½åŠ 
        
        Args:
            action: å›å¾©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç¨®åˆ¥
            callback: å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        """
        self.recovery_manager.add_recovery_callback(action, callback)
    
    async def resolve_alert(self, alert_id: str, resolution_notes: str = ""):
        """ã‚¢ãƒ©ãƒ¼ãƒˆè§£æ±º
        
        Args:
            alert_id: ã‚¢ãƒ©ãƒ¼ãƒˆID
            resolution_notes: è§£æ±ºãƒãƒ¼ãƒˆ
        """
        await self.alert_manager.resolve_alert(alert_id, resolution_notes)
    
    def load_config(self, config_path: str):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        try:
            with open(config_path, encoding="utf-8") as f:
                config_data = json.load(f)
            
            for source_data in config_data.get("data_sources", []):
                from .enums import DataSourceType, MonitoringLevel, RecoveryAction
                
                config = DataSourceConfig(
                    source_id=source_data["source_id"],
                    source_type=DataSourceType(source_data["source_type"]),
                    endpoint_url=source_data.get("endpoint_url"),
                    connection_params=source_data.get("connection_params", {}),
                    expected_frequency=source_data.get("expected_frequency", 60),
                    freshness_threshold=source_data.get("freshness_threshold", 300),
                    quality_threshold=source_data.get("quality_threshold", 80.0),
                    monitoring_level=MonitoringLevel(
                        source_data.get("monitoring_level", "standard")
                    ),
                    enable_recovery=source_data.get("enable_recovery", True),
                    recovery_strategy=RecoveryAction(
                        source_data.get("recovery_strategy", "retry")
                    ),
                    max_retry_attempts=source_data.get("max_retry_attempts", 3),
                    sla_target=source_data.get("sla_target", 99.9),
                )
                self.add_data_source(config)
            
            self.logger.info(
                f"è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {len(self.data_sources)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹"
            )
            
        except Exception as e:
            self.logger.error(f"è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


# Factory function
def create_advanced_freshness_monitor(
    config_path: Optional[str] = None,
) -> AdvancedFreshnessMonitor:
    """é«˜åº¦ãƒ‡ãƒ¼ã‚¿é®®åº¦ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ä½œæˆ
    
    Args:
        config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        é«˜åº¦ãƒ‡ãƒ¼ã‚¿é®®åº¦ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return AdvancedFreshnessMonitor(config_path)