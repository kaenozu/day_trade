#!/usr/bin/env python3
"""
Accuracy Benchmark System for Ensemble Learning

Issue #462å¯¾å¿œ: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®äºˆæ¸¬ç²¾åº¦95%è¶…é”æˆã®ãŸã‚ã®
åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨æ”¹å–„ææ¡ˆã‚·ã‚¹ãƒ†ãƒ 
"""

import time
import warnings
from typing import Dict, List, Any, Tuple, Optional, NamedTuple
import numpy as np
import pandas as pd
from dataclasses import dataclass
from pathlib import Path

warnings.filterwarnings("ignore")

from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error, explained_variance_score
)
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from .ensemble_system import EnsembleSystem, EnsembleConfig, EnsembleMethod
from ..utils.logging_config import get_context_logger

logger = get_context_logger(__name__)


class AccuracyMetrics(NamedTuple):
    """ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    mse: float
    rmse: float
    mae: float
    r2_score: float
    mape: float
    explained_variance: float
    hit_rate: float
    sharpe_ratio: float
    max_drawdown: float
    accuracy_percentage: float


@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""
    # ãƒ‡ãƒ¼ã‚¿è¨­å®š
    train_size: float = 0.7
    val_size: float = 0.15
    test_size: float = 0.15

    # æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼
    n_splits: int = 5
    gap: int = 0  # å­¦ç¿’ãƒ»æ¤œè¨¼é–“ã®ã‚®ãƒ£ãƒƒãƒ—

    # ç‰¹å¾´é‡è¨­å®š
    n_features: int = 20
    sequence_length: int = 30  # LSTMç”¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
    calculate_financial_metrics: bool = True
    generate_detailed_report: bool = True

    # æœ€é©åŒ–è¨­å®š
    enable_hyperparameter_tuning: bool = True
    max_optimization_time: int = 300  # ç§’


class AccuracyBenchmark:
    """
    ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

    ç¾åœ¨ã®äºˆæ¸¬ç²¾åº¦ã‚’æ¸¬å®šã—ã€95%é”æˆã«å‘ã‘ãŸæ”¹å–„ææ¡ˆã‚’æä¾›
    """

    def __init__(self, config: Optional[BenchmarkConfig] = None):
        """
        åˆæœŸåŒ–

        Args:
            config: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
        """
        self.config = config or BenchmarkConfig()
        self.benchmark_results: Dict[str, Any] = {}
        self.improvement_suggestions: List[str] = []

        logger.info(f"AccuracyBenchmarkåˆæœŸåŒ–å®Œäº†: {self.config}")

    def generate_synthetic_stock_data(self, n_samples: int = 5000,
                                    add_noise: bool = True,
                                    trend_strength: float = 0.3) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """
        é«˜å“è³ªãªåˆæˆæ ªä¾¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

        Args:
            n_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
            add_noise: ãƒã‚¤ã‚ºè¿½åŠ ãƒ•ãƒ©ã‚°
            trend_strength: ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦

        Returns:
            ç‰¹å¾´é‡ã€ç›®æ¨™å¤‰æ•°ã€ç‰¹å¾´é‡åã®ã‚¿ãƒ—ãƒ«
        """
        np.random.seed(42)  # å†ç¾æ€§ç¢ºä¿

        # æ™‚ç³»åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        time_idx = np.arange(n_samples)

        # åŸºæœ¬çš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»å­£ç¯€æ€§
        trend = trend_strength * (time_idx / n_samples)
        seasonal = 0.1 * np.sin(2 * np.pi * time_idx / 252)  # å¹´æ¬¡å­£ç¯€æ€§
        weekly = 0.05 * np.sin(2 * np.pi * time_idx / 5)     # é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³

        # æŠ€è¡“æŒ‡æ¨™ãƒ™ãƒ¼ã‚¹ã®ç‰¹å¾´é‡ç”Ÿæˆ
        features = []
        feature_names = []

        # 1. ä¾¡æ ¼é–¢é€£ç‰¹å¾´é‡
        base_price = 100 + trend + seasonal + weekly
        if add_noise:
            base_price += np.random.normal(0, 0.5, n_samples)

        # ç§»å‹•å¹³å‡
        for window in [5, 10, 20]:
            ma = pd.Series(base_price).rolling(window).mean().fillna(method='bfill').values
            features.append(ma)
            feature_names.append(f'MA_{window}')

        # 2. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡
        returns = np.diff(base_price, prepend=base_price[0])
        for window in [5, 10, 20]:
            vol = pd.Series(returns).rolling(window).std().fillna(0).values
            features.append(vol)
            feature_names.append(f'VOL_{window}')

        # 3. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç‰¹å¾´é‡
        for lag in [1, 5, 10]:
            momentum = np.concatenate([
                np.zeros(lag),
                np.diff(base_price, lag)
            ])[:n_samples]
            features.append(momentum)
            feature_names.append(f'MOMENTUM_{lag}')

        # 4. RSIé¢¨æŒ‡æ¨™
        rsi_like = np.tanh(returns / np.std(returns)) * 50 + 50
        features.append(rsi_like)
        feature_names.append('RSI_LIKE')

        # 5. ãƒ©ãƒ³ãƒ€ãƒ ç‰¹å¾´é‡ï¼ˆãƒã‚¤ã‚ºãƒ†ã‚¹ãƒˆç”¨ï¼‰
        for i in range(3):
            noise_feature = np.random.normal(0, 1, n_samples)
            features.append(noise_feature)
            feature_names.append(f'NOISE_{i}')

        # ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ä½œæˆ
        X = np.column_stack(features)

        # ç›®æ¨™å¤‰æ•°ï¼šç¿Œæ—¥ãƒªã‚¿ãƒ¼ãƒ³äºˆæ¸¬
        y = np.concatenate([
            returns[1:],
            [returns[-1]]  # æœ€å¾Œã®å€¤ã¯å‰ã®å€¤ã‚’ã‚³ãƒ”ãƒ¼
        ])

        # æ­£è¦åŒ–
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

        logger.info(f"åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {X.shape}, target range: [{y.min():.4f}, {y.max():.4f}]")

        return X, y, feature_names

    def calculate_comprehensive_metrics(self, y_true: np.ndarray,
                                      y_pred: np.ndarray) -> AccuracyMetrics:
        """
        åŒ…æ‹¬çš„ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—

        Args:
            y_true: å®Ÿéš›ã®å€¤
            y_pred: äºˆæ¸¬å€¤

        Returns:
            AccuracyMetrics: åŒ…æ‹¬çš„ãªç²¾åº¦æŒ‡æ¨™
        """
        # åŸºæœ¬çš„ãªå›å¸°æŒ‡æ¨™
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        # MAPEï¼ˆã‚¼ãƒ­é™¤ç®—å¯¾ç­–ï¼‰
        try:
            mape = mean_absolute_percentage_error(y_true, y_pred)
        except:
            mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100

        # åˆ†æ•£èª¬æ˜ç‡
        explained_var = explained_variance_score(y_true, y_pred)

        # Hit Rateï¼ˆæ–¹å‘æ€§äºˆæ¸¬ç²¾åº¦ï¼‰
        if len(y_true) > 1:
            true_directions = np.sign(np.diff(y_true))
            pred_directions = np.sign(np.diff(y_pred))
            hit_rate = np.mean(true_directions == pred_directions)
        else:
            hit_rate = 0.5

        # é‡‘èæŒ‡æ¨™
        sharpe_ratio = self._calculate_sharpe_ratio(y_pred, y_true)
        max_drawdown = self._calculate_max_drawdown(y_pred)

        # ç·åˆç²¾åº¦ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆè¤‡æ•°æŒ‡æ¨™ã®çµ„ã¿åˆã‚ã›ï¼‰
        accuracy_components = [
            max(0, r2 * 100),  # RÂ² to percentage
            max(0, (1 - rmse / np.std(y_true)) * 100),  # RMSE normalized
            hit_rate * 100,  # Hit rate as percentage
            max(0, (1 - mape / 100) * 100)  # MAPE inverted
        ]
        accuracy_percentage = np.mean(accuracy_components)

        return AccuracyMetrics(
            mse=mse,
            rmse=rmse,
            mae=mae,
            r2_score=r2,
            mape=mape,
            explained_variance=explained_var,
            hit_rate=hit_rate,
            sharpe_ratio=sharpe_ratio,
            max_drawdown=max_drawdown,
            accuracy_percentage=min(99.99, accuracy_percentage)  # Cap at 99.99%
        )

    def _calculate_sharpe_ratio(self, predictions: np.ndarray,
                               actual: np.ndarray,
                               risk_free_rate: float = 0.02) -> float:
        """Sharpeæ¯”è¨ˆç®—"""
        try:
            strategy_returns = predictions - np.mean(predictions)
            if np.std(strategy_returns) == 0:
                return 0.0
            sharpe = (np.mean(strategy_returns) - risk_free_rate) / np.std(strategy_returns)
            return sharpe
        except:
            return 0.0

    def _calculate_max_drawdown(self, values: np.ndarray) -> float:
        """æœ€å¤§ãƒ‰ãƒ­ãƒ¼ãƒ€ã‚¦ãƒ³è¨ˆç®—"""
        try:
            cumulative = np.cumsum(values)
            running_max = np.maximum.accumulate(cumulative)
            drawdown = running_max - cumulative
            return np.max(drawdown) / (np.max(running_max) + 1e-8) * 100
        except:
            return 0.0

    def run_cross_validation_benchmark(self, X: np.ndarray, y: np.ndarray,
                                     feature_names: List[str],
                                     ensemble_config: Optional[EnsembleConfig] = None) -> Dict[str, Any]:
        """
        æ™‚ç³»åˆ—äº¤å·®æ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

        Args:
            X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
            y: ç›®æ¨™å¤‰æ•°
            feature_names: ç‰¹å¾´é‡å
            ensemble_config: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®š

        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        if ensemble_config is None:
            ensemble_config = EnsembleConfig()

        logger.info(f"äº¤å·®æ¤œè¨¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: {X.shape}, CV={self.config.n_splits}åˆ†å‰²")

        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(
            n_splits=self.config.n_splits,
            gap=self.config.gap
        )

        cv_results = []
        fold_predictions = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            logger.info(f"Fold {fold + 1}/{self.config.n_splits} å®Ÿè¡Œä¸­...")

            # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            val_size = int(len(train_idx) * 0.2)
            if val_size > 0:
                X_val = X_train[-val_size:]
                y_val = y_train[-val_size:]
                X_train = X_train[:-val_size]
                y_train = y_train[:-val_size]
                validation_data = (X_val, y_val)
            else:
                validation_data = None

            try:
                # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚·ã‚¹ãƒ†ãƒ ä½œæˆãƒ»å­¦ç¿’
                ensemble = EnsembleSystem(ensemble_config)

                start_time = time.time()
                train_results = ensemble.fit(X_train, y_train,
                                           validation_data=validation_data,
                                           feature_names=feature_names)
                training_time = time.time() - start_time

                # äºˆæ¸¬å®Ÿè¡Œ
                prediction = ensemble.predict(X_test, method=EnsembleMethod.WEIGHTED)

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
                metrics = self.calculate_comprehensive_metrics(y_test, prediction.final_predictions)

                fold_result = {
                    'fold': fold + 1,
                    'train_size': len(X_train),
                    'test_size': len(X_test),
                    'training_time': training_time,
                    'prediction_time': prediction.processing_time,
                    'metrics': metrics._asdict(),
                    'individual_predictions': prediction.individual_predictions,
                    'model_weights': prediction.model_weights,
                    'ensemble_confidence': np.mean(prediction.ensemble_confidence)
                }

                cv_results.append(fold_result)
                fold_predictions.append({
                    'y_true': y_test,
                    'y_pred': prediction.final_predictions,
                    'fold': fold + 1
                })

                logger.info(f"Fold {fold + 1} å®Œäº†: ç²¾åº¦={metrics.accuracy_percentage:.2f}%, "
                          f"Hit Rate={metrics.hit_rate:.3f}")

            except Exception as e:
                logger.error(f"Fold {fold + 1} ã‚¨ãƒ©ãƒ¼: {e}")
                cv_results.append({
                    'fold': fold + 1,
                    'error': str(e),
                    'metrics': None
                })

        # çµæœé›†è¨ˆ
        successful_folds = [r for r in cv_results if 'error' not in r]

        if successful_folds:
            # å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            avg_metrics = {}
            for metric_name in successful_folds[0]['metrics'].keys():
                values = [fold['metrics'][metric_name] for fold in successful_folds]
                avg_metrics[metric_name] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }

            # å…¨ä½“çµæœ
            benchmark_result = {
                'cross_validation_summary': {
                    'n_successful_folds': len(successful_folds),
                    'n_total_folds': self.config.n_splits,
                    'success_rate': len(successful_folds) / self.config.n_splits,
                    'avg_metrics': avg_metrics,
                    'individual_fold_results': cv_results,
                    'fold_predictions': fold_predictions
                },
                'ensemble_config': ensemble_config.__dict__,
                'benchmark_config': self.config.__dict__,
                'execution_timestamp': time.time()
            }

            # æ”¹å–„ææ¡ˆç”Ÿæˆ
            self._generate_improvement_suggestions(avg_metrics, successful_folds)
            benchmark_result['improvement_suggestions'] = self.improvement_suggestions

            logger.info(f"äº¤å·®æ¤œè¨¼å®Œäº†: å¹³å‡ç²¾åº¦={avg_metrics['accuracy_percentage']['mean']:.2f}% "
                       f"(Â±{avg_metrics['accuracy_percentage']['std']:.2f}%)")
        else:
            benchmark_result = {
                'error': 'å…¨ã¦ã®Foldã§å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ',
                'failed_results': cv_results
            }

        return benchmark_result

    def _generate_improvement_suggestions(self, avg_metrics: Dict[str, Dict[str, float]],
                                        fold_results: List[Dict[str, Any]]):
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        suggestions = []

        current_accuracy = avg_metrics['accuracy_percentage']['mean']
        target_accuracy = 95.0

        suggestions.append(f"ç¾åœ¨ã®å¹³å‡ç²¾åº¦: {current_accuracy:.2f}%")
        suggestions.append(f"ç›®æ¨™ç²¾åº¦: {target_accuracy:.2f}%")
        suggestions.append(f"æ”¹å–„å¿…è¦é‡: {target_accuracy - current_accuracy:.2f}%")

        # RÂ²ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®ææ¡ˆ
        r2_score = avg_metrics['r2_score']['mean']
        if r2_score < 0.9:
            suggestions.append("âœ¦ RÂ²ã‚¹ã‚³ã‚¢æ”¹å–„ææ¡ˆ:")
            suggestions.append("  - ã‚ˆã‚Šè¤‡é›‘ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
            suggestions.append("  - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å¼·åŒ–")
            suggestions.append("  - è¿½åŠ ã®éç·šå½¢ãƒ¢ãƒ‡ãƒ«å°å…¥")

        # Hit Rateæ”¹å–„ææ¡ˆ
        hit_rate = avg_metrics['hit_rate']['mean']
        if hit_rate < 0.85:
            suggestions.append("âœ¦ Hit Rateæ”¹å–„ææ¡ˆ:")
            suggestions.append("  - æ–¹å‘æ€§äºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«è¿½åŠ ")
            suggestions.append("  - åˆ†é¡ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å°å…¥")
            suggestions.append("  - æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ãƒ¢ãƒ‡ãƒ«å¼·åŒ–")

        # RMSEæ”¹å–„ææ¡ˆ
        rmse_normalized = 1 - avg_metrics['rmse']['mean'] / np.sqrt(avg_metrics['mse']['mean'])
        if rmse_normalized < 0.9:
            suggestions.append("âœ¦ RMSEæ”¹å–„ææ¡ˆ:")
            suggestions.append("  - ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ”¹å–„")
            suggestions.append("  - å¤–ã‚Œå€¤é™¤å»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å°å…¥")
            suggestions.append("  - ã‚ˆã‚Šé«˜ç²¾åº¦ãªåŸºåº•ãƒ¢ãƒ‡ãƒ«è¿½åŠ ")

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«é‡ã¿åˆ†æ
        weight_analysis = self._analyze_model_weights(fold_results)
        if weight_analysis:
            suggestions.extend(weight_analysis)

        # 95%é”æˆã®ãŸã‚ã®å…·ä½“çš„ææ¡ˆ
        suggestions.append("\nğŸ¯ 95%ç²¾åº¦é”æˆã®ãŸã‚ã®å„ªå…ˆé †ä½:")
        accuracy_gap = target_accuracy - current_accuracy

        if accuracy_gap > 10:
            suggestions.append("1. ã€é«˜å„ªå…ˆåº¦ã€‘åŸºåº•ãƒ¢ãƒ‡ãƒ«ã®å¤§å¹…å¼·åŒ–")
            suggestions.append("   - XGBoost/CatBoostè¿½åŠ ")
            suggestions.append("   - Neural Networkç³»ãƒ¢ãƒ‡ãƒ«æ”¹è‰¯")
            suggestions.append("2. ã€é«˜å„ªå…ˆåº¦ã€‘ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
            suggestions.append("   - é«˜æ¬¡ç‰¹å¾´é‡ä½œæˆ")
            suggestions.append("   - å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±åˆ")
        elif accuracy_gap > 5:
            suggestions.append("1. ã€ä¸­å„ªå…ˆåº¦ã€‘ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–")
            suggestions.append("   - Optuna/Hyperoptå°å…¥")
            suggestions.append("   - Grid Searchå¼·åŒ–")
            suggestions.append("2. ã€ä¸­å„ªå…ˆåº¦ã€‘ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•æ”¹è‰¯")
            suggestions.append("   - Stackingå±¤æ•°å¢—åŠ ")
            suggestions.append("   - å‹•çš„é‡ã¿èª¿æ•´æ”¹å–„")
        else:
            suggestions.append("1. ã€ä½å„ªå…ˆåº¦ã€‘å¾®èª¿æ•´æœ€é©åŒ–")
            suggestions.append("   - å­¦ç¿’ç‡èª¿æ•´")
            suggestions.append("   - æ­£å‰‡åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")

        self.improvement_suggestions = suggestions

    def _analyze_model_weights(self, fold_results: List[Dict[str, Any]]) -> List[str]:
        """ãƒ¢ãƒ‡ãƒ«é‡ã¿åˆ†æã¨ææ¡ˆ"""
        suggestions = []

        try:
            # å„ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡é‡ã¿è¨ˆç®—
            all_weights = {}
            for result in fold_results:
                if 'model_weights' in result:
                    for model, weight in result['model_weights'].items():
                        if model not in all_weights:
                            all_weights[model] = []
                        all_weights[model].append(weight)

            if all_weights:
                suggestions.append("âœ¦ ãƒ¢ãƒ‡ãƒ«é‡ã¿åˆ†æ:")
                avg_weights = {model: np.mean(weights) for model, weights in all_weights.items()}

                # é‡ã¿ãŒä½ã„ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
                sorted_weights = sorted(avg_weights.items(), key=lambda x: x[1], reverse=True)

                for model, weight in sorted_weights:
                    suggestions.append(f"  - {model}: {weight:.3f}")

                # æœ€ã‚‚é‡ã¿ã®ä½ã„ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ææ¡ˆ
                lowest_weight_model = sorted_weights[-1][0]
                if sorted_weights[-1][1] < 0.1:
                    suggestions.append(f"  âš  {lowest_weight_model} ã®é‡ã¿ãŒä½ã„ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãŒå¿…è¦")

        except Exception as e:
            logger.warning(f"é‡ã¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        return suggestions

    def generate_benchmark_report(self, results: Dict[str, Any],
                                output_path: Optional[str] = None) -> str:
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

        Args:
            results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            output_path: å‡ºåŠ›ãƒ‘ã‚¹

        Returns:
            ãƒ¬ãƒãƒ¼ãƒˆæ–‡å­—åˆ—
        """
        report_lines = []

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        report_lines.extend([
            "=" * 80,
            "ENSEMBLE LEARNING ACCURACY BENCHMARK REPORT",
            f"Issue #462: äºˆæ¸¬ç²¾åº¦95%è¶…é”æˆã®ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆ†æ",
            "=" * 80,
            f"å®Ÿè¡Œæ™‚åˆ»: {pd.Timestamp.now()}",
            ""
        ])

        # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        if 'error' in results:
            report_lines.extend([
                "âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼",
                f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {results['error']}",
                ""
            ])
            return "\n".join(report_lines)

        # ã‚µãƒãƒªãƒ¼æƒ…å ±
        cv_summary = results['cross_validation_summary']
        avg_metrics = cv_summary['avg_metrics']

        report_lines.extend([
            "ğŸ“Š BENCHMARK SUMMARY",
            "-" * 50,
            f"äº¤å·®æ¤œè¨¼Foldæ•°: {cv_summary['n_successful_folds']}/{cv_summary['n_total_folds']}",
            f"æˆåŠŸç‡: {cv_summary['success_rate']:.1%}",
            ""
        ])

        # ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        report_lines.extend([
            "ğŸ¯ ACCURACY METRICS",
            "-" * 50
        ])

        key_metrics = ['accuracy_percentage', 'r2_score', 'hit_rate', 'rmse', 'mae']
        for metric in key_metrics:
            if metric in avg_metrics:
                m = avg_metrics[metric]
                report_lines.append(f"{metric.upper():20s}: {m['mean']:6.3f} Â± {m['std']:5.3f} "
                                  f"[{m['min']:6.3f}, {m['max']:6.3f}]")

        report_lines.append("")

        # 95%é”æˆè©•ä¾¡
        current_accuracy = avg_metrics['accuracy_percentage']['mean']
        target_accuracy = 95.0
        gap = target_accuracy - current_accuracy

        report_lines.extend([
            "ğŸ¯ 95% ACCURACY TARGET ANALYSIS",
            "-" * 50,
            f"ç¾åœ¨ã®ç²¾åº¦:     {current_accuracy:6.2f}%",
            f"ç›®æ¨™ç²¾åº¦:       {target_accuracy:6.2f}%",
            f"å¿…è¦æ”¹å–„é‡:     {gap:6.2f}%",
            f"é”æˆåº¦:         {(current_accuracy/target_accuracy)*100:6.1f}%",
            ""
        ])

        # é”æˆå¯èƒ½æ€§è©•ä¾¡
        if gap <= 1:
            status = "âœ… ã»ã¼é”æˆ - å¾®èª¿æ•´ã§95%é”æˆå¯èƒ½"
        elif gap <= 3:
            status = "ğŸŸ¡ æœ‰æœ› - ä¸­ç¨‹åº¦ã®æ”¹å–„ã§95%é”æˆå¯èƒ½"
        elif gap <= 8:
            status = "ğŸŸ  èª²é¡Œ - å¤§å¹…æ”¹å–„ãŒå¿…è¦ã ãŒé”æˆå¯èƒ½"
        else:
            status = "ğŸ”´ å›°é›£ - æ ¹æœ¬çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒå¤‰æ›´ãŒå¿…è¦"

        report_lines.extend([
            f"é”æˆå¯èƒ½æ€§è©•ä¾¡: {status}",
            ""
        ])

        # æ”¹å–„ææ¡ˆ
        if 'improvement_suggestions' in results:
            report_lines.extend([
                "ğŸ’¡ IMPROVEMENT SUGGESTIONS",
                "-" * 50
            ])
            for suggestion in results['improvement_suggestions']:
                report_lines.append(suggestion)
            report_lines.append("")

        # å€‹åˆ¥Foldè©³ç´°
        if self.config.generate_detailed_report:
            report_lines.extend([
                "ğŸ“‹ DETAILED FOLD RESULTS",
                "-" * 50
            ])

            for fold_result in cv_summary['individual_fold_results']:
                if 'error' not in fold_result:
                    metrics = fold_result['metrics']
                    report_lines.extend([
                        f"Fold {fold_result['fold']}:",
                        f"  Accuracy: {metrics['accuracy_percentage']:6.2f}%",
                        f"  Hit Rate: {metrics['hit_rate']:6.3f}",
                        f"  RÂ² Score: {metrics['r2_score']:6.3f}",
                        f"  Training Time: {fold_result['training_time']:6.1f}s",
                        ""
                    ])

        # ãƒ•ãƒƒã‚¿ãƒ¼
        report_lines.extend([
            "=" * 80,
            "ğŸ“ RECOMMENDATIONS FOR 95% ACCURACY",
            "-" * 50,
            "1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å¼·åŒ–",
            "2. é«˜åº¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å°å…¥",
            "3. ã‚ˆã‚Šå¼·åŠ›ãªåŸºåº•ãƒ¢ãƒ‡ãƒ«è¿½åŠ ",
            "4. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ”¹è‰¯",
            "5. ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Šã¨ãƒã‚¤ã‚ºé™¤å»",
            "=" * 80
        ])

        report_text = "\n".join(report_lines)

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        if output_path:
            try:
                Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(report_text)
                logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
            except Exception as e:
                logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return report_text

    def run_full_benchmark(self, data_size: int = 3000,
                         ensemble_configs: Optional[List[EnsembleConfig]] = None,
                         output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

        Args:
            data_size: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            ensemble_configs: ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨­å®šã®ãƒªã‚¹ãƒˆ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        logger.info(f"ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º={data_size}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        if ensemble_configs is None:
            ensemble_configs = [
                # åŸºæœ¬è¨­å®š
                EnsembleConfig(
                    use_lstm_transformer=False,  # é«˜é€ŸåŒ–ã®ãŸã‚ç„¡åŠ¹
                    use_random_forest=True,
                    use_gradient_boosting=True,
                    use_svr=True,
                    enable_stacking=False,
                    enable_dynamic_weighting=False
                ),
                # é«˜ç²¾åº¦è¨­å®š
                EnsembleConfig(
                    use_lstm_transformer=False,  # é«˜é€ŸåŒ–ã®ãŸã‚ç„¡åŠ¹
                    use_random_forest=True,
                    use_gradient_boosting=True,
                    use_svr=True,
                    enable_stacking=True,
                    enable_dynamic_weighting=True
                )
            ]

        # åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        X, y, feature_names = self.generate_synthetic_stock_data(n_samples=data_size)

        # å„è¨­å®šã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        all_results = {}
        best_config = None
        best_accuracy = 0

        for i, config in enumerate(ensemble_configs):
            config_name = f"config_{i+1}"
            logger.info(f"è¨­å®š {config_name} ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹...")

            try:
                results = self.run_cross_validation_benchmark(X, y, feature_names, config)

                if 'cross_validation_summary' in results:
                    accuracy = results['cross_validation_summary']['avg_metrics']['accuracy_percentage']['mean']
                    logger.info(f"è¨­å®š {config_name} å®Œäº†: ç²¾åº¦={accuracy:.2f}%")

                    if accuracy > best_accuracy:
                        best_accuracy = accuracy
                        best_config = config_name
                else:
                    logger.warning(f"è¨­å®š {config_name} å¤±æ•—")

                all_results[config_name] = results

            except Exception as e:
                logger.error(f"è¨­å®š {config_name} ã‚¨ãƒ©ãƒ¼: {e}")
                all_results[config_name] = {'error': str(e)}

        # çµ±åˆçµæœä½œæˆ
        full_benchmark_result = {
            'individual_config_results': all_results,
            'best_configuration': best_config,
            'best_accuracy': best_accuracy,
            'data_info': {
                'n_samples': data_size,
                'n_features': len(feature_names),
                'feature_names': feature_names
            },
            'benchmark_config': self.config.__dict__,
            'execution_info': {
                'timestamp': time.time(),
                'n_configs_tested': len(ensemble_configs)
            }
        }

        # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if output_dir:
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            # å„è¨­å®šã®ãƒ¬ãƒãƒ¼ãƒˆ
            for config_name, results in all_results.items():
                if 'error' not in results:
                    report_path = output_dir / f"benchmark_report_{config_name}.txt"
                    self.generate_benchmark_report(results, str(report_path))

            # çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
            summary_report = self._generate_summary_report(full_benchmark_result)
            with open(output_dir / "benchmark_summary.txt", 'w', encoding='utf-8') as f:
                f.write(summary_report)

        logger.info(f"ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: æœ€é«˜ç²¾åº¦={best_accuracy:.2f}% (è¨­å®š: {best_config})")

        return full_benchmark_result

    def _generate_summary_report(self, results: Dict[str, Any]) -> str:
        """çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        lines = [
            "=" * 80,
            "ENSEMBLE LEARNING FULL BENCHMARK SUMMARY",
            f"Issue #462: 95%ç²¾åº¦é”æˆã®ãŸã‚ã®åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯",
            "=" * 80,
            f"å®Ÿè¡Œæ™‚åˆ»: {pd.Timestamp.now()}",
            f"ãƒ†ã‚¹ãƒˆè¨­å®šæ•°: {results['execution_info']['n_configs_tested']}",
            "",
            "ğŸ† BEST PERFORMANCE",
            "-" * 50,
            f"æœ€é«˜ç²¾åº¦: {results['best_accuracy']:.2f}%",
            f"æœ€é©è¨­å®š: {results['best_configuration']}",
            "",
            "ğŸ“ˆ CONFIGURATION COMPARISON",
            "-" * 50
        ]

        # å„è¨­å®šã®æ¯”è¼ƒ
        for config_name, config_results in results['individual_config_results'].items():
            if 'error' not in config_results and 'cross_validation_summary' in config_results:
                cv_summary = config_results['cross_validation_summary']
                accuracy = cv_summary['avg_metrics']['accuracy_percentage']['mean']
                accuracy_std = cv_summary['avg_metrics']['accuracy_percentage']['std']
                hit_rate = cv_summary['avg_metrics']['hit_rate']['mean']

                lines.extend([
                    f"{config_name.upper()}:",
                    f"  ç²¾åº¦: {accuracy:6.2f}% Â± {accuracy_std:4.2f}%",
                    f"  Hit Rate: {hit_rate:6.3f}",
                    f"  æˆåŠŸç‡: {cv_summary['success_rate']:.1%}",
                    ""
                ])

        # 95%é”æˆã®ãŸã‚ã®æœ€çµ‚ææ¡ˆ
        best_accuracy = results['best_accuracy']
        gap_to_95 = 95.0 - best_accuracy

        lines.extend([
            "ğŸ¯ 95% ACCURACY ACHIEVEMENT PLAN",
            "-" * 50,
            f"ç¾åœ¨ã®æœ€é«˜ç²¾åº¦: {best_accuracy:6.2f}%",
            f"ç›®æ¨™ã¨ã®å·®:     {gap_to_95:6.2f}%",
            ""
        ])

        if gap_to_95 <= 0:
            lines.append("ğŸ‰ 95%ç²¾åº¦é”æˆæ¸ˆã¿ï¼")
        elif gap_to_95 <= 2:
            lines.extend([
                "âœ… 95%é”æˆã¾ã§ã‚ã¨ã‚ãšã‹ï¼",
                "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:",
                "1. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç´°ã‹ã„èª¿æ•´",
                "2. ç‰¹å¾´é‡ã®å¾®èª¿æ•´",
                "3. ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã®æ”¹å–„"
            ])
        elif gap_to_95 <= 5:
            lines.extend([
                "ğŸŸ¡ ä¸­ç¨‹åº¦ã®æ”¹å–„ã§95%é”æˆå¯èƒ½",
                "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:",
                "1. ã‚ˆã‚Šå¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ ",
                "2. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®æ”¹è‰¯",
                "3. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¼·åŒ–"
            ])
        else:
            lines.extend([
                "ğŸ”´ å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦",
                "æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:",
                "1. æ ¹æœ¬çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´",
                "2. å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®çµ±åˆ",
                "3. ã‚ˆã‚Šé«˜åº¦ãªæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å°å…¥"
            ])

        lines.extend([
            "",
            "=" * 80,
            "è©³ç´°ãªæ”¹å–„ææ¡ˆã¯å„è¨­å®šã®å€‹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚",
            "=" * 80
        ])

        return "\n".join(lines)


def run_accuracy_benchmark_demo():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("=" * 60)
    print("Issue #462: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ç²¾åº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)

    try:
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
        benchmark_config = BenchmarkConfig(
            n_splits=3,  # ãƒ‡ãƒ¢ç”¨ã«é«˜é€ŸåŒ–
            generate_detailed_report=True
        )

        benchmark = AccuracyBenchmark(benchmark_config)

        print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")

        # ãƒ•ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        results = benchmark.run_full_benchmark(
            data_size=1000,  # ãƒ‡ãƒ¢ç”¨ã«å°ã•ãªã‚µã‚¤ã‚º
            output_dir="benchmark_reports"
        )

        # çµæœè¡¨ç¤º
        best_accuracy = results['best_accuracy']
        best_config = results['best_configuration']

        print(f"\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")
        print(f"æœ€é«˜ç²¾åº¦: {best_accuracy:.2f}%")
        print(f"æœ€é©è¨­å®š: {best_config}")
        print(f"95%é”æˆã¾ã§: {95.0 - best_accuracy:.2f}%ã®æ”¹å–„ãŒå¿…è¦")

        if best_accuracy >= 95.0:
            print("95%ç²¾åº¦é”æˆï¼")
        elif best_accuracy >= 90.0:
            print("90%è¶…é”æˆ - ã‚‚ã†å°‘ã—ã§95%ï¼")
        else:
            print("ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¿…è¦")

        print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: benchmark_reports/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„")

        return True

    except Exception as e:
        print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return False


if __name__ == "__main__":
    success = run_accuracy_benchmark_demo()
    exit(0 if success else 1)