#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced Prediction System - æ”¹è‰¯äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

Issue #810å¯¾å¿œï¼šäºˆæ¸¬ç²¾åº¦å‘ä¸Šã‚·ã‚¹ãƒ†ãƒ 
ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã€æ”¹è‰¯ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€é«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’
"""

import asyncio
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import sqlite3
from pathlib import Path
import pickle
import warnings
warnings.filterwarnings('ignore')

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

@dataclass
class EnhancedPrediction:
    """æ”¹è‰¯äºˆæ¸¬çµæœ"""
    symbol: str
    prediction: int  # -1: ä¸‹è½, 0: æ¨ªã°ã„, 1: ä¸Šæ˜‡
    confidence: float
    probability_distribution: Dict[int, float]  # {-1: prob, 0: prob, 1: prob}
    model_consensus: Dict[str, int]  # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
    feature_importance: Dict[str, float]
    timestamp: datetime
    data_quality_score: float
    prediction_horizon: str = "1day"

@dataclass
class ModelPerformanceMetrics:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    model_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    training_samples: int
    feature_count: int
    last_updated: datetime

class AdvancedFeatureEngineering:
    """é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def create_comprehensive_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ"""

        if len(data) < 50:  # æœ€ä½é™ã®ãƒ‡ãƒ¼ã‚¿æ•°
            self.logger.warning(f"ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(data)}ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€ä½50å¿…è¦ï¼‰")

        features = pd.DataFrame(index=data.index)

        # 1. ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        features.update(self._create_price_features(data))

        # 2. æŠ€è¡“æŒ‡æ¨™ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        features.update(self._create_technical_indicators(data))

        # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡
        features.update(self._create_volatility_features(data))

        # 4. å‡ºæ¥é«˜åˆ†æç‰¹å¾´é‡
        features.update(self._create_volume_features(data))

        # 5. æ™‚ç³»åˆ—ç‰¹å¾´é‡
        features.update(self._create_temporal_features(data))

        # 6. ç›¸å¯¾å¼·åº¦ç‰¹å¾´é‡
        features.update(self._create_momentum_features(data))

        # 7. çµ±è¨ˆçš„ç‰¹å¾´é‡
        features.update(self._create_statistical_features(data))

        # 8. ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ç‰¹å¾´é‡
        features.update(self._create_pattern_features(data))

        # æ¬ æå€¤å‡¦ç†ï¼ˆæ”¹è‰¯ç‰ˆã€éæ¨å¥¨è­¦å‘Šå¯¾å¿œï¼‰
        features = features.ffill().bfill()
        features = features.replace([np.inf, -np.inf], 0)

        self.logger.info(f"åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆå®Œäº†: {len(features.columns)}ç‰¹å¾´é‡, {len(features)}ã‚µãƒ³ãƒ—ãƒ«")

        return features

    def _create_price_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ä¾¡æ ¼ãƒ™ãƒ¼ã‚¹ç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        if 'Close' in data.columns:
            close = data['Close']

            # åŸºæœ¬ãƒªã‚¿ãƒ¼ãƒ³
            features['return_1d'] = close.pct_change(1)
            features['return_3d'] = close.pct_change(3)
            features['return_5d'] = close.pct_change(5)
            features['return_10d'] = close.pct_change(10)
            features['return_20d'] = close.pct_change(20)

            # å¯¾æ•°ãƒªã‚¿ãƒ¼ãƒ³
            features['log_return_1d'] = np.log(close / close.shift(1))
            features['log_return_5d'] = np.log(close / close.shift(5))

            # ä¾¡æ ¼ç›¸å¯¾ä½ç½®
            features['price_rank_5d'] = close.rolling(5).rank() / 5
            features['price_rank_10d'] = close.rolling(10).rank() / 10
            features['price_rank_20d'] = close.rolling(20).rank() / 20

            # ç§»å‹•å¹³å‡ã‹ã‚‰ã®ä¹–é›¢ç‡
            for period in [5, 10, 20, 50]:
                sma = close.rolling(period).mean()
                features[f'sma_deviation_{period}d'] = (close - sma) / sma

        # OHLCç‰¹å¾´é‡
        if all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):
            features['hl_ratio'] = (data['High'] - data['Low']) / data['Close']
            features['oc_ratio'] = (data['Close'] - data['Open']) / data['Open']
            features['body_ratio'] = abs(data['Close'] - data['Open']) / (data['High'] - data['Low'] + 1e-10)
            features['upper_shadow'] = (data['High'] - np.maximum(data['Open'], data['Close'])) / data['Close']
            features['lower_shadow'] = (np.minimum(data['Open'], data['Close']) - data['Low']) / data['Close']

        return features

    def _create_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
        """æŠ€è¡“æŒ‡æ¨™ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""

        features = pd.DataFrame(index=data.index)

        if 'Close' not in data.columns:
            return features

        close = data['Close']

        # ç§»å‹•å¹³å‡ç³»
        for period in [5, 10, 20, 50]:
            sma = close.rolling(period).mean()
            ema = close.ewm(span=period).mean()

            features[f'sma_{period}'] = sma / close
            features[f'ema_{period}'] = ema / close
            features[f'sma_slope_{period}'] = sma.diff() / sma
            features[f'ema_slope_{period}'] = ema.diff() / ema

        # RSIï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        for period in [9, 14, 21]:
            delta = close.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / (loss + 1e-10)
            features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        for period in [10, 20]:
            sma = close.rolling(period).mean()
            std = close.rolling(period).std()
            features[f'bb_upper_{period}'] = (sma + 2*std) / close
            features[f'bb_lower_{period}'] = (sma - 2*std) / close
            features[f'bb_width_{period}'] = (4*std) / sma
            features[f'bb_position_{period}'] = (close - sma) / (2*std + 1e-10)

        # MACD
        ema12 = close.ewm(span=12).mean()
        ema26 = close.ewm(span=26).mean()
        macd_line = ema12 - ema26
        signal_line = macd_line.ewm(span=9).mean()
        features['macd'] = macd_line / close
        features['macd_signal'] = signal_line / close
        features['macd_histogram'] = (macd_line - signal_line) / close

        # ã‚¹ãƒˆã‚­ãƒ£ã‚¹ãƒ†ã‚£ã‚¯ã‚¹
        if all(col in data.columns for col in ['High', 'Low']):
            for period in [9, 14]:
                lowest_low = data['Low'].rolling(period).min()
                highest_high = data['High'].rolling(period).max()
                k_percent = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-10)
                features[f'stoch_k_{period}'] = k_percent
                features[f'stoch_d_{period}'] = k_percent.rolling(3).mean()

        # Williams %R
        if all(col in data.columns for col in ['High', 'Low']):
            for period in [14, 21]:
                highest_high = data['High'].rolling(period).max()
                lowest_low = data['Low'].rolling(period).min()
                features[f'williams_r_{period}'] = -100 * (highest_high - close) / (highest_high - lowest_low + 1e-10)

        return features

    def _create_volatility_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        if 'Close' not in data.columns:
            return features

        returns = data['Close'].pct_change()

        # æ­´å²çš„ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆè¤‡æ•°æœŸé–“ï¼‰
        for period in [5, 10, 20, 30]:
            vol = returns.rolling(period).std() * np.sqrt(252)
            features[f'volatility_{period}d'] = vol
            features[f'volatility_rank_{period}d'] = vol.rolling(60).rank() / 60

        # GARCHé¢¨ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        features['ewm_volatility'] = returns.ewm(span=30).std() * np.sqrt(252)

        # True Range
        if all(col in data.columns for col in ['High', 'Low', 'Close']):
            high_low = data['High'] - data['Low']
            high_close = np.abs(data['High'] - data['Close'].shift())
            low_close = np.abs(data['Low'] - data['Close'].shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))

            # ATR
            for period in [14, 20]:
                atr = true_range.rolling(period).mean()
                features[f'atr_{period}'] = atr / data['Close']
                features[f'atr_ratio_{period}'] = atr / atr.rolling(60).mean()

        # ãƒ‘ãƒ¼ã‚­ãƒ³ã‚½ãƒ³æ¨å®šå€¤
        if all(col in data.columns for col in ['High', 'Low']):
            features['parkinson_vol'] = np.sqrt(
                0.361 * (np.log(data['High'] / data['Low'])) ** 2
            )

        return features

    def _create_volume_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """å‡ºæ¥é«˜åˆ†æç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        if 'Volume' not in data.columns:
            return features

        volume = data['Volume']

        # å‡ºæ¥é«˜ç§»å‹•å¹³å‡
        for period in [5, 10, 20]:
            vol_sma = volume.rolling(period).mean()
            features[f'volume_sma_{period}'] = volume / vol_sma
            features[f'volume_trend_{period}'] = vol_sma.pct_change()

        # å‡ºæ¥é«˜ãƒ©ãƒ³ã‚¯
        features['volume_rank_20d'] = volume.rolling(20).rank() / 20
        features['volume_rank_60d'] = volume.rolling(60).rank() / 60

        # ä¾¡æ ¼ãƒ»å‡ºæ¥é«˜é–¢ä¿‚
        if 'Close' in data.columns:
            price_change = data['Close'].pct_change()
            features['pv_trend'] = price_change * volume
            features['volume_price_correlation'] = price_change.rolling(20).corr(volume.pct_change())

        # OBVï¼ˆOn Balance Volumeï¼‰
        if 'Close' in data.columns:
            price_change = data['Close'].diff()
            obv = (np.sign(price_change) * volume).cumsum()
            features['obv'] = obv / obv.rolling(20).mean()
            features['obv_slope'] = obv.pct_change()

        return features

    def _create_temporal_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ™‚ç³»åˆ—ç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        # æ›œæ—¥ãƒ»æœˆåŠ¹æœ
        if hasattr(data.index, 'dayofweek'):
            features['day_of_week'] = data.index.dayofweek
            features['is_monday'] = (data.index.dayofweek == 0).astype(int)
            features['is_friday'] = (data.index.dayofweek == 4).astype(int)
            features['month'] = data.index.month

        # æœŸé–“åŠ¹æœ
        features['quarter'] = data.index.quarter if hasattr(data.index, 'quarter') else 1

        # ãƒ©ã‚°ç‰¹å¾´é‡
        if 'Close' in data.columns:
            returns = data['Close'].pct_change()
            for lag in [1, 2, 3, 5]:
                features[f'return_lag_{lag}'] = returns.shift(lag)

        return features

    def _create_momentum_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ç›¸å¯¾å¼·åº¦ç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        if 'Close' not in data.columns:
            return features

        close = data['Close']

        # Rate of Change (ROC)
        for period in [5, 10, 15, 20]:
            features[f'roc_{period}'] = (close - close.shift(period)) / close.shift(period)

        # Momentum
        for period in [5, 10, 20]:
            features[f'momentum_{period}'] = close - close.shift(period)

        # Price Oscillator
        features['price_osc'] = (close.ewm(span=12).mean() - close.ewm(span=26).mean()) / close

        return features

    def _create_statistical_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """çµ±è¨ˆçš„ç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        if 'Close' not in data.columns:
            return features

        close = data['Close']
        returns = close.pct_change()

        # çµ±è¨ˆçš„ãƒ¢ãƒ¼ãƒ¡ãƒ³ãƒˆ
        for period in [10, 20]:
            features[f'skewness_{period}'] = returns.rolling(period).skew()
            features[f'kurtosis_{period}'] = returns.rolling(period).kurt()

        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
        for period in [10, 20]:
            price_bins = pd.cut(close.rolling(period).apply(lambda x: x.iloc[-1]), bins=5, labels=False)
            features[f'entropy_{period}'] = price_bins

        return features

    def _create_pattern_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ç‰¹å¾´é‡"""

        features = pd.DataFrame(index=data.index)

        if not all(col in data.columns for col in ['Open', 'High', 'Low', 'Close']):
            return features

        # ãƒ­ãƒ¼ã‚½ã‚¯è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³
        body = data['Close'] - data['Open']
        upper_shadow = data['High'] - np.maximum(data['Open'], data['Close'])
        lower_shadow = np.minimum(data['Open'], data['Close']) - data['Low']

        # Doji
        features['is_doji'] = (abs(body) / (data['High'] - data['Low'] + 1e-10) < 0.1).astype(int)

        # Hammer/Hanging Man
        features['is_hammer'] = ((lower_shadow > 2 * abs(body)) & (upper_shadow < abs(body))).astype(int)

        # Gap
        features['gap_up'] = (data['Open'] > data['High'].shift()).astype(int)
        features['gap_down'] = (data['Open'] < data['Low'].shift()).astype(int)

        return features

class EnhancedModelEnsemble:
    """æ”¹è‰¯ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.performance_metrics = {}

        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.model_configs = {
            'random_forest': {
                'model': RandomForestClassifier(
                    n_estimators=200,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=-1
                ),
                'use_scaler': False
            },
            'xgboost': {
                'model': xgb.XGBClassifier(
                    n_estimators=200,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    random_state=42,
                    n_jobs=-1,
                    eval_metric='mlogloss'
                ),
                'use_scaler': True
            },
            'gradient_boosting': {
                'model': GradientBoostingClassifier(
                    n_estimators=200,
                    max_depth=5,
                    learning_rate=0.1,
                    random_state=42
                ),
                'use_scaler': True
            },
            'logistic_regression': {
                'model': LogisticRegression(
                    max_iter=1000,
                    random_state=42,
                    multi_class='ovr'
                ),
                'use_scaler': True
            },
            'svm': {
                'model': SVC(
                    probability=True,
                    random_state=42
                ),
                'use_scaler': True
            }
        }

    def train_models(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""

        training_results = {}

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å¤‰æ›ï¼ˆ3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼š-1, 0, 1ï¼‰
        y_classes = self._create_target_classes(y)

        # æ™‚ç³»åˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=3)

        for model_name, config in self.model_configs.items():
            try:
                model = config['model']
                use_scaler = config['use_scaler']

                # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                X_processed = X.copy()
                if use_scaler:
                    scaler = RobustScaler()
                    X_processed = pd.DataFrame(
                        scaler.fit_transform(X_processed),
                        columns=X_processed.columns,
                        index=X_processed.index
                    )
                    self.scalers[model_name] = scaler

                # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                cv_scores = []
                for train_idx, val_idx in tscv.split(X_processed):
                    X_train_cv, X_val_cv = X_processed.iloc[train_idx], X_processed.iloc[val_idx]
                    y_train_cv, y_val_cv = y_classes.iloc[train_idx], y_classes.iloc[val_idx]

                    model_cv = model.__class__(**model.get_params())
                    model_cv.fit(X_train_cv, y_train_cv)
                    y_pred_cv = model_cv.predict(X_val_cv)
                    cv_scores.append(accuracy_score(y_val_cv, y_pred_cv))

                # å…¨ãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚å­¦ç¿’
                model.fit(X_processed, y_classes)
                self.models[model_name] = model

                # ç‰¹å¾´é‡é‡è¦åº¦
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(X.columns, model.feature_importances_))
                    self.feature_importance[model_name] = importance

                # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                y_pred = model.predict(X_processed)
                metrics = ModelPerformanceMetrics(
                    model_name=model_name,
                    accuracy=accuracy_score(y_classes, y_pred),
                    precision=precision_score(y_classes, y_pred, average='weighted', zero_division=0),
                    recall=recall_score(y_classes, y_pred, average='weighted', zero_division=0),
                    f1_score=f1_score(y_classes, y_pred, average='weighted', zero_division=0),
                    training_samples=len(X_processed),
                    feature_count=len(X.columns),
                    last_updated=datetime.now()
                )
                self.performance_metrics[model_name] = metrics

                training_results[model_name] = {
                    'cv_accuracy': np.mean(cv_scores),
                    'cv_std': np.std(cv_scores),
                    'final_accuracy': metrics.accuracy,
                    'feature_count': len(X.columns)
                }

                self.logger.info(f"{model_name} å­¦ç¿’å®Œäº†: CVç²¾åº¦={np.mean(cv_scores):.3f}Â±{np.std(cv_scores):.3f}")

            except Exception as e:
                self.logger.error(f"{model_name} å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                training_results[model_name] = {'error': str(e)}

        return training_results

    def predict(self, X: pd.DataFrame) -> EnhancedPrediction:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""

        if not self.models:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        predictions = {}
        probabilities = {}

        # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        for model_name, model in self.models.items():
            try:
                X_processed = X.copy()

                # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                if model_name in self.scalers:
                    scaler = self.scalers[model_name]
                    X_processed = pd.DataFrame(
                        scaler.transform(X_processed),
                        columns=X_processed.columns,
                        index=X_processed.index
                    )

                # äºˆæ¸¬å®Ÿè¡Œ
                pred = model.predict(X_processed.iloc[-1:].values)[0]
                prob = model.predict_proba(X_processed.iloc[-1:].values)[0]

                predictions[model_name] = pred
                probabilities[model_name] = prob

            except Exception as e:
                self.logger.error(f"{model_name} äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
                predictions[model_name] = 0
                probabilities[model_name] = np.array([0.33, 0.34, 0.33])

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        final_prediction, final_confidence, prob_dist = self._ensemble_prediction(predictions, probabilities)

        # ç‰¹å¾´é‡é‡è¦åº¦ï¼ˆå¹³å‡ï¼‰
        avg_importance = {}
        if self.feature_importance:
            all_features = set()
            for importance_dict in self.feature_importance.values():
                all_features.update(importance_dict.keys())

            for feature in all_features:
                importance_values = [
                    importance_dict.get(feature, 0)
                    for importance_dict in self.feature_importance.values()
                ]
                avg_importance[feature] = np.mean(importance_values)

        return EnhancedPrediction(
            symbol=X.index[-1] if hasattr(X.index[-1], 'strftime') else "unknown",
            prediction=final_prediction,
            confidence=final_confidence,
            probability_distribution=prob_dist,
            model_consensus=predictions,
            feature_importance=dict(sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:10]),
            timestamp=datetime.now(),
            data_quality_score=95.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        )

    def _create_target_classes(self, returns: pd.Series, threshold: float = 0.01) -> pd.Series:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ä½œæˆ"""

        future_returns = returns.shift(-1)  # ç¿Œæ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³

        conditions = [
            future_returns < -threshold,  # ä¸‹è½ï¼ˆ-1ï¼‰
            future_returns > threshold,   # ä¸Šæ˜‡ï¼ˆ1ï¼‰
        ]
        choices = [-1, 1]

        y_classes = np.select(conditions, choices, default=0)  # æ¨ªã°ã„ï¼ˆ0ï¼‰

        return pd.Series(y_classes, index=returns.index)

    def _ensemble_prediction(self, predictions: Dict[str, int],
                           probabilities: Dict[str, np.ndarray]) -> Tuple[int, float, Dict[int, float]]:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬"""

        if not predictions:
            return 0, 0.0, {-1: 0.33, 0: 0.34, 1: 0.33}

        # é‡ã¿ï¼ˆæ€§èƒ½ã«åŸºã¥ãï¼‰
        weights = {}
        for model_name in predictions.keys():
            if model_name in self.performance_metrics:
                weights[model_name] = self.performance_metrics[model_name].accuracy
            else:
                weights[model_name] = 0.5

        # é‡ã¿ã®æ­£è¦åŒ–
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {k: v/total_weight for k, v in weights.items()}
        else:
            weights = {k: 1/len(predictions) for k in predictions.keys()}

        # é‡ã¿ä»˜ãæŠ•ç¥¨
        weighted_probs = np.zeros(3)  # [-1, 0, 1]ã«å¯¾å¿œ

        for model_name, prob in probabilities.items():
            if model_name in weights:
                weighted_probs += prob * weights[model_name]

        # æœ€çµ‚äºˆæ¸¬
        class_mapping = {0: -1, 1: 0, 2: 1}  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ -> ã‚¯ãƒ©ã‚¹
        final_class_idx = np.argmax(weighted_probs)
        final_prediction = class_mapping[final_class_idx]
        final_confidence = weighted_probs[final_class_idx]

        prob_distribution = {
            -1: weighted_probs[0],
            0: weighted_probs[1],
            1: weighted_probs[2]
        }

        return final_prediction, final_confidence, prob_distribution

class EnhancedPredictionSystem:
    """æ”¹è‰¯äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.feature_engineer = AdvancedFeatureEngineering()
        self.model_ensemble = EnhancedModelEnsemble()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = Path("enhanced_prediction_data/models_and_performance.db")
        self.db_path.parent.mkdir(exist_ok=True)
        self._init_database()

        self.logger.info("Enhanced prediction system initialized")

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS model_performance (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        model_name TEXT NOT NULL,
                        accuracy REAL NOT NULL,
                        precision_score REAL NOT NULL,
                        recall_score REAL NOT NULL,
                        f1_score REAL NOT NULL,
                        training_samples INTEGER NOT NULL,
                        feature_count INTEGER NOT NULL,
                        timestamp TEXT NOT NULL
                    )
                ''')

                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS predictions_log (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        prediction INTEGER NOT NULL,
                        confidence REAL NOT NULL,
                        timestamp TEXT NOT NULL,
                        data_quality_score REAL,
                        actual_result INTEGER
                    )
                ''')

                conn.commit()

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    async def train_enhanced_models(self, symbol: str) -> Dict[str, Any]:
        """æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«å­¦ç¿’"""

        try:
            # ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ6ãƒ¶æœˆï¼‰
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, "6mo")

            if data is None or len(data) < 100:
                return {"error": f"å­¦ç¿’ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“: {len(data) if data else 0}ä»¶"}

            # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
            from data_quality_manager import data_quality_manager
            quality_result = await data_quality_manager.evaluate_data_quality(symbol)
            data_quality_score = quality_result.get('overall_score', 50)

            if data_quality_score < 70:
                self.logger.warning(f"ãƒ‡ãƒ¼ã‚¿å“è³ªãŒä½ã„: {data_quality_score:.1f}/100")

            # æ”¹è‰¯ç‰¹å¾´é‡ä½œæˆ
            features = self.feature_engineer.create_comprehensive_features(data)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆ
            returns = data['Close'].pct_change().dropna()

            # ãƒ‡ãƒ¼ã‚¿æ•´åˆ—
            common_index = features.index.intersection(returns.index)
            X = features.loc[common_index].dropna()
            y = returns.loc[X.index]

            if len(X) < 50:
                return {"error": f"å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ä¸è¶³: {len(X)}ä»¶ï¼ˆæœ€ä½50ä»¶å¿…è¦ï¼‰"}

            # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
            training_results = self.model_ensemble.train_models(X, y)

            # çµæœä¿å­˜
            await self._save_performance_metrics(symbol, training_results)

            # å­¦ç¿’çµæœã‚µãƒãƒªãƒ¼
            avg_accuracy = np.mean([
                result.get('final_accuracy', 0)
                for result in training_results.values()
                if 'error' not in result
            ])

            return {
                "symbol": symbol,
                "training_samples": len(X),
                "feature_count": len(X.columns),
                "data_quality_score": data_quality_score,
                "avg_model_accuracy": avg_accuracy,
                "model_results": training_results,
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            self.logger.error(f"æ”¹è‰¯ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return {"error": str(e)}

    async def predict_with_enhanced_models(self, symbol: str) -> Optional[EnhancedPrediction]:
        """æ”¹è‰¯äºˆæ¸¬å®Ÿè¡Œ"""

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, "6mo")

            if data is None or len(data) < 50:
                self.logger.error(f"äºˆæ¸¬ç”¨ãƒ‡ãƒ¼ã‚¿ä¸è¶³: {len(data) if data else 0}ä»¶")
                return None

            # ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
            from data_quality_manager import data_quality_manager
            quality_result = await data_quality_manager.evaluate_data_quality(symbol)
            data_quality_score = quality_result.get('overall_score', 50)

            # ç‰¹å¾´é‡ä½œæˆ
            features = self.feature_engineer.create_comprehensive_features(data)

            if len(features.columns) == 0:
                self.logger.error("ç‰¹å¾´é‡ãŒä½œæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return None

            # äºˆæ¸¬å®Ÿè¡Œ
            prediction = self.model_ensemble.predict(features)
            prediction.symbol = symbol
            prediction.data_quality_score = data_quality_score

            # äºˆæ¸¬ãƒ­ã‚°ä¿å­˜
            await self._save_prediction_log(prediction)

            self.logger.info(f"æ”¹è‰¯äºˆæ¸¬å®Œäº†: {symbol}, äºˆæ¸¬={prediction.prediction}, ä¿¡é ¼åº¦={prediction.confidence:.3f}")

            return prediction

        except Exception as e:
            self.logger.error(f"æ”¹è‰¯äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return None

    async def _save_performance_metrics(self, symbol: str, training_results: Dict[str, Any]):
        """æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                timestamp = datetime.now().isoformat()

                for model_name, model_metrics in self.model_ensemble.performance_metrics.items():
                    cursor.execute('''
                        INSERT INTO model_performance
                        (symbol, model_name, accuracy, precision_score, recall_score,
                         f1_score, training_samples, feature_count, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        symbol,
                        model_name,
                        model_metrics.accuracy,
                        model_metrics.precision,
                        model_metrics.recall,
                        model_metrics.f1_score,
                        model_metrics.training_samples,
                        model_metrics.feature_count,
                        timestamp
                    ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    async def _save_prediction_log(self, prediction: EnhancedPrediction):
        """äºˆæ¸¬ãƒ­ã‚°ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                cursor.execute('''
                    INSERT INTO predictions_log
                    (symbol, prediction, confidence, timestamp, data_quality_score)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    prediction.symbol,
                    prediction.prediction,
                    prediction.confidence,
                    prediction.timestamp.isoformat(),
                    prediction.data_quality_score
                ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"äºˆæ¸¬ãƒ­ã‚°ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
enhanced_prediction_system = EnhancedPredictionSystem()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def run_enhanced_prediction_test():
    """æ”¹è‰¯äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""

    print("=== ğŸš€ æ”¹è‰¯äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")

    test_symbols = ["7203", "8306"]

    for symbol in test_symbols:
        print(f"\n--- {symbol} æ”¹è‰¯äºˆæ¸¬ãƒ†ã‚¹ãƒˆ ---")

        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
        print("ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
        training_result = await enhanced_prediction_system.train_enhanced_models(symbol)

        if 'error' in training_result:
            print(f"âŒ å­¦ç¿’å¤±æ•—: {training_result['error']}")
            continue

        print(f"âœ… å­¦ç¿’å®Œäº†:")
        print(f"  å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«æ•°: {training_result['training_samples']}")
        print(f"  ç‰¹å¾´é‡æ•°: {training_result['feature_count']}")
        print(f"  ãƒ‡ãƒ¼ã‚¿å“è³ª: {training_result['data_quality_score']:.1f}/100")
        print(f"  å¹³å‡ç²¾åº¦: {training_result['avg_model_accuracy']:.1%}")

        # äºˆæ¸¬å®Ÿè¡Œ
        print("äºˆæ¸¬å®Ÿè¡Œä¸­...")
        prediction = await enhanced_prediction_system.predict_with_enhanced_models(symbol)

        if prediction:
            print(f"âœ… äºˆæ¸¬å®Œäº†:")
            print(f"  äºˆæ¸¬: {prediction.prediction} ({'ä¸Šæ˜‡' if prediction.prediction > 0 else 'ä¸‹è½' if prediction.prediction < 0 else 'æ¨ªã°ã„'})")
            print(f"  ä¿¡é ¼åº¦: {prediction.confidence:.1%}")
            print(f"  ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢: {prediction.data_quality_score:.1f}/100")

            # ç¢ºç‡åˆ†å¸ƒè¡¨ç¤º
            print(f"  ç¢ºç‡åˆ†å¸ƒ:")
            for direction, prob in prediction.probability_distribution.items():
                direction_name = {-1: 'ä¸‹è½', 0: 'æ¨ªã°ã„', 1: 'ä¸Šæ˜‡'}[direction]
                print(f"    {direction_name}: {prob:.1%}")

            # ãƒ¢ãƒ‡ãƒ«åˆæ„è¡¨ç¤º
            print(f"  ãƒ¢ãƒ‡ãƒ«åˆæ„:")
            for model, pred in prediction.model_consensus.items():
                pred_name = {-1: 'ä¸‹è½', 0: 'æ¨ªã°ã„', 1: 'ä¸Šæ˜‡'}[pred]
                print(f"    {model}: {pred_name}")
        else:
            print(f"âŒ äºˆæ¸¬å¤±æ•—")

    print(f"\nâœ… æ”¹è‰¯äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_enhanced_prediction_test())