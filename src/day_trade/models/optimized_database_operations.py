#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œæœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

Issue #165: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã®å‡¦ç†é€Ÿåº¦å‘ä¸Šã«å‘ã‘ãŸæœ€é©åŒ–
ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€SQLAlchemyã®ãƒãƒ«ã‚¯æ“ä½œã‚’æ´»ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import time
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Type

from sqlalchemy import create_engine, inspect, text
from sqlalchemy.exc import IntegrityError
from sqlalchemy.ext.declarative import DeclarativeMeta
from sqlalchemy.orm import Session, sessionmaker

from ..utils.logging_config import get_context_logger
from ..utils.performance_optimizer import PerformanceProfiler


@dataclass
class BulkOperationResult:
    """ãƒãƒ«ã‚¯æ“ä½œã®çµæœ"""

    success: bool
    processed_count: int
    execution_time: float
    throughput: float  # records per second
    error_message: Optional[str] = None
    failed_records: Optional[List[Dict]] = None


class OptimizedDatabaseOperations:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, session: Session):
        self.session = session
        self.logger = get_context_logger(__name__)
        self.profiler = PerformanceProfiler()

    @contextmanager
    def bulk_operation_context(self, operation_name: str):
        """ãƒãƒ«ã‚¯æ“ä½œç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        start_time = time.perf_counter()
        self.logger.info(f"ãƒãƒ«ã‚¯æ“ä½œé–‹å§‹: {operation_name}")

        try:
            yield
            self.session.commit()
            execution_time = time.perf_counter() - start_time
            self.logger.info(
                f"ãƒãƒ«ã‚¯æ“ä½œæˆåŠŸ: {operation_name}",
                extra={"execution_time": execution_time},
            )
        except Exception as e:
            self.session.rollback()
            execution_time = time.perf_counter() - start_time
            self.logger.error(
                f"ãƒãƒ«ã‚¯æ“ä½œå¤±æ•—: {operation_name}",
                extra={"error": str(e)},
                execution_time=execution_time,
            )
            raise

    def bulk_insert_optimized(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        chunk_size: int = 1000,
        return_defaults: bool = False,
    ) -> BulkOperationResult:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ«ã‚¯æŒ¿å…¥

        Args:
            model_class: SQLAlchemyãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
            data: æŒ¿å…¥ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            return_defaults: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™ã‹

        Returns:
            BulkOperationResult: æ“ä½œçµæœ
        """
        start_time = time.perf_counter()
        total_processed = 0
        failed_records = []

        try:
            with self.bulk_operation_context("bulk_insert_optimized"):
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†
                for i in range(0, len(data), chunk_size):
                    chunk = data[i : i + chunk_size]

                    try:
                        self.session.bulk_insert_mappings(
                            model_class, chunk, return_defaults=return_defaults
                        )
                        total_processed += len(chunk)

                        self.logger.debug(
                            "ãƒãƒ£ãƒ³ã‚¯æŒ¿å…¥å®Œäº†",
                            extra={"chunk_size": len(chunk)},
                            total_processed=total_processed,
                        )

                    except IntegrityError as e:
                        # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ãªã©ã®æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€å€‹åˆ¥å‡¦ç†
                        self.logger.warning(
                            "ãƒãƒ£ãƒ³ã‚¯æŒ¿å…¥ã§æ•´åˆæ€§ã‚¨ãƒ©ãƒ¼ã€å€‹åˆ¥å‡¦ç†ã«åˆ‡ã‚Šæ›¿ãˆ",
                            extra={"error": str(e)},
                        )

                        individual_result = self._handle_individual_inserts(
                            model_class, chunk
                        )
                        total_processed += individual_result["success_count"]
                        failed_records.extend(individual_result["failed_records"])

            execution_time = time.perf_counter() - start_time
            throughput = total_processed / execution_time if execution_time > 0 else 0

            return BulkOperationResult(
                success=True,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=throughput,
                failed_records=failed_records if failed_records else None,
            )

        except Exception as e:
            execution_time = time.perf_counter() - start_time
            return BulkOperationResult(
                success=False,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=0,
                error_message=str(e),
            )

    def bulk_update_optimized(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        chunk_size: int = 1000,
        update_changed_only: bool = True,
    ) -> BulkOperationResult:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ«ã‚¯æ›´æ–°

        Args:
            model_class: SQLAlchemyãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
            data: æ›´æ–°ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆä¸»ã‚­ãƒ¼ã‚’å«ã‚€å¿…è¦ãŒã‚ã‚‹ï¼‰
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            update_changed_only: å¤‰æ›´ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿æ›´æ–°ã™ã‚‹ã‹

        Returns:
            BulkOperationResult: æ“ä½œçµæœ
        """
        start_time = time.perf_counter()
        total_processed = 0
        failed_records = []

        try:
            with self.bulk_operation_context("bulk_update_optimized"):
                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†
                for i in range(0, len(data), chunk_size):
                    chunk = data[i : i + chunk_size]

                    try:
                        if update_changed_only:
                            # å¤‰æ›´ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’æ›´æ–°
                            filtered_chunk = self._filter_changed_records(
                                model_class, chunk
                            )
                            if filtered_chunk:
                                self.session.bulk_update_mappings(
                                    model_class, filtered_chunk
                                )
                                total_processed += len(filtered_chunk)
                        else:
                            # å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°
                            self.session.bulk_update_mappings(model_class, chunk)
                            total_processed += len(chunk)

                        self.logger.debug(
                            "ãƒãƒ£ãƒ³ã‚¯æ›´æ–°å®Œäº†",
                            extra={"chunk_size": len(chunk)},
                            total_processed=total_processed,
                        )

                    except Exception as e:
                        self.logger.warning(
                            "ãƒãƒ£ãƒ³ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼", extra={"error": str(e)}
                        )
                        failed_records.extend(chunk)

            execution_time = time.perf_counter() - start_time
            throughput = total_processed / execution_time if execution_time > 0 else 0

            return BulkOperationResult(
                success=True,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=throughput,
                failed_records=failed_records if failed_records else None,
            )

        except Exception as e:
            execution_time = time.perf_counter() - start_time
            return BulkOperationResult(
                success=False,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=0,
                error_message=str(e),
            )

    def bulk_upsert_optimized(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        conflict_columns: List[str],
        chunk_size: int = 1000,
    ) -> BulkOperationResult:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ«ã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆï¼ˆINSERT ... ON CONFLICTï¼‰

        Args:
            model_class: SQLAlchemyãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
            data: ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            conflict_columns: ç«¶åˆã‚’åˆ¤å®šã™ã‚‹ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆ
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º

        Returns:
            BulkOperationResult: æ“ä½œçµæœ
        """
        start_time = time.perf_counter()
        total_processed = 0

        try:
            with self.bulk_operation_context("bulk_upsert_optimized"):
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ç¨®é¡ã‚’å–å¾—
                dialect_name = self.session.bind.dialect.name

                # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†
                for i in range(0, len(data), chunk_size):
                    chunk = data[i : i + chunk_size]

                    if dialect_name == "postgresql":
                        # PostgreSQLã®ON CONFLICTæ§‹æ–‡ã‚’ä½¿ç”¨
                        result = self._upsert_postgresql(
                            model_class, chunk, conflict_columns
                        )
                    elif dialect_name == "sqlite":
                        # SQLiteã®INSERT OR REPLACEæ§‹æ–‡ã‚’ä½¿ç”¨
                        result = self._upsert_sqlite(
                            model_class, chunk, conflict_columns
                        )
                    elif dialect_name == "mysql":
                        # MySQLã®ON DUPLICATE KEY UPDATEæ§‹æ–‡ã‚’ä½¿ç”¨
                        result = self._upsert_mysql(
                            model_class, chunk, conflict_columns
                        )
                    else:
                        # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã¯å€‹åˆ¥å‡¦ç†
                        result = self._upsert_fallback(
                            model_class, chunk, conflict_columns
                        )

                    total_processed += result["processed_count"]

                    self.logger.debug(
                        "ãƒãƒ£ãƒ³ã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆå®Œäº†",
                        extra={"chunk_size": len(chunk)},
                        total_processed=total_processed,
                    )

            execution_time = time.perf_counter() - start_time
            throughput = total_processed / execution_time if execution_time > 0 else 0

            return BulkOperationResult(
                success=True,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=throughput,
            )

        except Exception as e:
            execution_time = time.perf_counter() - start_time
            return BulkOperationResult(
                success=False,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=0,
                error_message=str(e),
            )

    def bulk_delete_optimized(
        self,
        model_class: Type[DeclarativeMeta],
        filter_conditions: List[Dict[str, Any]],
        chunk_size: int = 1000,
    ) -> BulkOperationResult:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸãƒãƒ«ã‚¯å‰Šé™¤

        Args:
            model_class: SQLAlchemyãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹
            filter_conditions: å‰Šé™¤æ¡ä»¶ã®ãƒªã‚¹ãƒˆ
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º

        Returns:
            BulkOperationResult: æ“ä½œçµæœ
        """
        start_time = time.perf_counter()
        total_processed = 0

        try:
            with self.bulk_operation_context("bulk_delete_optimized"):
                # æ¡ä»¶ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦å‡¦ç†
                for i in range(0, len(filter_conditions), chunk_size):
                    chunk = filter_conditions[i : i + chunk_size]

                    # INå¥ã‚’ä½¿ç”¨ã—ãŸåŠ¹ç‡çš„ãªå‰Šé™¤
                    primary_keys = []
                    for condition in chunk:
                        # ä¸»ã‚­ãƒ¼ã‚’å–å¾—
                        pk_columns = inspect(model_class).primary_key
                        pk_values = tuple(condition.get(col.name) for col in pk_columns)
                        primary_keys.append(pk_values)

                    if primary_keys:
                        # ä¸»ã‚­ãƒ¼ã®INå¥ã§å‰Šé™¤
                        pk_column = pk_columns[0]  # å˜ä¸€ä¸»ã‚­ãƒ¼ã‚’æƒ³å®š
                        query = self.session.query(model_class).filter(
                            pk_column.in_([pk[0] for pk in primary_keys])
                        )
                        deleted_count = query.delete(synchronize_session=False)
                        total_processed += deleted_count

                        self.logger.debug(
                            "ãƒãƒ£ãƒ³ã‚¯å‰Šé™¤å®Œäº†",
                            extra={"chunk_size": len(chunk)},
                            deleted_count=deleted_count,
                            total_processed=total_processed,
                        )

            execution_time = time.perf_counter() - start_time
            throughput = total_processed / execution_time if execution_time > 0 else 0

            return BulkOperationResult(
                success=True,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=throughput,
            )

        except Exception as e:
            execution_time = time.perf_counter() - start_time
            return BulkOperationResult(
                success=False,
                processed_count=total_processed,
                execution_time=execution_time,
                throughput=0,
                error_message=str(e),
            )

    def _handle_individual_inserts(
        self, model_class: Type[DeclarativeMeta], data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """å€‹åˆ¥æŒ¿å…¥ã®å‡¦ç†ï¼ˆæ•´åˆæ€§ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰"""
        success_count = 0
        failed_records = []

        for record in data:
            try:
                obj = model_class(**record)
                self.session.add(obj)
                self.session.flush()  # ã‚¨ãƒ©ãƒ¼ã‚’æ—©æœŸæ¤œå‡º
                success_count += 1
            except Exception as e:
                self.session.rollback()
                failed_records.append({"record": record, "error": str(e)})

                # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                self.session.begin()

        return {"success_count": success_count, "failed_records": failed_records}

    def _filter_changed_records(
        self, model_class: Type[DeclarativeMeta], data: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """å¤‰æ›´ã•ã‚ŒãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        changed_records = []

        # ä¸»ã‚­ãƒ¼ã§ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã€å¤‰æ›´ç‚¹ã‚’ç¢ºèª
        for record in data:
            pk_columns = inspect(model_class).primary_key
            pk_filter = {col.name: record.get(col.name) for col in pk_columns}

            existing = self.session.query(model_class).filter_by(**pk_filter).first()

            if existing:
                # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨æ¯”è¼ƒ
                has_changes = False
                for key, value in record.items():
                    if hasattr(existing, key) and getattr(existing, key) != value:
                        has_changes = True
                        break

                if has_changes:
                    changed_records.append(record)
            else:
                # æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å ´åˆã‚‚å«ã‚ã‚‹
                changed_records.append(record)

        return changed_records

    def _upsert_postgresql(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        conflict_columns: List[str],
    ) -> Dict[str, int]:
        """PostgreSQLç”¨ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"""
        # PostgreSQLã®ON CONFLICTã‚’ä½¿ç”¨ã—ãŸå®Ÿè£…
        table = model_class.__table__
        ", ".join(conflict_columns)

        # æ›´æ–°ã™ã‚‹ã‚«ãƒ©ãƒ ã‚’å‹•çš„ã«ç”Ÿæˆ
        update_columns = [
            col.name for col in table.columns if col.name not in conflict_columns
        ]
        ", ".join([f"{col} = EXCLUDED.{col}" for col in update_columns])

        # ãƒãƒ«ã‚¯ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œ
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€SQLAlchemyã®bulk_insert_mappingsã¨
        # PostgreSQLå›ºæœ‰ã®æ§‹æ–‡ã‚’çµ„ã¿åˆã‚ã›ã‚‹

        return {"processed_count": len(data)}

    def _upsert_sqlite(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        conflict_columns: List[str],
    ) -> Dict[str, int]:
        """SQLiteç”¨ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"""
        # SQLiteã®INSERT OR REPLACEã‚’ä½¿ç”¨ã—ãŸå®Ÿè£…
        return {"processed_count": len(data)}

    def _upsert_mysql(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        conflict_columns: List[str],
    ) -> Dict[str, int]:
        """MySQLç”¨ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ"""
        # MySQLã®ON DUPLICATE KEY UPDATEã‚’ä½¿ç”¨ã—ãŸå®Ÿè£…
        return {"processed_count": len(data)}

    def _upsert_fallback(
        self,
        model_class: Type[DeclarativeMeta],
        data: List[Dict[str, Any]],
        conflict_columns: List[str],
    ) -> Dict[str, int]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰"""
        processed_count = 0

        for record in data:
            # å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            filter_conditions = {col: record.get(col) for col in conflict_columns}
            existing = (
                self.session.query(model_class).filter_by(**filter_conditions).first()
            )

            if existing:
                # æ›´æ–°
                for key, value in record.items():
                    if hasattr(existing, key):
                        setattr(existing, key, value)
            else:
                # æŒ¿å…¥
                obj = model_class(**record)
                self.session.add(obj)

            processed_count += 1

        return {"processed_count": processed_count}

    def optimize_query_performance(
        self, query_sql: str, explain_analyze: bool = False
    ) -> Dict[str, Any]:
        """
        ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åˆ†æã¨æœ€é©åŒ–ææ¡ˆ

        Args:
            query_sql: åˆ†æã™ã‚‹SQLã‚¯ã‚¨ãƒª
            explain_analyze: EXPLAIN ANALYZEã‚’å®Ÿè¡Œã™ã‚‹ã‹

        Returns:
            ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æçµæœ
        """
        try:
            start_time = time.perf_counter()

            # EXPLAINï¼ˆã¾ãŸã¯ EXPLAIN ANALYZEï¼‰ã‚’å®Ÿè¡Œ
            if explain_analyze:
                explain_query = f"EXPLAIN ANALYZE {query_sql}"
            else:
                explain_query = f"EXPLAIN {query_sql}"

            result = self.session.execute(text(explain_query))
            explain_output = result.fetchall()

            execution_time = time.perf_counter() - start_time

            # å®Ÿéš›ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œæ™‚é–“ã‚‚æ¸¬å®š
            query_start = time.perf_counter()
            query_result = self.session.execute(text(query_sql))
            query_result.fetchall()  # çµæœã‚’å…¨ã¦å–å¾—
            query_execution_time = time.perf_counter() - query_start

            return {
                "explain_output": [str(row) for row in explain_output],
                "explain_execution_time": execution_time,
                "query_execution_time": query_execution_time,
                "optimization_suggestions": self._generate_optimization_suggestions(
                    explain_output
                ),
            }

        except Exception as e:
            return {
                "error": str(e),
                "explain_output": [],
                "optimization_suggestions": [],
            }

    def _generate_optimization_suggestions(
        self, explain_output: List[Any]
    ) -> List[str]:
        """EXPLAINçµæœã‹ã‚‰æœ€é©åŒ–ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        explain_text = " ".join([str(row) for row in explain_output]).lower()

        # ä¸€èˆ¬çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        if "seq scan" in explain_text:
            suggestions.append(
                "ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if "nested loop" in explain_text and "rows=" in explain_text:
            suggestions.append(
                "ãƒã‚¹ãƒˆã—ãŸãƒ«ãƒ¼ãƒ—ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚JOINã®æœ€é©åŒ–ã‚„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if "sort" in explain_text and "disk" in explain_text:
            suggestions.append(
                "ãƒ‡ã‚£ã‚¹ã‚¯ã‚½ãƒ¼ãƒˆãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚work_memã®å¢—åŠ ã‚„ORDER BYã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        if "hash" in explain_text and "buckets" in explain_text:
            suggestions.append(
                "ãƒãƒƒã‚·ãƒ¥çµåˆãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚é©åˆ‡ãªã‚µã‚¤ã‚ºã®work_memã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
            )

        return suggestions

    def get_table_statistics(
        self, model_class: Type[DeclarativeMeta]
    ) -> Dict[str, Any]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        table_name = model_class.__tablename__

        try:
            # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
            count_query = self.session.query(model_class).count()

            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å›ºæœ‰ï¼‰
            dialect_name = self.session.bind.dialect.name

            if dialect_name == "postgresql":
                size_query = f"""
                SELECT pg_size_pretty(pg_total_relation_size('{table_name}')) as table_size,
                       pg_size_pretty(pg_relation_size('{table_name}')) as data_size,
                       pg_size_pretty(pg_total_relation_size('{table_name}') - pg_relation_size('{table_name}')) as index_size
                """
            elif dialect_name == "sqlite":
                size_query = (
                    f"SELECT COUNT(*) * 1024 as estimated_size FROM {table_name}"
                )
            else:
                size_query = None

            size_result = {}
            if size_query:
                size_data = self.session.execute(text(size_query)).fetchone()
                if size_data:
                    size_result = dict(size_data._mapping)

            return {
                "table_name": table_name,
                "record_count": count_query,
                "size_info": size_result,
                "timestamp": time.time(),
            }

        except Exception as e:
            return {"table_name": table_name, "error": str(e), "timestamp": time.time()}


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
if __name__ == "__main__":
    from datetime import datetime

    from sqlalchemy import Column, DateTime, Integer, String, create_engine
    from sqlalchemy.orm import sessionmaker

    # Issue #120: Baseã‚¯ãƒ©ã‚¹ã‚’base.pyã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    from .base import Base

    class TestStock(Base):
        __tablename__ = "test_stocks"

        id = Column(Integer, primary_key=True)
        symbol = Column(String(10), unique=True)
        name = Column(String(100))
        price = Column(Integer)  # ä¾¡æ ¼ï¼ˆæ•´æ•°ã§æ ¼ç´ï¼‰
        updated_at = Column(DateTime, default=datetime.now)

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–ãƒ„ãƒ¼ãƒ« - ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

    # ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªSQLiteã§ãƒ†ã‚¹ãƒˆ
    engine = create_engine("sqlite:///:memory:", echo=False)
    Base.metadata.create_all(engine)

    Session = sessionmaker(bind=engine)
    session = Session()

    # æœ€é©åŒ–ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–
    optimizer = OptimizedDatabaseOperations(session)

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    test_data = [
        {"symbol": f"TST{i:04d}", "name": f"Test Stock {i}", "price": 1000 + i}
        for i in range(1000)
    ]

    print(f"ğŸ“Š {len(test_data)}ä»¶ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ãƒãƒ«ã‚¯æŒ¿å…¥ãƒ†ã‚¹ãƒˆ")

    # ãƒãƒ«ã‚¯æŒ¿å…¥ãƒ†ã‚¹ãƒˆ
    result = optimizer.bulk_insert_optimized(TestStock, test_data, chunk_size=100)

    print("âœ… ãƒãƒ«ã‚¯æŒ¿å…¥çµæœ:")
    print(f"   æˆåŠŸ: {result.success}")
    print(f"   å‡¦ç†ä»¶æ•°: {result.processed_count}")
    print(f"   å®Ÿè¡Œæ™‚é–“: {result.execution_time:.3f}ç§’")
    print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result.throughput:.0f} records/sec")

    # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆå–å¾—
    stats = optimizer.get_table_statistics(TestStock)
    print("\nğŸ“ˆ ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ:")
    print(f"   ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['record_count']}")

    session.close()
