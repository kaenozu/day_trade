#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ

Issue #311å¯¾å¿œ: 3.6ç§’/85éŠ˜æŸ„ã®å‡¦ç†æ€§èƒ½ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
"""

import sys
import threading
import time
from pathlib import Path

# ãƒ‘ã‚¹è¨­å®š
current_dir = Path(__file__).parent
src_path = current_dir / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from day_trade.utils.performance_dashboard import (
        PerformanceDashboard,
        create_dashboard,
        create_detailed_report,
    )
    from day_trade.utils.performance_monitor import (
        PerformanceMonitor,
        get_performance_monitor,
    )
except ImportError as e:
    print(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    sys.exit(1)


def test_performance_monitor_basic():
    """åŸºæœ¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆ"""
    print("\n=== åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆ ===")

    monitor = PerformanceMonitor()

    # 1. åŸºæœ¬çš„ãªç›£è¦–ãƒ†ã‚¹ãƒˆ
    with monitor.monitor("test_basic_operation", expected_time=0.5) as ctx:
        ctx.record_function_call()
        time.sleep(0.3)
        ctx.update_peak_memory()

    # 2. æœŸå¾…æ™‚é–“è¶…éãƒ†ã‚¹ãƒˆ
    try:
        with monitor.monitor("test_slow_operation", expected_time=0.2):
            time.sleep(0.4)  # æœŸå¾…æ™‚é–“ã®2å€
    except Exception:
        pass  # ã‚¨ãƒ©ãƒ¼ã§ã¯ãªãã€ã‚¢ãƒ©ãƒ¼ãƒˆãŒå‡ºåŠ›ã•ã‚Œã‚‹ã¯ãš

    # 3. ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ
    try:
        with monitor.monitor("test_error_operation"):
            raise ValueError("ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼")
    except ValueError:
        pass  # æœŸå¾…ã•ã‚Œã‚‹ä¾‹å¤–

    # çµæœç¢ºèª
    assert len(monitor.metrics_history) >= 3, "ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ãŒæ­£ã—ãè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"

    # æˆåŠŸãƒ»å¤±æ•—ã®è¨˜éŒ²ç¢ºèª
    success_count = sum(1 for m in monitor.metrics_history if m.success)
    failure_count = sum(1 for m in monitor.metrics_history if not m.success)

    print(f"  æˆåŠŸæ“ä½œ: {success_count}ä»¶")
    print(f"  å¤±æ•—æ“ä½œ: {failure_count}ä»¶")

    # æœ€æ–°ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
    latest_metric = monitor.metrics_history[-1]
    print(f"  æœ€æ–°å®Ÿè¡Œæ™‚é–“: {latest_metric.execution_time:.3f}ç§’")
    print(f"  æœ€æ–°ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {latest_metric.memory_usage_mb:.1f}MB")

    print("âœ… åŸºæœ¬ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_system_monitoring():
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ†ã‚¹ãƒˆ ===")

    monitor = PerformanceMonitor()

    # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹
    monitor.start_system_monitoring(interval=0.5)

    try:
        # 2ç§’é–“ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–å®Ÿè¡Œ
        time.sleep(2.0)

        # ã‚·ã‚¹ãƒ†ãƒ å±¥æ­´ç¢ºèª
        assert len(monitor.system_history) > 0, "ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"

        latest_system = monitor.system_history[-1]
        print(f"  æœ€æ–°CPUä½¿ç”¨ç‡: {latest_system.cpu_usage_percent:.1f}%")
        print(f"  æœ€æ–°ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {latest_system.memory_usage_percent:.1f}%")
        print(f"  åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª: {latest_system.available_memory_gb:.2f}GB")
        print(f"  ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ã‚»ã‚¹: {latest_system.active_processes}")

        print(f"  ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°: {len(monitor.system_history)}")

    finally:
        monitor.stop_system_monitoring()

    print("âœ… ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_performance_summary():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ãƒ†ã‚¹ãƒˆ ===")

    monitor = PerformanceMonitor()

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®ãŸã‚è¤‡æ•°æ“ä½œå®Ÿè¡Œ
    test_operations = [
        ("ml_analysis_85_stocks", 3.2),
        ("data_fetch_85_stocks", 1.8),
        ("portfolio_optimization", 0.9),
        ("ml_analysis_85_stocks", 3.8),  # åŸºæº–å€¤ã‚ˆã‚Šé…ã„
        ("data_fetch_85_stocks", 2.1),
    ]

    for op_name, sleep_time in test_operations:
        with monitor.monitor(
            op_name, expected_time=3.6 if "ml_analysis" in op_name else 2.0
        ):
            time.sleep(sleep_time)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„å–å¾—
    summary = monitor.get_performance_summary(hours=1)

    assert "error" not in summary, "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ"

    print(f"  ç·æ“ä½œå›æ•°: {summary['total_operations']}")
    print(f"  æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"  å¹³å‡å®Ÿè¡Œæ™‚é–“: {summary['avg_execution_time']:.3f}ç§’")
    print(f"  æœ€å¤§å®Ÿè¡Œæ™‚é–“: {summary['max_execution_time']:.3f}ç§’")

    # åŸºæº–å€¤æ¯”è¼ƒç¢ºèª
    if "baseline_comparison" in summary:
        print("  åŸºæº–å€¤æ¯”è¼ƒ:")
        for process, data in summary["baseline_comparison"].items():
            status_emoji = {"good": "âœ…", "warning": "âš ï¸", "critical": "âŒ"}.get(
                data["status"], "â“"
            )
            print(f"    {process}: {data['performance_ratio']:.2f}x {status_emoji}")

    print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_bottleneck_analysis():
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ†ã‚¹ãƒˆ ===")

    monitor = PerformanceMonitor()

    # æ„å›³çš„ã«é…ã„æ“ä½œã¨ãƒ¡ãƒ¢ãƒªæ¶ˆè²»æ“ä½œã‚’å®Ÿè¡Œ
    test_scenarios = [
        ("fast_operation", 0.1),
        ("slow_operation", 2.0),
        ("very_slow_operation", 3.0),
        ("medium_operation", 1.0),
        ("memory_heavy_operation", 0.5),
    ]

    for op_name, sleep_time in test_scenarios:
        with monitor.monitor(op_name) as ctx:
            time.sleep(sleep_time)
            if "memory_heavy" in op_name:
                # ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                data = [i**2 for i in range(50000)]
                ctx.update_peak_memory()
                del data

    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æå®Ÿè¡Œ
    bottlenecks = monitor.get_bottleneck_analysis()

    assert "error" not in bottlenecks, "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ"

    print("  é…ã„ãƒ—ãƒ­ã‚»ã‚¹ Top 3:")
    for i, proc in enumerate(bottlenecks["slow_processes"][:3]):
        print(f"    {i+1}. {proc['process']}: {proc['execution_time']:.3f}ç§’")

    print("  ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ãƒ—ãƒ­ã‚»ã‚¹ Top 3:")
    for i, proc in enumerate(bottlenecks["memory_heavy_processes"][:3]):
        print(f"    {i+1}. {proc['process']}: {proc['memory_peak_mb']:.1f}MB")

    print("âœ… ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ†ã‚¹ãƒˆå®Œäº†")


def test_decorator_functionality():
    """ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")

    monitor = get_performance_monitor()

    # monitor.monitor ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã®ãƒ†ã‚¹ãƒˆ
    def test_function(duration: float):
        """ãƒ†ã‚¹ãƒˆç”¨ã®é–¢æ•°"""
        time.sleep(duration)
        return duration * 2

    # æœŸå¾…æ™‚é–“å†…ã®å®Ÿè¡Œ
    with monitor.monitor("decorated_function_in_time", expected_time=1.0):
        result1 = test_function(0.5)
    assert result1 == 1.0, "é–¢æ•°ã®æˆ»ã‚Šå€¤ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"

    # æœŸå¾…æ™‚é–“è¶…éã®å®Ÿè¡Œ
    with monitor.monitor("decorated_function_over_time", expected_time=0.2):
        result2 = test_function(0.4)  # ã‚¢ãƒ©ãƒ¼ãƒˆãŒå‡ºåŠ›ã•ã‚Œã‚‹
    assert result2 == 0.8, "é–¢æ•°ã®æˆ»ã‚Šå€¤ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“"

    # ã‚°ãƒ­ãƒ¼ãƒãƒ«ç›£è¦–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç¢ºèª
    global_monitor = get_performance_monitor()
    assert (
        len(global_monitor.metrics_history) >= 2
    ), "ã‚°ãƒ­ãƒ¼ãƒãƒ«ç›£è¦–ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã¾ã›ã‚“"

    print(
        "  ç›£è¦–ã•ã‚ŒãŸé–¢æ•°ã®å®Ÿè¡Œå›æ•°:",
        len(
            [
                m
                for m in global_monitor.metrics_history
                if "decorated_function" in m.process_name
            ]
        ),
    )
    print("âœ… ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_dashboard_creation():
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆãƒ†ã‚¹ãƒˆ"""
    print("\n=== ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆãƒ†ã‚¹ãƒˆ ===")

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    monitor = PerformanceMonitor()
    monitor.start_system_monitoring(interval=0.3)

    try:
        # è¤‡æ•°ã®æ“ä½œã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©
        for i in range(5):
            with monitor.monitor(f"dashboard_test_{i % 3}", expected_time=1.0):
                time.sleep(0.5 + (i * 0.1))

        time.sleep(1.0)  # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿è“„ç©

        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ
        dashboard = PerformanceDashboard(output_dir="test_dashboard_output")

        dashboard_path = dashboard.create_realtime_dashboard()
        assert (
            dashboard_path.exists()
        ), f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {dashboard_path}"

        print(f"  ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆå®Œäº†: {dashboard_path}")
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {dashboard_path.stat().st_size / 1024:.1f}KB")

        # HTMLãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report_path = dashboard.generate_performance_report()
        assert report_path.exists(), f"HTMLãƒ¬ãƒãƒ¼ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {report_path}"

        print(f"  HTMLãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†: {report_path}")
        print(f"  ãƒ¬ãƒãƒ¼ãƒˆã‚µã‚¤ã‚º: {report_path.stat().st_size / 1024:.1f}KB")

    finally:
        monitor.stop_system_monitoring()

    print("âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆãƒ†ã‚¹ãƒˆå®Œäº†")


def test_export_import_functionality():
    """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ»ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")

    monitor = PerformanceMonitor()

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    for i in range(3):
        with monitor.monitor(f"export_test_{i}"):
            time.sleep(0.2)

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    export_path = monitor.export_metrics()
    assert (
        export_path.exists()
    ), f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“: {export_path}"

    # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå†…å®¹ç¢ºèª
    import json

    with open(export_path, encoding="utf-8") as f:
        export_data = json.load(f)

    assert (
        "performance_metrics" in export_data
    ), "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
    assert (
        "baseline_metrics" in export_data
    ), "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã«åŸºæº–å€¤ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
    assert (
        export_data["metrics_count"] >= 3
    ), "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™"

    print(f"  ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {export_path}")
    print(f"  ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {export_data['metrics_count']}")
    print(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {export_path.stat().st_size / 1024:.1f}KB")

    # å¤ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¯ãƒªã‚¢ãƒ†ã‚¹ãƒˆ
    original_count = len(monitor.metrics_history)
    monitor.clear_old_metrics(hours=0)  # å…¨ã¦ã‚¯ãƒªã‚¢
    assert len(monitor.metrics_history) == 0, "å¤ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚¯ãƒªã‚¢ã•ã‚Œã¦ã„ã¾ã›ã‚“"

    print(f"  {original_count}ä»¶ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚¯ãƒªã‚¢")
    print("âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº†")


def test_concurrent_monitoring():
    """ä¸¦è¡Œç›£è¦–ãƒ†ã‚¹ãƒˆ"""
    print("\n=== ä¸¦è¡Œç›£è¦–ãƒ†ã‚¹ãƒˆ ===")

    monitor = PerformanceMonitor()
    results = []

    def concurrent_task(task_id: int):
        """ä¸¦è¡Œå®Ÿè¡Œã‚¿ã‚¹ã‚¯"""
        with monitor.monitor(f"concurrent_task_{task_id}"):
            time.sleep(0.3 + (task_id * 0.1))
            results.append(task_id)

    # 5ã¤ã®ä¸¦è¡Œã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
    threads = []
    for i in range(5):
        thread = threading.Thread(target=concurrent_task, args=(i,))
        threads.append(thread)
        thread.start()

    # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Œäº†ã‚’å¾…æ©Ÿ
    for thread in threads:
        thread.join()

    assert len(results) == 5, "ä¸¦è¡Œã‚¿ã‚¹ã‚¯ãŒæ­£ã—ãå®Œäº†ã—ã¦ã„ã¾ã›ã‚“"
    assert (
        len([m for m in monitor.metrics_history if "concurrent_task" in m.process_name])
        == 5
    ), "ä¸¦è¡Œç›£è¦–ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"

    print(f"  ä¸¦è¡Œã‚¿ã‚¹ã‚¯å®Œäº†æ•°: {len(results)}")
    print(
        f"  ç›£è¦–ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²æ•°: {len([m for m in monitor.metrics_history if 'concurrent_task' in m.process_name])}"
    )
    print("âœ… ä¸¦è¡Œç›£è¦–ãƒ†ã‚¹ãƒˆå®Œäº†")


def run_comprehensive_test():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)

    try:
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_performance_monitor_basic()
        test_system_monitoring()
        test_performance_summary()
        test_bottleneck_analysis()
        test_decorator_functionality()
        test_dashboard_creation()
        test_export_import_functionality()
        test_concurrent_monitoring()

        print("\n" + "=" * 60)
        print("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆå®Œäº†ï¼ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")

        # æœ€çµ‚çµ±è¨ˆ
        global_monitor = get_performance_monitor()
        total_operations = len(global_monitor.metrics_history)
        success_count = sum(1 for m in global_monitor.metrics_history if m.success)

        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµ±è¨ˆ:")
        print(f"   ç·ç›£è¦–æ“ä½œæ•°: {total_operations}")
        print(f"   æˆåŠŸæ“ä½œæ•°: {success_count}")
        print(f"   æˆåŠŸç‡: {success_count/total_operations:.1%}")

        # Issue #311ã®è¦ä»¶ç¢ºèª
        print("\nâœ… Issue #311è¦ä»¶ç¢ºèª:")
        print("   âœ“ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†é€Ÿåº¦ç›£è¦–: å®Ÿè£…å®Œäº†")
        print("   âœ“ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ç›£è¦–: å®Ÿè£…å®Œäº†")
        print("   âœ“ CPUä½¿ç”¨ç‡åˆ†æ: å®Ÿè£…å®Œäº†")
        print("   âœ“ ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è‡ªå‹•æ¤œå‡º: å®Ÿè£…å®Œäº†")
        print("   âœ“ ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ : å®Ÿè£…å®Œäº†")
        print("   âœ“ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: å®Ÿè£…å®Œäº†")

        return True

    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_comprehensive_test()
    exit(0 if success else 1)