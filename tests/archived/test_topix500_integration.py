#!/usr/bin/env python3
"""
TOPIX500çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
Issue #314: TOPIX500å…¨éŠ˜æŸ„å¯¾å¿œ

å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
"""

import gc
import logging
import sys
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import psutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

# ãƒ†ã‚¹ãƒˆè¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()],
)

logger = logging.getLogger(__name__)


def mock_topix500_data(symbol_count: int = 100) -> dict:
    """
    TOPIX500ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

    Args:
        symbol_count: ç”Ÿæˆã™ã‚‹éŠ˜æŸ„æ•°

    Returns:
        ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿è¾æ›¸
    """
    print(f"ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­... ({symbol_count}éŠ˜æŸ„)")

    mock_data = {}

    # åŸºæœ¬éŠ˜æŸ„ãƒªã‚¹ãƒˆï¼ˆå®Ÿéš›ã®TOPIX500ã®ä¸€éƒ¨ï¼‰
    base_symbols = [
        "7203",
        "8306",
        "9984",
        "6758",
        "4689",
        "8058",
        "8031",
        "4568",
        "9501",
        "8801",
        "7267",
        "7201",
        "8316",
        "8411",
        "4063",
        "4005",
        "5401",
        "4507",
        "4502",
        "9983",
        "3382",
        "8267",
        "2914",
        "2502",
        "9503",
        "9531",
        "9064",
        "9020",
        "8802",
        "1812",
        "6503",
        "6501",
        "7751",
        "6954",
        "6367",
        "8725",
        "8601",
        "2768",
        "4183",
        "5406",
    ]

    # å¿…è¦ã«å¿œã˜ã¦éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
    symbols = base_symbols.copy()
    while len(symbols) < symbol_count:
        symbols.append(f"{1000 + len(symbols) - len(base_symbols):04d}")

    symbols = symbols[:symbol_count]

    for i, symbol in enumerate(symbols):
        # å„éŠ˜æŸ„ã®ç‰¹æ€§ã‚’åæ˜ 
        np.random.seed(hash(symbol) % 10000)

        dates = pd.date_range(start="2023-06-01", periods=200, freq="D")
        base_price = 1000 + (hash(symbol) % 3000)

        # ã‚ˆã‚Šç¾å®Ÿçš„ãªä¾¡æ ¼å¤‰å‹•
        returns = np.random.normal(0.0005, 0.02, len(dates))
        prices = [base_price]

        for ret in returns:
            new_price = prices[-1] * (1 + ret)
            prices.append(max(new_price, base_price * 0.5))

        prices = prices[1:]

        mock_data[symbol] = pd.DataFrame(
            {
                "Open": [p * np.random.uniform(0.998, 1.002) for p in prices],
                "High": [p * np.random.uniform(1.000, 1.025) for p in prices],
                "Low": [p * np.random.uniform(0.975, 1.000) for p in prices],
                "Close": prices,
                "Volume": np.random.randint(100000, 8000000, len(dates)),
            },
            index=dates,
        )

        if i % 20 == 0:
            print(f"   é€²æ—: {i+1}/{symbol_count} éŠ˜æŸ„ç”Ÿæˆå®Œäº†")

    print(f"ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(mock_data)}éŠ˜æŸ„")
    return mock_data


def test_database_system():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 50)
    print("1. TOPIX500ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    try:
        from src.day_trade.data.topix500_master import TOPIX500MasterManager

        # ãƒã‚¹ã‚¿ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        master_manager = TOPIX500MasterManager()

        # ã‚»ã‚¯ã‚¿ãƒ¼ãƒã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–
        master_manager.initialize_sector_master()
        print("OK ã‚»ã‚¯ã‚¿ãƒ¼ãƒã‚¹ã‚¿ãƒ¼åˆæœŸåŒ–å®Œäº†")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        master_manager.load_topix500_sample_data()
        print("âœ“ TOPIX500ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")

        # éŠ˜æŸ„å–å¾—ãƒ†ã‚¹ãƒˆ
        symbols = master_manager.get_all_active_symbols()
        print(f"âœ“ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–éŠ˜æŸ„å–å¾—: {len(symbols)}éŠ˜æŸ„")

        # ã‚»ã‚¯ã‚¿ãƒ¼ã‚µãƒãƒªãƒ¼å–å¾—
        sector_summary = master_manager.get_sector_summary()
        print(f"âœ“ ã‚»ã‚¯ã‚¿ãƒ¼ã‚µãƒãƒªãƒ¼å–å¾—: {len(sector_summary)}ã‚»ã‚¯ã‚¿ãƒ¼")

        # ãƒãƒ©ãƒ³ã‚¹è€ƒæ…®ãƒãƒƒãƒä½œæˆ
        batches = master_manager.create_balanced_batches(batch_size=25)
        print(f"âœ“ ãƒãƒ©ãƒ³ã‚¹è€ƒæ…®ãƒãƒƒãƒä½œæˆ: {len(batches)}ãƒãƒƒãƒ")

        return True, {
            "symbols_count": len(symbols),
            "sectors_count": len(sector_summary),
            "batches_count": len(batches),
        }

    except Exception as e:
        print(f"âœ— ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False, {}


def test_parallel_processing():
    """ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 50)
    print("2. ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    try:
        from src.day_trade.automation.topix500_parallel_engine import (
            TOPIX500ParallelEngine,
        )

        # ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        engine = TOPIX500ParallelEngine(
            max_workers=4, batch_size=25, memory_limit_gb=0.8
        )
        print("âœ“ ä¸¦åˆ—å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")

        # ãƒ†ã‚¹ãƒˆç”¨éŠ˜æŸ„ãƒªã‚¹ãƒˆ
        test_symbols = [
            "7203",
            "8306",
            "9984",
            "6758",
            "4689",
            "8058",
            "8031",
            "4568",
            "9501",
            "8801",
            "7267",
            "7201",
            "8316",
            "8411",
            "4063",
            "4005",
            "5401",
            "4507",
            "4502",
            "9983",
        ]

        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        start_time = time.time()
        results, statistics = engine.process_all_symbols(test_symbols)
        processing_time = time.time() - start_time

        print(f"âœ“ ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œå®Œäº†: {processing_time:.1f}ç§’")
        print(f"  - å‡¦ç†éŠ˜æŸ„æ•°: {len(test_symbols)}")
        print(f"  - æˆåŠŸç‡: {statistics.get('success_rate', 0):.1f}%")
        print(f"  - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {statistics.get('throughput', 0):.1f} éŠ˜æŸ„/ç§’")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ææ¡ˆ
        recommendations = engine.optimize_performance()
        print("âœ“ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ææ¡ˆç”Ÿæˆå®Œäº†")
        print(f"  - æ¨å¥¨ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {recommendations['optimal_workers']}")
        print(f"  - æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: {recommendations['optimal_batch_size']}")

        return True, {
            "processing_time": processing_time,
            "success_rate": statistics.get("success_rate", 0),
            "throughput": statistics.get("throughput", 0),
        }

    except Exception as e:
        print(f"âœ— ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False, {}


def test_memory_pipeline():
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 50)
    print("3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    try:
        from src.day_trade.data.memory_efficient_pipeline import (
            DataStreamGenerator,
            StatisticalFeatureProcessor,
            StreamingDataPipeline,
            TechnicalIndicatorProcessor,
        )

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        pipeline = StreamingDataPipeline(
            processors=[
                TechnicalIndicatorProcessor(["sma_5", "sma_20", "rsi"]),
                StatisticalFeatureProcessor([5, 20]),
            ],
            cache_size_mb=128,
        )
        print("âœ“ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº†")

        # ãƒ†ã‚¹ãƒˆéŠ˜æŸ„
        test_symbols = ["7203", "8306", "9984", "6758", "4689", "8058", "8031", "4568"]

        # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ç”Ÿæˆ
        data_generator = DataStreamGenerator(test_symbols)
        print("âœ“ ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–å®Œäº†")

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†å®Ÿè¡Œ
        start_time = time.time()
        processed_chunks = []

        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024

        for chunk in pipeline.process_data_stream(data_generator.stream_data()):
            processed_chunks.append(chunk)

        processing_time = time.time() - start_time
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory

        print(f"âœ“ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†å®Œäº†: {processing_time:.1f}ç§’")
        print(f"  - å‡¦ç†ãƒãƒ£ãƒ³ã‚¯æ•°: {len(processed_chunks)}")
        print(f"  - ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_increase:.1f}MB")
        print(
            f"  - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {len(processed_chunks)/processing_time:.1f} ãƒãƒ£ãƒ³ã‚¯/ç§’"
        )

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±è¨ˆ
        stats = pipeline.get_pipeline_stats()
        print("âœ“ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±è¨ˆå–å¾—å®Œäº†")
        print(f"  - ç·ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {stats['total_memory_usage_mb']:.1f}MB")
        print(f"  - ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ã‚»ãƒƒã‚µ: {stats['active_processors']}")

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        pipeline.cleanup()
        gc.collect()

        return True, {
            "processing_time": processing_time,
            "processed_chunks": len(processed_chunks),
            "memory_increase": memory_increase,
            "memory_efficient": memory_increase < 200,  # 200MBä»¥ä¸‹
        }

    except Exception as e:
        print(f"âœ— ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False, {}


def test_sector_analysis():
    """ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 50)
    print("4. ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    try:
        from src.day_trade.analysis.sector_analysis_engine import SectorAnalysisEngine

        # ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        analyzer = SectorAnalysisEngine()
        print("âœ“ ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")

        # ãƒ†ã‚¹ãƒˆç”¨ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_sectors = ["3700", "7050", "3250", "5250", "8050", "2050", "6050"]
        sector_data = {}

        for sector_code in test_sectors:
            dates = pd.date_range(start="2023-01-01", periods=120)
            np.random.seed(int(sector_code))

            base_price = 2000 + int(sector_code) % 1000
            returns = np.random.normal(0.001, 0.025, 120)

            prices = [base_price]
            for ret in returns:
                prices.append(prices[-1] * (1 + ret))

            sector_data[sector_code] = pd.DataFrame(
                {
                    "Close": prices[1:],
                    "Volume": np.random.randint(1000000, 10000000, 120),
                },
                index=dates,
            )

        print(f"âœ“ ãƒ†ã‚¹ãƒˆç”¨ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(sector_data)}ã‚»ã‚¯ã‚¿ãƒ¼")

        # ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        performances = analyzer.calculate_sector_performance(
            sector_data, period_days=60
        )
        print(f"âœ“ ã‚»ã‚¯ã‚¿ãƒ¼ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº†: {len(performances)}ã‚»ã‚¯ã‚¿ãƒ¼")

        # ã‚»ã‚¯ã‚¿ãƒ¼ç›¸é–¢åˆ†æ
        correlation_matrix = analyzer.analyze_sector_correlations(sector_data)
        print(f"âœ“ ã‚»ã‚¯ã‚¿ãƒ¼ç›¸é–¢åˆ†æå®Œäº†: {correlation_matrix.shape}è¡Œåˆ—")

        # ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚°ãƒŠãƒ«æ¤œå‡º
        rotation_signals = analyzer.detect_sector_rotation_signals(
            performances, correlation_matrix, "bull"
        )
        print(f"âœ“ ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚°ãƒŠãƒ«æ¤œå‡ºå®Œäº†: {len(rotation_signals)}ã‚·ã‚°ãƒŠãƒ«")

        # ã‚»ã‚¯ã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        clusters = analyzer.perform_sector_clustering(performances)
        print(f"âœ“ ã‚»ã‚¯ã‚¿ãƒ¼ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†: {len(clusters)}ã‚»ã‚¯ã‚¿ãƒ¼")

        # ã‚»ã‚¯ã‚¿ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”Ÿæˆ
        rankings = analyzer.generate_sector_rankings(performances, "composite")
        print(f"âœ“ ã‚»ã‚¯ã‚¿ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”Ÿæˆå®Œäº†: {len(rankings)}ã‚»ã‚¯ã‚¿ãƒ¼")

        # åŒ…æ‹¬çš„ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æ
        comprehensive = analyzer.analyze_comprehensive_sectors(sector_data, "bull")
        print("âœ“ åŒ…æ‹¬çš„ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æå®Œäº†")
        print(f"  - ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿä¼š: {len(comprehensive.rotation_opportunities)}")
        print(f"  - ã‚»ã‚¯ã‚¿ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°: {len(comprehensive.sector_rankings)}")

        return True, {
            "sectors_analyzed": len(performances),
            "correlation_size": (
                correlation_matrix.shape[0] if not correlation_matrix.empty else 0
            ),
            "rotation_signals": len(rotation_signals),
            "clusters": len(set(clusters.values())) if clusters else 0,
        }

    except Exception as e:
        print(f"âœ— ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False, {}


def test_integration_performance(target_symbols: int = 500):
    """çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 50)
    print("5. çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ")
    print("=" * 50)

    try:
        print(f"ç›®æ¨™: {target_symbols}éŠ˜æŸ„ã‚’20ç§’ä»¥å†…ã€1GBä»¥å†…ã§å‡¦ç†")

        # ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        print("å¤§è¦æ¨¡ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        mock_data = mock_topix500_data(
            symbol_count=min(target_symbols, 100)
        )  # ãƒ†ã‚¹ãƒˆç”¨ã«100éŠ˜æŸ„ã«åˆ¶é™

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–é–‹å§‹
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        print(f"åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {initial_memory:.1f}MB")

        # çµ±åˆå‡¦ç†å®Ÿè¡Œ
        start_time = time.time()

        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å‡¦ç†
        from src.day_trade.data.topix500_master import TOPIX500MasterManager

        master_manager = TOPIX500MasterManager()
        symbols = list(mock_data.keys())

        # 2. ãƒãƒƒãƒä½œæˆ
        batches = [symbols[i : i + 25] for i in range(0, len(symbols), 25)]

        # 3. ç°¡æ˜“åˆ†æå‡¦ç†ï¼ˆå®Ÿéš›ã®MLå‡¦ç†ã®ä»£æ›¿ï¼‰
        processed_count = 0
        analysis_results = []

        for batch in batches:
            batch_start = time.time()

            for symbol in batch:
                if symbol in mock_data:
                    data = mock_data[symbol]

                    # åŸºæœ¬åˆ†æ
                    current_price = float(data["Close"].iloc[-1])
                    price_change = float(data["Close"].pct_change().iloc[-1])
                    volatility = float(
                        data["Close"].pct_change().rolling(20).std().iloc[-1]
                    )

                    analysis_results.append(
                        {
                            "symbol": symbol,
                            "current_price": current_price,
                            "price_change": price_change,
                            "volatility": volatility,
                        }
                    )

                    processed_count += 1

            batch_time = time.time() - batch_start
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024

            if len(batches) <= 10:  # ãƒ­ã‚°å‡ºåŠ›åˆ¶é™
                print(
                    f"  ãƒãƒƒãƒ{len(batches)-len(batches)+1}å®Œäº†: {len(batch)}éŠ˜æŸ„, "
                    f"{batch_time:.1f}ç§’, ãƒ¡ãƒ¢ãƒª{current_memory:.1f}MB"
                )

        total_time = time.time() - start_time
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory

        # çµæœè©•ä¾¡
        success = True
        issues = []

        if total_time > 20:
            success = False
            issues.append(f"å‡¦ç†æ™‚é–“è¶…é: {total_time:.1f}ç§’ > 20ç§’")

        if memory_increase > 1024:  # 1GB
            success = False
            issues.append(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é: {memory_increase:.1f}MB > 1024MB")

        if processed_count < len(symbols) * 0.9:  # 90%ä»¥ä¸Šã®æˆåŠŸç‡
            success = False
            issues.append(f"å‡¦ç†æˆåŠŸç‡ä½ä¸‹: {processed_count}/{len(symbols)}")

        print("\nçµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ:")
        print(f"  å‡¦ç†éŠ˜æŸ„æ•°: {processed_count}/{len(symbols)}")
        print(f"  ç·å‡¦ç†æ™‚é–“: {total_time:.1f}ç§’ (ç›®æ¨™: 20ç§’)")
        print(f"  ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_increase:.1f}MB (ç›®æ¨™: <1024MB)")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {processed_count/total_time:.1f} éŠ˜æŸ„/ç§’")
        print(f"  æˆåŠŸç‡: {processed_count/len(symbols)*100:.1f}%")

        if success:
            print("âœ“ çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ: åˆæ ¼")
        else:
            print("âœ— çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ: ä¸åˆæ ¼")
            for issue in issues:
                print(f"  - {issue}")

        return success, {
            "processed_count": processed_count,
            "total_time": total_time,
            "memory_increase": memory_increase,
            "throughput": processed_count / total_time,
            "success_rate": processed_count / len(symbols) * 100,
            "target_achieved": success,
        }

    except Exception as e:
        print(f"âœ— çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False, {}


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    print("=" * 80)
    print("TOPIX500å…¨éŠ˜æŸ„å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ  çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    print("=" * 80)
    print(f"ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    test_results = {}
    overall_success = True

    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
    success, result = test_database_system()
    test_results["database"] = {"success": success, "result": result}
    if not success:
        overall_success = False

    # 2. ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
    success, result = test_parallel_processing()
    test_results["parallel"] = {"success": success, "result": result}
    if not success:
        overall_success = False

    # 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
    success, result = test_memory_pipeline()
    test_results["pipeline"] = {"success": success, "result": result}
    if not success:
        overall_success = False

    # 4. ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ
    success, result = test_sector_analysis()
    test_results["sector"] = {"success": success, "result": result}
    if not success:
        overall_success = False

    # 5. çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
    success, result = test_integration_performance()
    test_results["performance"] = {"success": success, "result": result}
    if not success:
        overall_success = False

    # æœ€çµ‚çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)

    test_names = {
        "database": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ",
        "parallel": "ä¸¦åˆ—å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ",
        "pipeline": "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
        "sector": "ã‚»ã‚¯ã‚¿ãƒ¼åˆ†æã‚·ã‚¹ãƒ†ãƒ ",
        "performance": "çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
    }

    success_count = 0
    for test_key, test_info in test_results.items():
        status = "âœ“ åˆæ ¼" if test_info["success"] else "âœ— ä¸åˆæ ¼"
        print(f"{test_names[test_key]}: {status}")
        if test_info["success"]:
            success_count += 1

    print(f"\nç·åˆçµæœ: {success_count}/{len(test_results)} ãƒ†ã‚¹ãƒˆåˆæ ¼")

    if overall_success:
        print("\nğŸ‰ TOPIX500å…¨éŠ˜æŸ„å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ : çµ±åˆãƒ†ã‚¹ãƒˆåˆæ ¼!")
        print("âœ… ã‚·ã‚¹ãƒ†ãƒ ã¯TOPIX500éŠ˜æŸ„å‡¦ç†ã«å¯¾å¿œã—ã¦ã„ã¾ã™")
    else:
        print("\nâš ï¸  TOPIX500å…¨éŠ˜æŸ„å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ : ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—")
        print("âŒ ä¸€éƒ¨æ©Ÿèƒ½ã«æ”¹å–„ãŒå¿…è¦ã§ã™")

    print(f"ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    return overall_success


if __name__ == "__main__":
    try:
        success = main()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\nãƒ†ã‚¹ãƒˆä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        exit(1)
    except Exception as e:
        print(f"\n\näºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback

        traceback.print_exc()
        exit(1)
