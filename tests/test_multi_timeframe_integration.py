#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Multi-Timeframe Integration Test - ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ

Issue #882å¯¾å¿œï¼šãƒ‡ã‚¤ãƒˆãƒ¬ãƒ¼ãƒ‰ä»¥å¤–ã®å–å¼•æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆ
æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æºç¢ºèª
"""

import asyncio
import pytest
import pandas as pd
import numpy as np
import logging
import tempfile
import os
from pathlib import Path
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ 
try:
    from multi_timeframe_predictor import (
        MultiTimeframePredictor, TimeFrame, MultiTimeframePredictionTask,
        MultiTimeframePrediction, IntegratedPrediction
    )
    MULTI_TIMEFRAME_AVAILABLE = True
except ImportError as e:
    MULTI_TIMEFRAME_AVAILABLE = False
    print(f"Multi-timeframe predictor not available: {e}")

try:
    from ml_prediction_models import MLPredictionModels, ModelType, PredictionTask
    ML_MODELS_AVAILABLE = True
except ImportError:
    ML_MODELS_AVAILABLE = False

# ãƒ†ã‚¹ãƒˆè¨­å®š
TEST_SYMBOLS = ["7203", "8306", "9984"]
if MULTI_TIMEFRAME_AVAILABLE:
    TEST_TIMEFRAMES = [TimeFrame.DAILY, TimeFrame.WEEKLY]
else:
    TEST_TIMEFRAMES = [] # Fallback or empty list if not available

class TestMultiTimeframeIntegration:
    """ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def temp_config(self):
        """ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        config_content = """
# ãƒ†ã‚¹ãƒˆç”¨ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ è¨­å®š
timeframes:
  daily:
    name: "ãƒ‡ã‚¤ãƒˆãƒ¬ãƒ¼ãƒ‰"
    prediction_horizon_days: 1
    data_period: "6mo"
    min_training_samples: 50
    enabled: true
    description: "ãƒ†ã‚¹ãƒˆç”¨æ—¥æ¬¡äºˆæ¸¬"

  weekly:
    name: "é€±é–“äºˆæ¸¬"
    prediction_horizon_days: 7
    data_period: "1y"
    min_training_samples: 100
    enabled: true
    description: "ãƒ†ã‚¹ãƒˆç”¨é€±é–“äºˆæ¸¬"

prediction_tasks:
  daily:
    price_direction:
      enabled: true
      threshold_percent: 1.0
      classes: ["ä¸‹è½", "æ¨ªã°ã„", "ä¸Šæ˜‡"]
      weight: 1.0

  weekly:
    price_direction:
      enabled: true
      threshold_percent: 3.0
      classes: ["ä¸‹è½", "æ¨ªã°ã„", "ä¸Šæ˜‡"]
      weight: 1.0

models:
  daily:
    random_forest:
      n_estimators: 50
      max_depth: 10
      min_samples_split: 5

  weekly:
    random_forest:
      n_estimators: 50
      max_depth: 10
      min_samples_split: 5
"""

        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False, encoding='utf-8') as f:
            f.write(config_content)
            temp_config_path = f.name

        yield Path(temp_config_path)

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            os.unlink(temp_config_path)
        except:
            pass

    @pytest.fixture
    def sample_data(self):
        """ã‚µãƒ³ãƒ—ãƒ«æ ªä¾¡ãƒ‡ãƒ¼ã‚¿"""
        dates = pd.date_range(end=datetime.now(), periods=200, freq='D')
        np.random.seed(42)

        prices = [1000]
        for _ in range(len(dates)-1):
            change = np.random.normal(0, 0.02)
            prices.append(prices[-1] * (1 + change))

        return pd.DataFrame({
            'Open': [p * 0.99 for p in prices],
            'High': [p * 1.02 for p in prices],
            'Low': [p * 0.98 for p in prices],
            'Close': prices,
            'Volume': np.random.randint(10000, 100000, len(dates))
        }, index=dates)

    @pytest.mark.asyncio
    async def test_system_initialization(self, temp_config):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # è¨­å®šç¢ºèª
        assert predictor.config is not None
        assert len(predictor.timeframes) >= 2
        assert TimeFrame.DAILY in predictor.timeframes
        assert TimeFrame.WEEKLY in predictor.timeframes

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç¢ºèª
        assert predictor.db_path.exists()

        print("âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆå®Œäº†")

    @pytest.mark.asyncio
    async def test_data_preparation(self, temp_config, sample_data):
        """ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ãƒ¢ãƒƒã‚¯
        with patch('multi_timeframe_predictor.real_data_provider') as mock_provider:
            mock_provider.get_stock_data.return_value = sample_data

            # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿æº–å‚™
            features, targets = await predictor.prepare_timeframe_data("7203", TimeFrame.DAILY)

            assert not features.empty
            assert len(targets) > 0
            assert 'price_direction' in targets
            assert features.shape[0] > 0

            # é€±æ¬¡ãƒ‡ãƒ¼ã‚¿æº–å‚™
            features_weekly, targets_weekly = await predictor.prepare_timeframe_data("7203", TimeFrame.WEEKLY)

            assert not features_weekly.empty
            assert len(targets_weekly) > 0

            print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ†ã‚¹ãƒˆå®Œäº† - æ—¥æ¬¡: {features.shape}, é€±æ¬¡: {features_weekly.shape}")

    @pytest.mark.asyncio
    async def test_feature_engineering(self, temp_config, sample_data):
        """ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # æ—¥æ¬¡ç‰¹å¾´é‡
        features_daily = await predictor._extract_timeframe_features(sample_data, TimeFrame.DAILY)
        assert 'returns' in features_daily.columns
        assert 'rsi' in features_daily.columns
        assert 'macd' in features_daily.columns

        # é€±æ¬¡ç‰¹å¾´é‡
        features_weekly = await predictor._extract_timeframe_features(sample_data, TimeFrame.WEEKLY)
        assert 'weekly_trend' in features_weekly.columns
        assert 'weekly_volatility' in features_weekly.columns

        # ç‰¹å¾´é‡æ•°æ¯”è¼ƒï¼ˆé€±æ¬¡ã¯æ—¥æ¬¡ã‚ˆã‚Šå¤šã„ç‰¹å¾´é‡ï¼‰
        assert len(features_weekly.columns) >= len(features_daily.columns)

        print(f"âœ… ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº† - æ—¥æ¬¡: {len(features_daily.columns)}ç‰¹å¾´é‡, é€±æ¬¡: {len(features_weekly.columns)}ç‰¹å¾´é‡")

    @pytest.mark.asyncio
    async def test_target_creation(self, temp_config, sample_data):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # æ—¥æ¬¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ1æ—¥å¾Œï¼‰
        targets_daily = predictor._create_timeframe_targets(sample_data, TimeFrame.DAILY, 1)
        assert 'price_direction' in targets_daily

        direction_values = targets_daily['price_direction'].dropna().unique()
        assert 'ä¸Šæ˜‡' in direction_values or 'ä¸‹è½' in direction_values or 'æ¨ªã°ã„' in direction_values

        # é€±æ¬¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆ7æ—¥å¾Œï¼‰
        targets_weekly = predictor._create_timeframe_targets(sample_data, TimeFrame.WEEKLY, 7)
        assert 'price_direction' in targets_weekly

        print(f"âœ… ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆãƒ†ã‚¹ãƒˆå®Œäº† - æ—¥æ¬¡: {len(targets_daily)}ã‚¿ã‚¹ã‚¯, é€±æ¬¡: {len(targets_weekly)}ã‚¿ã‚¹ã‚¯")

    @pytest.mark.asyncio
    async def test_model_training(self, temp_config, sample_data):
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        with patch('multi_timeframe_predictor.real_data_provider') as mock_provider:
            mock_provider.get_stock_data.return_value = sample_data

            # æ—¥æ¬¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            try:
                performances_daily = await predictor.train_timeframe_models("7203", TimeFrame.DAILY)
                assert len(performances_daily) > 0
                assert 'price_direction' in performances_daily

                # ãƒ¢ãƒ‡ãƒ«ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
                assert TimeFrame.DAILY in predictor.trained_models

                print(f"âœ… æ—¥æ¬¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(performances_daily)}ã‚¿ã‚¹ã‚¯")

            except Exception as e:
                print(f"âš ï¸ æ—¥æ¬¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã§ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§ï¼‰: {e}")

            # é€±æ¬¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            try:
                performances_weekly = await predictor.train_timeframe_models("7203", TimeFrame.WEEKLY)
                assert len(performances_weekly) > 0

                print(f"âœ… é€±æ¬¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(performances_weekly)}ã‚¿ã‚¹ã‚¯")

            except Exception as e:
                print(f"âš ï¸ é€±æ¬¡ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã§ã‚¨ãƒ©ãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§ï¼‰: {e}")

    @pytest.mark.asyncio
    async def test_prediction_execution(self, temp_config, sample_data):
        """äºˆæ¸¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        with patch('multi_timeframe_predictor.real_data_provider') as mock_provider:
            mock_provider.get_stock_data.return_value = sample_data

            try:
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                await predictor.train_timeframe_models("7203", TimeFrame.DAILY)

                # äºˆæ¸¬å®Ÿè¡Œ
                predictions = await predictor._predict_timeframe("7203", TimeFrame.DAILY)

                if predictions:
                    assert len(predictions) > 0
                    assert all(isinstance(p, MultiTimeframePrediction) for p in predictions)
                    assert all(p.symbol == "7203" for p in predictions)
                    assert all(p.timeframe == TimeFrame.DAILY for p in predictions)

                    print(f"âœ… äºˆæ¸¬å®Ÿè¡Œãƒ†ã‚¹ãƒˆå®Œäº†: {len(predictions)}äºˆæ¸¬")
                else:
                    print("âš ï¸ äºˆæ¸¬çµæœãªã—ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å¯èƒ½æ€§ï¼‰")

            except Exception as e:
                print(f"âš ï¸ äºˆæ¸¬å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼: {e}")

    @pytest.mark.asyncio
    async def test_integrated_prediction(self, temp_config, sample_data):
        """çµ±åˆäºˆæ¸¬ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        with patch('multi_timeframe_predictor.real_data_provider') as mock_provider:
            mock_provider.get_stock_data.return_value = sample_data

            try:
                # è¤‡æ•°ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ã§è¨“ç·´
                await predictor.train_timeframe_models("7203", TimeFrame.DAILY)

                # çµ±åˆäºˆæ¸¬
                integrated = await predictor.predict_all_timeframes("7203")

                assert isinstance(integrated, IntegratedPrediction)
                assert integrated.symbol == "7203"
                assert integrated.integrated_direction in ["ä¸Šæ˜‡", "ä¸‹è½", "æ¨ªã°ã„", "ä¸æ˜"]
                assert 0 <= integrated.integrated_confidence <= 1
                assert 0 <= integrated.consistency_score <= 1

                print(f"âœ… çµ±åˆäºˆæ¸¬ãƒ†ã‚¹ãƒˆå®Œäº†")
                print(f"   æ–¹å‘: {integrated.integrated_direction}")
                print(f"   ä¿¡é ¼åº¦: {integrated.integrated_confidence:.3f}")
                print(f"   ä¸€è²«æ€§: {integrated.consistency_score:.3f}")

            except Exception as e:
                print(f"âš ï¸ çµ±åˆäºˆæ¸¬ã§ã‚¨ãƒ©ãƒ¼: {e}")

    @pytest.mark.asyncio
    async def test_existing_system_integration(self, temp_config, sample_data):
        """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE or not ML_MODELS_AVAILABLE:
            pytest.skip("Required systems not available")

        # ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
        multi_predictor = MultiTimeframePredictor(config_path=temp_config)

        # æ—¢å­˜MLäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
        ml_models = MLPredictionModels()

        with patch('multi_timeframe_predictor.real_data_provider') as mock_provider:
            with patch('ml_prediction_models.real_data_provider') as mock_ml_provider:
                mock_provider.get_stock_data.return_value = sample_data
                mock_ml_provider.get_stock_data.return_value = sample_data

                try:
                    # ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã§è¨“ç·´
                    multi_performances = await multi_predictor.train_timeframe_models("7203", TimeFrame.DAILY)
                    ml_performances = await ml_models.train_models("7203", "6mo")

                    # ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã§äºˆæ¸¬
                    multi_integrated = await multi_predictor.predict_all_timeframes("7203")

                    # ç‰¹å¾´é‡æº–å‚™ï¼ˆMLäºˆæ¸¬ç”¨ï¼‰
                    features, _ = await ml_models.prepare_training_data("7203", "1mo")
                    latest_features = features.tail(1)
                    ml_predictions = await ml_models.predict("7203", latest_features)

                    # çµæœæ¯”è¼ƒ
                    print(f"âœ… æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
                    print(f"   ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ : {multi_integrated.integrated_direction}")

                    if ml_predictions:
                        for ml_pred in ml_predictions:
                            print(f"   æ—¢å­˜MLäºˆæ¸¬: {ml_pred.final_prediction}")

                except Exception as e:
                    print(f"âš ï¸ æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã§ã‚¨ãƒ©ãƒ¼: {e}")

    @pytest.mark.asyncio
    async def test_performance_persistence(self, temp_config, sample_data):
        """æ€§èƒ½ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        with patch('multi_timeframe_predictor.real_data_provider') as mock_provider:
            mock_provider.get_stock_data.return_value = sample_data

            try:
                # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
                performances = await predictor.train_timeframe_models("7203", TimeFrame.DAILY)

                # ã‚·ã‚¹ãƒ†ãƒ ã‚µãƒãƒªãƒ¼å–å¾—
                summary = await predictor.get_system_summary()

                assert 'enabled_timeframes' in summary
                assert 'trained_models_count' in summary
                assert 'recent_performances' in summary
                assert 'recent_predictions' in summary

                print(f"âœ… æ€§èƒ½ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ†ã‚¹ãƒˆå®Œäº†")
                print(f"   æœ‰åŠ¹ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ : {summary['enabled_timeframes']}")
                print(f"   è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ•°: {summary['trained_models_count']}")

            except Exception as e:
                print(f"âš ï¸ æ€§èƒ½ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ã§ã‚¨ãƒ©ãƒ¼: {e}")

    @pytest.mark.asyncio
    async def test_confidence_calculation(self, temp_config):
        """ä¿¡é ¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # åˆ†é¡ã‚¿ã‚¹ã‚¯ã®ä¿¡é ¼åº¦è¨ˆç®—
        # å…¨ãƒ¢ãƒ‡ãƒ«ãŒä¸€è‡´
        unanimous_predictions = {"RF": "ä¸Šæ˜‡", "XGB": "ä¸Šæ˜‡", "LGBM": "ä¸Šæ˜‡"}
        confidence_unanimous = predictor._calculate_prediction_confidence(unanimous_predictions, "price_direction")
        assert confidence_unanimous >= 0.8  # é«˜ã„ä¿¡é ¼åº¦

        # ãƒ¢ãƒ‡ãƒ«é–“ã§æ„è¦‹ãŒåˆ†ã‹ã‚Œã‚‹
        split_predictions = {"RF": "ä¸Šæ˜‡", "XGB": "ä¸‹è½", "LGBM": "æ¨ªã°ã„"}
        confidence_split = predictor._calculate_prediction_confidence(split_predictions, "price_direction")
        assert confidence_split <= 0.7  # ä½ã„ä¿¡é ¼åº¦

        # å›å¸°ã‚¿ã‚¹ã‚¯ã®ä¿¡é ¼åº¦è¨ˆç®—
        consistent_regression = {"RF": 1000.5, "XGB": 1000.3, "LGBM": 1000.7}
        confidence_regression = predictor._calculate_prediction_confidence(consistent_regression, "volatility")
        assert confidence_regression >= 0.5

        print(f"âœ… ä¿¡é ¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆå®Œäº†")
        print(f"   ä¸€è‡´äºˆæ¸¬ä¿¡é ¼åº¦: {confidence_unanimous:.3f}")
        print(f"   åˆ†æ•£äºˆæ¸¬ä¿¡é ¼åº¦: {confidence_split:.3f}")
        print(f"   å›å¸°äºˆæ¸¬ä¿¡é ¼åº¦: {confidence_regression:.3f}")

    @pytest.mark.asyncio
    async def test_consistency_score(self, temp_config):
        """ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # å…¨ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ ä¸€è‡´
        consistent_predictions = {
            TimeFrame.DAILY: "ä¸Šæ˜‡",
            TimeFrame.WEEKLY: "ä¸Šæ˜‡"
        }
        consistency_high = predictor._calculate_consistency_score(consistent_predictions)
        assert consistency_high == 1.0

        # ã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã§ä¸ä¸€è‡´
        inconsistent_predictions = {
            TimeFrame.DAILY: "ä¸Šæ˜‡",
            TimeFrame.WEEKLY: "ä¸‹è½"
        }
        consistency_low = predictor._calculate_consistency_score(inconsistent_predictions)
        assert consistency_low < 1.0

        print(f"âœ… ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ†ã‚¹ãƒˆå®Œäº†")
        print(f"   ä¸€è‡´æ™‚ã‚¹ã‚³ã‚¢: {consistency_high}")
        print(f"   ä¸ä¸€è‡´æ™‚ã‚¹ã‚³ã‚¢: {consistency_low}")

    @pytest.mark.asyncio
    async def test_risk_assessment(self, temp_config):
        """ãƒªã‚¹ã‚¯è©•ä¾¡ãƒ†ã‚¹ãƒˆ"""
        if not MULTI_TIMEFRAME_AVAILABLE:
            pytest.skip("Multi-timeframe predictor not available")

        predictor = MultiTimeframePredictor(config_path=temp_config)

        # ä½ãƒªã‚¹ã‚¯ï¼ˆé«˜ä¸€è²«æ€§ã€ä½ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        low_risk_predictions = {
            TimeFrame.DAILY: [
                MultiTimeframePrediction(
                    symbol="7203", timestamp=datetime.now(), timeframe=TimeFrame.DAILY,
                    task=MultiTimeframePredictionTask.VOLATILITY, prediction=0.01,
                    confidence=0.8, model_predictions={}, feature_importance={}, explanation=""
                )
            ]
        }
        risk_low = predictor._assess_risk(low_risk_predictions, 0.9)
        assert "ä½" in risk_low

        # é«˜ãƒªã‚¹ã‚¯ï¼ˆä½ä¸€è²«æ€§ã€é«˜ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼‰
        high_risk_predictions = {
            TimeFrame.DAILY: [
                MultiTimeframePrediction(
                    symbol="7203", timestamp=datetime.now(), timeframe=TimeFrame.DAILY,
                    task=MultiTimeframePredictionTask.VOLATILITY, prediction=0.05,
                    confidence=0.6, model_predictions={}, feature_importance={}, explanation=""
                )
            ]
        }
        risk_high = predictor._assess_risk(high_risk_predictions, 0.3)
        assert "é«˜" in risk_high

        print(f"âœ… ãƒªã‚¹ã‚¯è©•ä¾¡ãƒ†ã‚¹ãƒˆå®Œäº†")
        print(f"   ä½ãƒªã‚¹ã‚¯è©•ä¾¡: {risk_low}")
        print(f"   é«˜ãƒªã‚¹ã‚¯è©•ä¾¡: {risk_high}")

# ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
def print_test_separator(test_name: str):
    """ãƒ†ã‚¹ãƒˆåŒºåˆ‡ã‚Šè¡¨ç¤º"""
    print(f"\n{'='*50}")
    print(f"  {test_name}")
    print(f"{'='*50}")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–¢æ•°
async def run_integration_tests():
    """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print_test_separator("ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

    if not MULTI_TIMEFRAME_AVAILABLE:
        print("âŒ ãƒãƒ«ãƒã‚¿ã‚¤ãƒ ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return

    test_instance = TestMultiTimeframeIntegration()

    # ãƒ†ã‚¹ãƒˆç”¨è¨­å®šã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
    import tempfile
    import yaml

    config_content = {
        'timeframes': {
            'daily': {
                'name': 'ãƒ‡ã‚¤ãƒˆãƒ¬ãƒ¼ãƒ‰',
                'prediction_horizon_days': 1,
                'data_period': '6mo',
                'min_training_samples': 50,
                'enabled': True,
                'description': 'ãƒ†ã‚¹ãƒˆç”¨æ—¥æ¬¡äºˆæ¸¬'
            }
        },
        'prediction_tasks': {
            'daily': {
                'price_direction': {
                    'enabled': True,
                    'threshold_percent': 1.0,
                    'classes': ['ä¸‹è½', 'æ¨ªã°ã„', 'ä¸Šæ˜‡'],
                    'weight': 1.0
                }
            }
        },
        'models': {
            'daily': {
                'random_forest': {
                    'n_estimators': 50,
                    'max_depth': 10,
                    'min_samples_split': 5
                }
            }
        }
    }

    with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False, encoding='utf-8') as f:
        yaml.dump(config_content, f, allow_unicode=True)
        temp_config_path = Path(f.name)

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    dates = pd.date_range(end=datetime.now(), periods=200, freq='D')
    np.random.seed(42)

    prices = [1000]
    for _ in range(len(dates)-1):
        change = np.random.normal(0, 0.02)
        prices.append(prices[-1] * (1 + change))

    sample_data = pd.DataFrame({
        'Open': [p * 0.99 for p in prices],
        'High': [p * 1.02 for p in prices],
        'Low': [p * 0.98 for p in prices],
        'Close': prices,
        'Volume': np.random.randint(10000, 100000, len(dates))
    }, index=dates)

    try:
        # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        tests = [
            ("ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–", test_instance.test_system_initialization(temp_config_path)),
            ("ãƒ‡ãƒ¼ã‚¿æº–å‚™", test_instance.test_data_preparation(temp_config_path, sample_data)),
            ("ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°", test_instance.test_feature_engineering(temp_config_path, sample_data)),
            ("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆ", test_instance.test_target_creation(temp_config_path, sample_data)),
            ("ãƒ¢ãƒ‡ãƒ«è¨“ç·´", test_instance.test_model_training(temp_config_path, sample_data)),
            ("äºˆæ¸¬å®Ÿè¡Œ", test_instance.test_prediction_execution(temp_config_path, sample_data)),
            ("çµ±åˆäºˆæ¸¬", test_instance.test_integrated_prediction(temp_config_path, sample_data)),
            ("æ€§èƒ½ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–", test_instance.test_performance_persistence(temp_config_path, sample_data)),
            ("ä¿¡é ¼åº¦è¨ˆç®—", test_instance.test_confidence_calculation(temp_config_path)),
            ("ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—", test_instance.test_consistency_score(temp_config_path)),
            ("ãƒªã‚¹ã‚¯è©•ä¾¡", test_instance.test_risk_assessment(temp_config_path))
        ]

        if ML_MODELS_AVAILABLE:
            tests.append(("æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ", test_instance.test_existing_system_integration(temp_config_path, sample_data)))

        success_count = 0
        total_count = len(tests)

        for test_name, test_coro in tests:
            print_test_separator(f"ãƒ†ã‚¹ãƒˆ: {test_name}")
            try:
                await test_coro
                success_count += 1
            except Exception as e:
                print(f"âŒ {test_name}ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()

        print_test_separator("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print(f"æˆåŠŸ: {success_count}/{total_count} ãƒ†ã‚¹ãƒˆ")
        print(f"æˆåŠŸç‡: {success_count/total_count*100:.1f}%")

        if success_count == total_count:
            print("ğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        else:
            print("âš ï¸ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")

    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        try:
            os.unlink(temp_config_path)
        except:
            pass

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_integration_tests())