#!/usr/bin/env python3
"""
Issue #760: åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã¨æ¤œè¨¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰

é«˜åº¦ãªãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ :
- è‡ªå‹•ãƒ†ã‚¹ãƒˆç™ºè¦‹ãƒ»å®Ÿè¡Œ
- ã‚«ãƒãƒ¬ãƒƒã‚¸ç›£è¦–ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµ±åˆ
- CI/CDçµ±åˆæ”¯æ´
"""

import asyncio
import unittest
import sys
import time
import json
import subprocess
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(project_root / 'test_automation.log')
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """ãƒ†ã‚¹ãƒˆçµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    status: str  # passed, failed, error, skipped
    duration: float
    coverage: Optional[float] = None
    error_message: Optional[str] = None
    output: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TestSuite:
    """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    test_files: List[Path]
    category: str  # unit, integration, performance, e2e
    priority: int = 1  # 1=é«˜, 2=ä¸­, 3=ä½
    timeout: int = 300  # ç§’
    dependencies: List[str] = field(default_factory=list)

class TestDiscovery:
    """ãƒ†ã‚¹ãƒˆè‡ªå‹•ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.test_patterns = [
            'test_*.py',
            '*_test.py'
        ]

    def discover_all_tests(self) -> Dict[str, TestSuite]:
        """å…¨ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•ç™ºè¦‹"""
        test_suites = {}

        # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
        unit_tests = self._discover_unit_tests()
        if unit_tests:
            test_suites['unit'] = TestSuite(
                name="Unit Tests",
                test_files=unit_tests,
                category="unit",
                priority=1,
                timeout=60
            )

        # çµ±åˆãƒ†ã‚¹ãƒˆ
        integration_tests = self._discover_integration_tests()
        if integration_tests:
            test_suites['integration'] = TestSuite(
                name="Integration Tests",
                test_files=integration_tests,
                category="integration",
                priority=2,
                timeout=180
            )

        # MLãƒ†ã‚¹ãƒˆ
        ml_tests = self._discover_ml_tests()
        if ml_tests:
            test_suites['ml'] = TestSuite(
                name="ML Tests",
                test_files=ml_tests,
                category="ml",
                priority=1,
                timeout=120
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        performance_tests = self._discover_performance_tests()
        if performance_tests:
            test_suites['performance'] = TestSuite(
                name="Performance Tests",
                test_files=performance_tests,
                category="performance",
                priority=3,
                timeout=600
            )

        logger.info(f"ç™ºè¦‹ã—ãŸãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ: {list(test_suites.keys())}")
        return test_suites

    def _discover_unit_tests(self) -> List[Path]:
        """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®ç™ºè¦‹"""
        unit_tests = []
        tests_dir = self.project_root / 'tests'

        # ç›´æ¥ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
        for pattern in self.test_patterns:
            unit_tests.extend(tests_dir.glob(pattern))

        # MLãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ†ã‚¹ãƒˆ
        ml_dir = tests_dir / 'ml'
        if ml_dir.exists():
            for pattern in self.test_patterns:
                unit_tests.extend(ml_dir.glob(pattern))

        return [t for t in unit_tests if self._is_unit_test(t)]

    def _discover_integration_tests(self) -> List[Path]:
        """çµ±åˆãƒ†ã‚¹ãƒˆã®ç™ºè¦‹"""
        integration_dir = self.project_root / 'tests' / 'integration'
        if not integration_dir.exists():
            return []

        integration_tests = []
        for pattern in self.test_patterns:
            integration_tests.extend(integration_dir.glob(pattern))

        return integration_tests

    def _discover_ml_tests(self) -> List[Path]:
        """MLãƒ†ã‚¹ãƒˆã®ç™ºè¦‹"""
        ml_tests = []
        ml_dirs = [
            self.project_root / 'tests' / 'ml',
            self.project_root / 'tests' / 'ml' / 'base_models',
            self.project_root / 'tests' / 'ml' / 'stacking'
        ]

        for ml_dir in ml_dirs:
            if ml_dir.exists():
                for pattern in self.test_patterns:
                    ml_tests.extend(ml_dir.glob(pattern))

        return ml_tests

    def _discover_performance_tests(self) -> List[Path]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ç™ºè¦‹"""
        performance_dir = self.project_root / 'tests' / 'performance'
        if not performance_dir.exists():
            return []

        performance_tests = []
        for pattern in self.test_patterns:
            performance_tests.extend(performance_dir.glob(pattern))

        return performance_tests

    def _is_unit_test(self, test_path: Path) -> bool:
        """ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‹ã©ã†ã‹ã®åˆ¤å®š"""
        # çµ±åˆãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä»¥å¤–ã®ãƒ†ã‚¹ãƒˆ
        return 'integration' not in str(test_path) and 'performance' not in str(test_path)

class TestExecutor:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, project_root: Path, max_workers: int = 4):
        self.project_root = project_root
        self.max_workers = max_workers

    async def execute_test_suite(self, test_suite: TestSuite) -> List[TestResult]:
        """ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ"""
        logger.info(f"ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹: {test_suite.name}")

        results = []

        # ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªãƒ†ã‚¹ãƒˆã®å ´åˆ
        if test_suite.category in ['unit', 'ml'] and len(test_suite.test_files) > 1:
            results = await self._execute_parallel(test_suite)
        else:
            results = await self._execute_sequential(test_suite)

        logger.info(f"ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†: {test_suite.name} - {len(results)} ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        return results

    async def _execute_parallel(self, test_suite: TestSuite) -> List[TestResult]:
        """ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ã‚¿ã‚¹ã‚¯ä½œæˆ
            future_to_test = {
                executor.submit(
                    self._run_single_test,
                    test_file,
                    test_suite.timeout
                ): test_file for test_file in test_suite.test_files
            }

            # çµæœåé›†
            for future in as_completed(future_to_test):
                test_file = future_to_test[future]
                try:
                    result = future.result(timeout=test_suite.timeout)
                    results.append(result)
                except Exception as e:
                    logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ {test_file}: {e}")
                    results.append(TestResult(
                        name=test_file.name,
                        status="error",
                        duration=0.0,
                        error_message=str(e)
                    ))

        return results

    async def _execute_sequential(self, test_suite: TestSuite) -> List[TestResult]:
        """é †æ¬¡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        results = []

        for test_file in test_suite.test_files:
            try:
                result = self._run_single_test(test_file, test_suite.timeout)
                results.append(result)
            except Exception as e:
                logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ {test_file}: {e}")
                results.append(TestResult(
                    name=test_file.name,
                    status="error",
                    duration=0.0,
                    error_message=str(e)
                ))

        return results

    def _run_single_test(self, test_file: Path, timeout: int) -> TestResult:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿè¡Œ"""
        start_time = time.time()

        try:
            # ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹ã®ç”Ÿæˆ
            relative_path = test_file.relative_to(self.project_root)
            module_path = str(relative_path.with_suffix('')).replace(os.sep, '.')

            # pytestå®Ÿè¡Œ
            cmd = [
                sys.executable, '-m', 'pytest',
                str(test_file),
                '-v',
                '--tb=short',
                '--disable-warnings'
            ]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=self.project_root
            )

            duration = time.time() - start_time

            if result.returncode == 0:
                status = "passed"
            else:
                status = "failed"

            return TestResult(
                name=test_file.name,
                status=status,
                duration=duration,
                output=result.stdout,
                error_message=result.stderr if result.stderr else None
            )

        except subprocess.TimeoutExpired:
            return TestResult(
                name=test_file.name,
                status="error",
                duration=time.time() - start_time,
                error_message=f"ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ ({timeout}ç§’)"
            )
        except Exception as e:
            return TestResult(
                name=test_file.name,
                status="error",
                duration=time.time() - start_time,
                error_message=str(e)
            )

class CoverageAnalyzer:
    """ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_root: Path):
        self.project_root = project_root

    def measure_coverage(self, test_files: List[Path]) -> Dict[str, Any]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š"""
        try:
            # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            coverage_config = self._create_coverage_config()

            # ã‚«ãƒãƒ¬ãƒƒã‚¸ä»˜ãã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            cmd = [
                sys.executable, '-m', 'pytest',
                '--cov=src',
                '--cov-report=json',
                '--cov-report=term-missing',
            ] + [str(f) for f in test_files]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=self.project_root
            )

            # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            coverage_file = self.project_root / 'coverage.json'
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                return self._parse_coverage_data(coverage_data)

            return {'total_coverage': 0.0, 'files': {}}

        except Exception as e:
            logger.error(f"ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {'total_coverage': 0.0, 'files': {}, 'error': str(e)}

    def _create_coverage_config(self) -> Path:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ"""
        config_content = """
[run]
source = src
omit =
    */tests/*
    */test_*
    */__pycache__/*
    */venv/*
    */env/*

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__.:
"""
        config_file = self.project_root / '.coveragerc'
        with open(config_file, 'w') as f:
            f.write(config_content.strip())

        return config_file

    def _parse_coverage_data(self, coverage_data: Dict) -> Dict[str, Any]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã®è§£æ"""
        total_coverage = coverage_data.get('totals', {}).get('percent_covered', 0.0)

        files_coverage = {}
        for filepath, file_data in coverage_data.get('files', {}).items():
            files_coverage[filepath] = {
                'coverage': file_data.get('summary', {}).get('percent_covered', 0.0),
                'lines_covered': file_data.get('summary', {}).get('covered_lines', 0),
                'lines_total': file_data.get('summary', {}).get('num_statements', 0)
            }

        return {
            'total_coverage': total_coverage,
            'files': files_coverage,
            'timestamp': time.time()
        }

class TestReporter:
    """ãƒ†ã‚¹ãƒˆçµæœãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_root: Path):
        self.project_root = project_root

    def generate_report(self,
                       test_results: Dict[str, List[TestResult]],
                       coverage_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""

        # çµ±è¨ˆè¨ˆç®—
        total_tests = sum(len(results) for results in test_results.values())
        passed_tests = sum(
            len([r for r in results if r.status == 'passed'])
            for results in test_results.values()
        )
        failed_tests = sum(
            len([r for r in results if r.status == 'failed'])
            for results in test_results.values()
        )
        error_tests = sum(
            len([r for r in results if r.status == 'error'])
            for results in test_results.values()
        )

        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        report = {
            'timestamp': time.time(),
            'summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'errors': error_tests,
                'success_rate': success_rate
            },
            'coverage': coverage_data,
            'test_suites': {},
            'recommendations': self._generate_recommendations(
                test_results, coverage_data, success_rate
            )
        }

        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆåˆ¥è©³ç´°
        for suite_name, results in test_results.items():
            suite_passed = len([r for r in results if r.status == 'passed'])
            suite_total = len(results)
            suite_success_rate = (suite_passed / suite_total * 100) if suite_total > 0 else 0

            report['test_suites'][suite_name] = {
                'total': suite_total,
                'passed': suite_passed,
                'failed': len([r for r in results if r.status == 'failed']),
                'errors': len([r for r in results if r.status == 'error']),
                'success_rate': suite_success_rate,
                'avg_duration': sum(r.duration for r in results) / len(results) if results else 0,
                'details': [
                    {
                        'name': r.name,
                        'status': r.status,
                        'duration': r.duration,
                        'error': r.error_message
                    } for r in results
                ]
            }

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        self._save_report(report)

        return report

    def _generate_recommendations(self,
                                test_results: Dict[str, List[TestResult]],
                                coverage_data: Dict[str, Any],
                                success_rate: float) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []

        # ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„
        total_coverage = coverage_data.get('total_coverage', 0.0)
        if total_coverage < 50:
            recommendations.append(
                f"ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ{total_coverage:.1f}%ã¨ä½ã„ã§ã™ã€‚ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®è¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # å¤±æ•—ãƒ†ã‚¹ãƒˆã®åˆ†æ
        failed_suites = []
        for suite_name, results in test_results.items():
            failed_count = len([r for r in results if r.status in ['failed', 'error']])
            if failed_count > 0:
                failed_suites.append(f"{suite_name} ({failed_count}ä»¶)")

        if failed_suites:
            recommendations.append(
                f"å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ: {', '.join(failed_suites)}ã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        # æˆåŠŸç‡æ”¹å–„
        if success_rate < 80:
            recommendations.append(
                f"æˆåŠŸç‡ãŒ{success_rate:.1f}%ã§ã™ã€‚ãƒ†ã‚¹ãƒˆã®å®‰å®šæ€§å‘ä¸Šã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„
        slow_tests = []
        for results in test_results.values():
            for result in results:
                if result.duration > 30:  # 30ç§’ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆ
                    slow_tests.append(f"{result.name} ({result.duration:.1f}s)")

        if slow_tests:
            recommendations.append(
                f"å®Ÿè¡Œæ™‚é–“ã®é•·ã„ãƒ†ã‚¹ãƒˆ: {', '.join(slow_tests[:3])}ãªã©ã€‚æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
            )

        return recommendations

    def _save_report(self, report: Dict[str, Any]) -> None:
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜"""
        # JSONå½¢å¼ã§ä¿å­˜
        report_file = self.project_root / 'test_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # äººé–“å‘ã‘ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
        self._save_text_report(report)

    def _save_text_report(self, report: Dict[str, Any]) -> None:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        timestamp_str = time.strftime('%Y-%m-%d %H:%M:%S')

        content = f"""
# Day Trade ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {timestamp_str}

## ğŸ“Š æ¦‚è¦
- ç·ãƒ†ã‚¹ãƒˆæ•°: {report['summary']['total_tests']}
- æˆåŠŸ: {report['summary']['passed']}
- å¤±æ•—: {report['summary']['failed']}
- ã‚¨ãƒ©ãƒ¼: {report['summary']['errors']}
- æˆåŠŸç‡: {report['summary']['success_rate']:.1f}%
- ã‚«ãƒãƒ¬ãƒƒã‚¸: {report['coverage'].get('total_coverage', 0):.1f}%

## ğŸ¯ ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆåˆ¥çµæœ
"""

        for suite_name, suite_data in report['test_suites'].items():
            content += f"""
### {suite_name}
- ãƒ†ã‚¹ãƒˆæ•°: {suite_data['total']}
- æˆåŠŸç‡: {suite_data['success_rate']:.1f}%
- å¹³å‡å®Ÿè¡Œæ™‚é–“: {suite_data['avg_duration']:.2f}ç§’
"""

        if report['recommendations']:
            content += "\n## ğŸ’¡ æ”¹å–„æ¨å¥¨äº‹é …\n"
            for i, rec in enumerate(report['recommendations'], 1):
                content += f"{i}. {rec}\n"

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_file = self.project_root / 'test_report.md'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(content)

class TestAutomationFramework:
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯"""

    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path(__file__).parent.parent
        self.discovery = TestDiscovery(self.project_root)
        self.executor = TestExecutor(self.project_root)
        self.coverage_analyzer = CoverageAnalyzer(self.project_root)
        self.reporter = TestReporter(self.project_root)

    async def run_comprehensive_tests(self,
                                    suite_filter: Optional[List[str]] = None,
                                    include_coverage: bool = True) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸš€ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–é–‹å§‹")
        start_time = time.time()

        # ãƒ†ã‚¹ãƒˆç™ºè¦‹
        test_suites = self.discovery.discover_all_tests()
        logger.info(f"ç™ºè¦‹ã—ãŸãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ: {list(test_suites.keys())}")

        # ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨
        if suite_filter:
            test_suites = {
                name: suite for name, suite in test_suites.items()
                if name in suite_filter
            }

        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_results = {}
        all_test_files = []

        # å„ªå…ˆåº¦é †ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        sorted_suites = sorted(
            test_suites.items(),
            key=lambda x: x[1].priority
        )

        for suite_name, test_suite in sorted_suites:
            logger.info(f"å®Ÿè¡Œä¸­: {suite_name}")
            results = await self.executor.execute_test_suite(test_suite)
            test_results[suite_name] = results
            all_test_files.extend(test_suite.test_files)

        # ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®š
        coverage_data = {}
        if include_coverage and all_test_files:
            logger.info("ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šä¸­...")
            coverage_data = self.coverage_analyzer.measure_coverage(all_test_files)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        logger.info("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        report = self.reporter.generate_report(test_results, coverage_data)

        # å®Ÿè¡Œæ™‚é–“
        total_time = time.time() - start_time
        report['execution_time'] = total_time

        logger.info(f"âœ… ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–å®Œäº† (å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’)")

        return report

    def print_summary(self, report: Dict[str, Any]) -> None:
        """çµæœã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ¯ Day Trade ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–çµæœ")
        print("="*80)

        summary = report['summary']
        print(f"ğŸ“Š ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['total_tests']}")
        print(f"âœ… æˆåŠŸ: {summary['passed']}")
        print(f"âŒ å¤±æ•—: {summary['failed']}")
        print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼: {summary['errors']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {summary['success_rate']:.1f}%")

        if 'total_coverage' in report['coverage']:
            print(f"ğŸ¯ ã‚«ãƒãƒ¬ãƒƒã‚¸: {report['coverage']['total_coverage']:.1f}%")

        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {report.get('execution_time', 0):.1f}ç§’")

        # æ¨å¥¨äº‹é …
        if report['recommendations']:
            print("\nğŸ’¡ æ”¹å–„æ¨å¥¨äº‹é …:")
            for i, rec in enumerate(report['recommendations'], 1):
                print(f"  {i}. {rec}")

        # è©•ä¾¡
        if summary['success_rate'] >= 90:
            print("\nğŸ‰ å„ªç§€ï¼ã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã¯éå¸¸ã«é«˜ã„ã§ã™ã€‚")
        elif summary['success_rate'] >= 70:
            print("\nğŸ‘ è‰¯å¥½ï¼ã„ãã¤ã‹ã®æ”¹å–„ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚")
        else:
            print("\nâš ï¸  æ³¨æ„ãŒå¿…è¦ï¼ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§å‘ä¸ŠãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚")

        print("="*80)

async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–
    framework = TestAutomationFramework()

    # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    report = await framework.run_comprehensive_tests()

    # çµæœè¡¨ç¤º
    framework.print_summary(report)

    # æˆåŠŸåˆ¤å®š
    success_rate = report['summary']['success_rate']
    return 0 if success_rate >= 50 else 1

if __name__ == "__main__":
    exit(asyncio.run(main()))