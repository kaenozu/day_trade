#!/usr/bin/env python3
"""
ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ„ãƒ¼ãƒ«

ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚’åˆ†æã—ã€ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã‚’ç‰¹å®š
"""

import ast
import re
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Set, Tuple

import json
from datetime import datetime


class RefactoringAnalyzer:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.python_files = list(base_dir.glob("**/*.py"))
        self.analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'total_files': len(self.python_files),
            'file_sizes': {},
            'duplicate_patterns': {},
            'dead_code_candidates': [],
            'complex_functions': [],
            'import_analysis': {},
            'architecture_issues': [],
            'performance_issues': [],
            'refactoring_priorities': []
        }

    def run_comprehensive_analysis(self) -> Dict:
        """åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ"""
        print("ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹åˆ†æé–‹å§‹...")

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æ
        self._analyze_file_sizes()

        # 2. é‡è¤‡ã‚³ãƒ¼ãƒ‰æ¤œå‡º
        self._detect_duplicate_code()

        # 3. ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰æ¤œå‡º
        self._detect_dead_code()

        # 4. è¤‡é›‘åº¦åˆ†æ
        self._analyze_complexity()

        # 5. ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ†æ
        self._analyze_imports()

        # 6. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å•é¡Œæ¤œå‡º
        self._detect_architecture_issues()

        # 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡º
        self._detect_performance_issues()

        # 8. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å„ªå…ˆåº¦è¨­å®š
        self._set_refactoring_priorities()

        return self.analysis_results

    def _analyze_file_sizes(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æ"""
        print("  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æä¸­...")

        large_files = []
        total_lines = 0

        for py_file in self.python_files:
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    line_count = len(lines)
                    total_lines += line_count

                    rel_path = py_file.relative_to(self.base_dir)
                    self.analysis_results['file_sizes'][str(rel_path)] = line_count

                    if line_count > 300:  # 300è¡Œä»¥ä¸Šã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«
                        large_files.append({
                            'file': str(rel_path),
                            'lines': line_count,
                            'priority': 'high' if line_count > 500 else 'medium'
                        })
            except Exception:
                continue

        self.analysis_results['large_files'] = large_files
        self.analysis_results['average_file_size'] = total_lines / len(self.python_files) if self.python_files else 0

    def _detect_duplicate_code(self):
        """é‡è¤‡ã‚³ãƒ¼ãƒ‰æ¤œå‡º"""
        print("  é‡è¤‡ã‚³ãƒ¼ãƒ‰æ¤œå‡ºä¸­...")

        function_signatures = defaultdict(list)
        class_names = defaultdict(list)
        import_patterns = defaultdict(list)

        for py_file in self.python_files:
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                rel_path = py_file.relative_to(self.base_dir)

                # é–¢æ•°å®šç¾©ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                func_matches = re.findall(r'def\s+(\w+)\s*\([^)]*\):', content)
                for func in func_matches:
                    function_signatures[func].append(str(rel_path))

                # ã‚¯ãƒ©ã‚¹åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                class_matches = re.findall(r'class\s+(\w+)(?:\([^)]*\))?:', content)
                for cls in class_matches:
                    class_names[cls].append(str(rel_path))

                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                import_matches = re.findall(r'from\s+[\w.]+\s+import\s+[\w, ]+|import\s+[\w.]+', content)
                for imp in import_matches:
                    import_patterns[imp].append(str(rel_path))

            except Exception:
                continue

        # é‡è¤‡æ¤œå‡º
        duplicate_functions = {k: v for k, v in function_signatures.items() if len(v) > 1}
        duplicate_classes = {k: v for k, v in class_names.items() if len(v) > 1}

        self.analysis_results['duplicate_patterns'] = {
            'functions': duplicate_functions,
            'classes': duplicate_classes,
            'common_imports': dict(list(import_patterns.items())[:10])  # ä¸Šä½10å€‹
        }

    def _detect_dead_code(self):
        """ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰æ¤œå‡ºå€™è£œ"""
        print("  ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰æ¤œå‡ºä¸­...")

        # ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡º
        dead_code_patterns = [
            r'# TODO:.*',
            r'# FIXME:.*',
            r'# DEPRECATED.*',
            r'if\s+False:',
            r'if\s+0:',
            r'def\s+\w+.*:\s*pass\s*$',
            r'class\s+\w+.*:\s*pass\s*$'
        ]

        candidates = []

        for py_file in self.python_files:
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                rel_path = py_file.relative_to(self.base_dir)
                issues = []

                for pattern in dead_code_patterns:
                    matches = re.findall(pattern, content, re.MULTILINE)
                    if matches:
                        issues.extend(matches)

                if issues:
                    candidates.append({
                        'file': str(rel_path),
                        'issues': issues[:5],  # æœ€åˆã®5å€‹ã®ã¿
                        'total_issues': len(issues)
                    })

            except Exception:
                continue

        self.analysis_results['dead_code_candidates'] = candidates

    def _analyze_complexity(self):
        """è¤‡é›‘åº¦åˆ†æ"""
        print("  è¤‡é›‘åº¦åˆ†æä¸­...")

        complex_functions = []

        for py_file in self.python_files:
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                try:
                    tree = ast.parse(content)
                    rel_path = py_file.relative_to(self.base_dir)

                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            # ç°¡æ˜“çš„ãªè¤‡é›‘åº¦è¨ˆç®—ï¼ˆãƒã‚¹ãƒˆãƒ¬ãƒ™ãƒ« + åˆ†å²æ•°ï¼‰
                            complexity = self._calculate_complexity(node)

                            if complexity > 10:  # è¤‡é›‘åº¦ãŒ10ã‚’è¶…ãˆã‚‹é–¢æ•°
                                complex_functions.append({
                                    'file': str(rel_path),
                                    'function': node.name,
                                    'complexity': complexity,
                                    'line': node.lineno
                                })
                except SyntaxError:
                    continue

            except Exception:
                continue

        # è¤‡é›‘åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        complex_functions.sort(key=lambda x: x['complexity'], reverse=True)
        self.analysis_results['complex_functions'] = complex_functions[:20]  # ä¸Šä½20å€‹

    def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """ç°¡æ˜“çš„ãªè¤‡é›‘åº¦è¨ˆç®—"""
        complexity = 1  # ãƒ™ãƒ¼ã‚¹è¤‡é›‘åº¦

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.Try, ast.With)):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

    def _analyze_imports(self):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ†æ"""
        print("  ã‚¤ãƒ³ãƒãƒ¼ãƒˆåˆ†æä¸­...")

        import_stats = defaultdict(int)
        circular_imports = []

        for py_file in self.python_files:
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®æŠ½å‡º
                imports = re.findall(r'(?:from\s+[\w.]+\s+)?import\s+[\w., ]+', content)

                for imp in imports:
                    import_stats[imp] += 1

            except Exception:
                continue

        # æœ€ã‚‚ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆä¸Šä½10å€‹ï¼‰
        top_imports = dict(sorted(import_stats.items(), key=lambda x: x[1], reverse=True)[:10])

        self.analysis_results['import_analysis'] = {
            'top_imports': top_imports,
            'total_unique_imports': len(import_stats),
            'potential_consolidation': len([k for k, v in import_stats.items() if v > 5])
        }

    def _detect_architecture_issues(self):
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å•é¡Œæ¤œå‡º"""
        print("  ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å•é¡Œæ¤œå‡ºä¸­...")

        issues = []

        # 1. å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å¯èƒ½æ€§
        src_files = [f for f in self.python_files if 'src' in str(f)]
        if len(src_files) != len(self.python_files):
            issues.append({
                'type': 'mixed_structure',
                'description': 'srcãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¤–ã«Pythonãƒ•ã‚¡ã‚¤ãƒ«ãŒæ•£åœ¨',
                'severity': 'medium'
            })

        # 2. å¤§ãã™ãã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        large_files = self.analysis_results.get('large_files', [])
        if len(large_files) > 5:
            issues.append({
                'type': 'large_modules',
                'description': f'{len(large_files)}å€‹ã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ300è¡Œä»¥ä¸Šï¼‰ãŒå­˜åœ¨',
                'severity': 'high'
            })

        # 3. é‡è¤‡ã‚¯ãƒ©ã‚¹/é–¢æ•°
        duplicates = self.analysis_results.get('duplicate_patterns', {})
        if duplicates.get('functions') or duplicates.get('classes'):
            issues.append({
                'type': 'code_duplication',
                'description': 'é‡è¤‡ã™ã‚‹ã‚¯ãƒ©ã‚¹ãƒ»é–¢æ•°ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ',
                'severity': 'medium'
            })

        self.analysis_results['architecture_issues'] = issues

    def _detect_performance_issues(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡º"""
        print("  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡ºä¸­...")

        issues = []

        performance_patterns = [
            (r'\.glob\(.*\*\*.*\)', 'recursive_glob', 'å†å¸°çš„globã®ä½¿ç”¨'),
            (r'for.*in.*\.keys\(\)', 'dict_keys_iteration', 'è¾æ›¸ã‚­ãƒ¼ã®éåŠ¹ç‡ãªåå¾©'),
            (r'\.append\(.*\).*in.*for', 'list_comprehension_candidate', 'ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã®å€™è£œ'),
            (r'time\.sleep\(', 'blocking_sleep', 'ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°sleep'),
            (r'open\(.*\)(?!\s*with)', 'file_handle_leak', 'ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ«ãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§')
        ]

        for py_file in self.python_files:
            try:
                with open(py_file, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()

                rel_path = py_file.relative_to(self.base_dir)
                file_issues = []

                for pattern, issue_type, description in performance_patterns:
                    matches = re.findall(pattern, content)
                    if matches:
                        file_issues.append({
                            'type': issue_type,
                            'description': description,
                            'count': len(matches)
                        })

                if file_issues:
                    issues.append({
                        'file': str(rel_path),
                        'issues': file_issues
                    })

            except Exception:
                continue

        self.analysis_results['performance_issues'] = issues

    def _set_refactoring_priorities(self):
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å„ªå…ˆåº¦è¨­å®š"""
        print("  ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å„ªå…ˆåº¦è¨­å®šä¸­...")

        priorities = []

        # 1. å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²
        large_files = self.analysis_results.get('large_files', [])
        for file_info in large_files:
            if file_info['priority'] == 'high':
                priorities.append({
                    'task': f"{file_info['file']} ã®åˆ†å‰²",
                    'priority': 'high',
                    'reason': f"{file_info['lines']}è¡Œã®å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«",
                    'estimated_effort': 'high'
                })

        # 2. è¤‡é›‘ãªé–¢æ•°ã®ç°¡ç´ åŒ–
        complex_functions = self.analysis_results.get('complex_functions', [])
        for func in complex_functions[:5]:  # ä¸Šä½5å€‹
            priorities.append({
                'task': f"{func['file']}:{func['function']} ã®ç°¡ç´ åŒ–",
                'priority': 'medium',
                'reason': f"è¤‡é›‘åº¦{func['complexity']}ã®é–¢æ•°",
                'estimated_effort': 'medium'
            })

        # 3. é‡è¤‡ã‚³ãƒ¼ãƒ‰ã®çµ±åˆ
        duplicates = self.analysis_results.get('duplicate_patterns', {})
        if duplicates.get('functions'):
            priorities.append({
                'task': 'é‡è¤‡é–¢æ•°ã®çµ±åˆ',
                'priority': 'medium',
                'reason': f"{len(duplicates['functions'])}å€‹ã®é‡è¤‡é–¢æ•°",
                'estimated_effort': 'medium'
            })

        # 4. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®è§£æ±º
        perf_issues = self.analysis_results.get('performance_issues', [])
        if len(perf_issues) > 5:
            priorities.append({
                'task': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®ä¿®æ­£',
                'priority': 'low',
                'reason': f"{len(perf_issues)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œ",
                'estimated_effort': 'medium'
            })

        # 5. ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰ã®å‰Šé™¤
        dead_code = self.analysis_results.get('dead_code_candidates', [])
        if len(dead_code) > 3:
            priorities.append({
                'task': 'ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰ã®å‰Šé™¤',
                'priority': 'low',
                'reason': f"{len(dead_code)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰å€™è£œ",
                'estimated_effort': 'low'
            })

        # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
        priority_order = {'high': 3, 'medium': 2, 'low': 1}
        priorities.sort(key=lambda x: priority_order.get(x['priority'], 0), reverse=True)

        self.analysis_results['refactoring_priorities'] = priorities

    def save_results(self, output_file: Path):
        """çµæœä¿å­˜"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)

        print(f"åˆ†æçµæœã‚’ä¿å­˜: {output_file}")

    def generate_report(self) -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""# ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {self.analysis_results['timestamp']}
å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.analysis_results['total_files']}å€‹

## ğŸ“Š æ¦‚è¦

### ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºçµ±è¨ˆ
- å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {self.analysis_results.get('average_file_size', 0):.1f}è¡Œ
- å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ300è¡Œä»¥ä¸Šï¼‰: {len(self.analysis_results.get('large_files', []))}å€‹

### é‡è¤‡ã‚³ãƒ¼ãƒ‰
- é‡è¤‡é–¢æ•°: {len(self.analysis_results.get('duplicate_patterns', {}).get('functions', {}))}å€‹
- é‡è¤‡ã‚¯ãƒ©ã‚¹: {len(self.analysis_results.get('duplicate_patterns', {}).get('classes', {}))}å€‹

### è¤‡é›‘åº¦
- é«˜è¤‡é›‘åº¦é–¢æ•°: {len(self.analysis_results.get('complex_functions', []))}å€‹

## ğŸ¯ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å„ªå…ˆé †ä½

"""

        for i, priority in enumerate(self.analysis_results.get('refactoring_priorities', []), 1):
            report += f"{i}. **{priority['task']}** ({priority['priority']})\n"
            report += f"   - ç†ç”±: {priority['reason']}\n"
            report += f"   - æ¨å®šå·¥æ•°: {priority['estimated_effort']}\n\n"

        report += f"""
## ğŸ“‹ è©³ç´°åˆ†æçµæœ

### å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«
"""

        for file_info in self.analysis_results.get('large_files', [])[:5]:
            report += f"- {file_info['file']}: {file_info['lines']}è¡Œ ({file_info['priority']})\n"

        report += f"""
### é«˜è¤‡é›‘åº¦é–¢æ•°
"""

        for func in self.analysis_results.get('complex_functions', [])[:5]:
            report += f"- {func['file']}:{func['function']} (è¤‡é›‘åº¦: {func['complexity']})\n"

        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æé–‹å§‹")
    print("=" * 40)

    base_dir = Path(__file__).parent
    analyzer = RefactoringAnalyzer(base_dir)

    # åˆ†æå®Ÿè¡Œ
    results = analyzer.run_comprehensive_analysis()

    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = base_dir / f"refactoring_analysis_{timestamp}.json"
    analyzer.save_results(output_file)

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = analyzer.generate_report()
    report_file = base_dir / f"refactoring_report_{timestamp}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")

    # è¦ç´„è¡¨ç¤º
    print("\n" + "=" * 40)
    print("ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æå®Œäº†")
    print("=" * 40)
    print(f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {results['total_files']}å€‹")
    print(f"å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«: {len(results.get('large_files', []))}å€‹")
    print(f"è¤‡é›‘ãªé–¢æ•°: {len(results.get('complex_functions', []))}å€‹")
    print(f"å„ªå…ˆã‚¿ã‚¹ã‚¯: {len(results.get('refactoring_priorities', []))}å€‹")
    print("=" * 40)


if __name__ == "__main__":
    main()