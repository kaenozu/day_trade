# Performance Optimization Guide
# Issue #870 æ‹¡å¼µäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Issue #870æ‹¡å¼µäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã®è©³ç´°ãªæœ€é©åŒ–æ‰‹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›ã€å‡¦ç†é€Ÿåº¦ã®å‘ä¸Šã€ã‚·ã‚¹ãƒ†ãƒ å®‰å®šæ€§ã®æ”¹å–„ã«ã¤ã„ã¦å®Ÿè·µçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æä¾›ã—ã¾ã™ã€‚

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–

### åŸºæœ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–

```python
from integrated_performance_optimizer import get_system_performance_report

def monitor_system_health():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã®ç›£è¦–"""
    report = get_system_performance_report()

    # é‡è¦æŒ‡æ¨™ã®æŠ½å‡º
    system_state = report['system_state']
    cache_perf = report['cache_performance']

    metrics = {
        'memory_usage_mb': system_state['memory_usage_mb'],
        'cpu_usage_percent': system_state['cpu_usage_percent'],
        'response_time_ms': system_state['response_time_ms'],
        'model_cache_hit_rate': cache_perf['model_cache']['hit_rate'],
        'feature_cache_size': cache_perf['feature_cache']['size']
    }

    # è­¦å‘Šãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
    warnings = []
    if metrics['memory_usage_mb'] > 1000:  # 1GBè¶…é
        warnings.append("é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡")
    if metrics['cpu_usage_percent'] > 80:
        warnings.append("é«˜CPUä½¿ç”¨ç‡")
    if metrics['response_time_ms'] > 1000:
        warnings.append("å¿œç­”æ™‚é–“é…å»¶")
    if metrics['model_cache_hit_rate'] < 0.7:
        warnings.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹")

    return metrics, warnings

# ä½¿ç”¨ä¾‹
metrics, warnings = monitor_system_health()
print("ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
for key, value in metrics.items():
    if 'rate' in key:
        print(f"  {key}: {value:.1%}")
    else:
        print(f"  {key}: {value:.1f}")

if warnings:
    print(f"âš ï¸  è­¦å‘Š: {', '.join(warnings)}")
else:
    print("âœ… ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸")
```

### ç¶™ç¶šçš„ç›£è¦–ã®è¨­å®š

```python
import time
import threading
from datetime import datetime

class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç¶™ç¶šç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self, check_interval=60, auto_optimize=True):
        self.check_interval = check_interval
        self.auto_optimize = auto_optimize
        self.running = False
        self.metrics_history = []

    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.running = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()
        print(f"ğŸ” ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹ (é–“éš”: {self.check_interval}ç§’)")

    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.running = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=5)
        print("â¹ï¸  ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–åœæ­¢")

    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.running:
            try:
                metrics, warnings = monitor_system_health()

                # å±¥æ­´ä¿å­˜
                metrics['timestamp'] = datetime.now()
                metrics['warnings'] = warnings
                self.metrics_history.append(metrics)

                # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-500:]

                # è‡ªå‹•æœ€é©åŒ–ã®åˆ¤å®š
                if self.auto_optimize and warnings:
                    print(f"[{datetime.now().strftime('%H:%M:%S')}] è‡ªå‹•æœ€é©åŒ–å®Ÿè¡Œ: {warnings}")
                    self._auto_optimize(warnings)

                time.sleep(self.check_interval)

            except Exception as e:
                print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(self.check_interval)

    def _auto_optimize(self, warnings):
        """è‡ªå‹•æœ€é©åŒ–"""
        from integrated_performance_optimizer import get_integrated_optimizer

        optimizer = get_integrated_optimizer()

        if "é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡" in warnings:
            optimizer.optimize_memory_usage()
        if "ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡ä½ä¸‹" in warnings:
            optimizer.optimize_model_cache()
        if "å¿œç­”æ™‚é–“é…å»¶" in warnings:
            optimizer.optimize_prediction_speed()

    def get_metrics_summary(self, last_n_minutes=60):
        """éå»Nåˆ†é–“ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹é›†è¨ˆ"""
        if not self.metrics_history:
            return {}

        cutoff_time = datetime.now() - timedelta(minutes=last_n_minutes)
        recent_metrics = [m for m in self.metrics_history if m['timestamp'] > cutoff_time]

        if not recent_metrics:
            return {}

        import numpy as np

        summary = {
            'avg_memory_mb': np.mean([m['memory_usage_mb'] for m in recent_metrics]),
            'max_memory_mb': np.max([m['memory_usage_mb'] for m in recent_metrics]),
            'avg_cpu_percent': np.mean([m['cpu_usage_percent'] for m in recent_metrics]),
            'avg_response_ms': np.mean([m['response_time_ms'] for m in recent_metrics]),
            'avg_cache_hit_rate': np.mean([m['model_cache_hit_rate'] for m in recent_metrics]),
            'warning_count': sum(len(m['warnings']) for m in recent_metrics),
            'total_samples': len(recent_metrics)
        }

        return summary

# ä½¿ç”¨ä¾‹
monitor = SystemMonitor(check_interval=30, auto_optimize=True)
# monitor.start_monitoring()  # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ç›£è¦–é–‹å§‹

# å¾Œã§çµ±è¨ˆç¢ºèª
# summary = monitor.get_metrics_summary(last_n_minutes=30)
# monitor.stop_monitoring()
```

## ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–

### é«˜åº¦ãªãƒ¡ãƒ¢ãƒªç®¡ç†

```python
import gc
import psutil
from datetime import timedelta

class AdvancedMemoryManager:
    """é«˜åº¦ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.memory_threshold_mb = 800  # 800MBé–¾å€¤
        self.cleanup_frequency = timedelta(minutes=15)
        self.last_cleanup = datetime.now()

    def analyze_memory_usage(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è©³ç´°åˆ†æ"""
        process = psutil.Process()
        memory_info = process.memory_info()

        analysis = {
            'rss_mb': memory_info.rss / 1024 / 1024,  # ç‰©ç†ãƒ¡ãƒ¢ãƒª
            'vms_mb': memory_info.vms / 1024 / 1024,  # ä»®æƒ³ãƒ¡ãƒ¢ãƒª
            'percent': process.memory_percent(),
            'available_mb': psutil.virtual_memory().available / 1024 / 1024,
            'gc_counts': gc.get_count(),
            'gc_threshold': gc.get_threshold()
        }

        return analysis

    def aggressive_cleanup(self):
        """ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("ğŸ§¹ ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ")

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¼·åˆ¶å®Ÿè¡Œ
        collected = {}
        for generation in range(3):
            collected[f'gen_{generation}'] = gc.collect(generation)

        # æœªå‚ç…§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å‰Šé™¤
        collected['unreachable'] = gc.collect()

        # çµ±åˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        from integrated_performance_optimizer import get_integrated_optimizer
        optimizer = get_integrated_optimizer()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç©æ¥µçš„ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        model_cache_before = len(optimizer.model_cache.cache)
        feature_cache_before = len(optimizer.feature_cache.cache)

        # ä½¿ç”¨é »åº¦ã®ä½ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        current_time = time.time()
        expired_models = []
        for key, access_time in optimizer.model_cache.access_times.items():
            if current_time - access_time > 3600:  # 1æ™‚é–“æœªä½¿ç”¨
                expired_models.append(key)

        for key in expired_models:
            if key in optimizer.model_cache.cache:
                del optimizer.model_cache.cache[key]
                del optimizer.model_cache.access_times[key]

        # ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‰Šæ¸›
        if len(optimizer.feature_cache.cache) > 100:
            for _ in range(50):  # 50ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
                optimizer.feature_cache._evict_least_used()

        model_cache_after = len(optimizer.model_cache.cache)
        feature_cache_after = len(optimizer.feature_cache.cache)

        self.last_cleanup = datetime.now()

        return {
            'gc_collected': collected,
            'model_cache_reduced': model_cache_before - model_cache_after,
            'feature_cache_reduced': feature_cache_before - feature_cache_after,
            'cleanup_time': datetime.now()
        }

    def should_cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå¿…è¦ã‹åˆ¤å®š"""
        analysis = self.analyze_memory_usage()
        time_elapsed = datetime.now() - self.last_cleanup

        return (analysis['rss_mb'] > self.memory_threshold_mb or
                time_elapsed > self.cleanup_frequency)

    def optimize_gc_settings(self):
        """ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šæœ€é©åŒ–"""
        # ã‚ˆã‚Šç©æ¥µçš„ãªGCè¨­å®š
        gc.set_threshold(700, 10, 10)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: (700, 10, 10)

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ç„¡åŠ¹åŒ–ï¼ˆæœ¬ç•ªç’°å¢ƒï¼‰
        gc.set_debug(0)

        print("âœ… ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³è¨­å®šæœ€é©åŒ–å®Œäº†")

# ä½¿ç”¨ä¾‹
memory_manager = AdvancedMemoryManager()

# ãƒ¡ãƒ¢ãƒªåˆ†æ
analysis = memory_manager.analyze_memory_usage()
print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {analysis['rss_mb']:.1f}MB ({analysis['percent']:.1f}%)")

# å¿…è¦ã«å¿œã˜ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
if memory_manager.should_cleanup():
    cleanup_result = memory_manager.aggressive_cleanup()
    print(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœ:")
    print(f"  GCå›å: {cleanup_result['gc_collected']}")
    print(f"  ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›: {cleanup_result['model_cache_reduced']}")
    print(f"  ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šæ¸›: {cleanup_result['feature_cache_reduced']}")

# GCè¨­å®šæœ€é©åŒ–
memory_manager.optimize_gc_settings()
```

### ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

```python
import tracemalloc
import linecache

class MemoryProfiler:
    """ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼"""

    def __init__(self):
        self.snapshots = []

    def start_profiling(self):
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹"""
        tracemalloc.start()
        print("ğŸ“Š ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹")

    def take_snapshot(self, label=""):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—"""
        if not tracemalloc.is_tracing():
            print("âš ï¸  ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãŒé–‹å§‹ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return

        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append({
            'snapshot': snapshot,
            'label': label,
            'timestamp': datetime.now()
        })
        print(f"ğŸ“¸ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—: {label}")

    def analyze_top_memory_usage(self, snapshot_index=-1, top_n=10):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ä¸Šä½ã®åˆ†æ"""
        if not self.snapshots:
            print("ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return

        snapshot = self.snapshots[snapshot_index]['snapshot']
        top_stats = snapshot.statistics('lineno')

        print(f"\nğŸ” ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ä¸Šä½ {top_n} ä»¶:")
        for index, stat in enumerate(top_stats[:top_n], 1):
            frame = stat.traceback.format()[-1]
            print(f"{index:2d}. {stat.size / 1024 / 1024:.1f}MB - {frame}")

    def compare_snapshots(self, before_index=0, after_index=-1):
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ¯”è¼ƒ"""
        if len(self.snapshots) < 2:
            print("æ¯”è¼ƒã™ã‚‹ã«ã¯æœ€ä½2ã¤ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãŒå¿…è¦ã§ã™")
            return

        before = self.snapshots[before_index]['snapshot']
        after = self.snapshots[after_index]['snapshot']

        top_stats = after.compare_to(before, 'lineno')

        print(f"\nğŸ“ˆ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤‰åŒ– (ä¸Šä½10ä»¶):")
        for stat in top_stats[:10]:
            size_diff = stat.size_diff / 1024 / 1024
            count_diff = stat.count_diff
            if size_diff > 0:
                print(f"  +{size_diff:.1f}MB (+{count_diff}) - {stat.traceback.format()[-1]}")

    def stop_profiling(self):
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åœæ­¢"""
        tracemalloc.stop()
        print("â¹ï¸  ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åœæ­¢")

# ä½¿ç”¨ä¾‹
profiler = MemoryProfiler()

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹
profiler.start_profiling()

# åŸºæº–ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
profiler.take_snapshot("baseline")

# ä½•ã‚‰ã‹ã®å‡¦ç†å®Ÿè¡Œ
from integrated_performance_optimizer import get_integrated_optimizer
optimizer = get_integrated_optimizer()

# å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
import numpy as np
for i in range(10):
    data = np.random.randn(1000, 100)
    optimizer.feature_cache.put_features(f"profile_test_{i}", data)

# å‡¦ç†å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
profiler.take_snapshot("after_processing")

# æœ€é©åŒ–å®Ÿè¡Œ
optimizer.run_comprehensive_optimization()

# æœ€é©åŒ–å¾Œã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
profiler.take_snapshot("after_optimization")

# åˆ†æçµæœè¡¨ç¤º
profiler.analyze_top_memory_usage(-1, 5)
profiler.compare_snapshots(0, -1)

# ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åœæ­¢
profiler.stop_profiling()
```

## CPUæœ€é©åŒ–

### ä¸¦åˆ—å‡¦ç†ã®æœ€é©åŒ–

```python
import os
import multiprocessing
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import numpy as np

class CPUOptimizer:
    """CPUæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.cpu_count = multiprocessing.cpu_count()
        self.optimal_workers = max(2, self.cpu_count // 2)

    def optimize_numpy_threads(self):
        """NumPy/SciPyã‚¹ãƒ¬ãƒƒãƒ‰æ•°æœ€é©åŒ–"""
        optimal_threads = max(1, self.cpu_count // 2)

        # OpenMP ã‚¹ãƒ¬ãƒƒãƒ‰æ•°è¨­å®š
        os.environ['OMP_NUM_THREADS'] = str(optimal_threads)
        os.environ['MKL_NUM_THREADS'] = str(optimal_threads)
        os.environ['NUMEXPR_NUM_THREADS'] = str(optimal_threads)
        os.environ['OPENBLAS_NUM_THREADS'] = str(optimal_threads)

        print(f"ğŸ”§ NumPyã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ {optimal_threads} ã«è¨­å®š")

        return optimal_threads

    def benchmark_threading_vs_multiprocessing(self, data_size=10000):
        """ã‚¹ãƒ¬ãƒƒãƒ‰ vs ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        def cpu_intensive_task(data):
            """CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯"""
            return np.sum(np.sin(data) ** 2 + np.cos(data) ** 2)

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_data = [np.random.randn(data_size) for _ in range(self.cpu_count)]

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†
        start_time = time.time()
        sequential_results = [cpu_intensive_task(data) for data in test_data]
        sequential_time = time.time() - start_time

        # ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†
        start_time = time.time()
        with ThreadPoolExecutor(max_workers=self.optimal_workers) as executor:
            thread_results = list(executor.map(cpu_intensive_task, test_data))
        thread_time = time.time() - start_time

        # ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å‡¦ç†
        start_time = time.time()
        with ProcessPoolExecutor(max_workers=self.optimal_workers) as executor:
            process_results = list(executor.map(cpu_intensive_task, test_data))
        process_time = time.time() - start_time

        # çµæœæ¯”è¼ƒ
        benchmark_results = {
            'sequential_time': sequential_time,
            'thread_time': thread_time,
            'process_time': process_time,
            'thread_speedup': sequential_time / thread_time,
            'process_speedup': sequential_time / process_time,
            'optimal_method': 'thread' if thread_time < process_time else 'process',
            'data_size': data_size,
            'workers': self.optimal_workers
        }

        print(f"âš¡ CPUæœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print(f"  Sequential: {sequential_time:.3f}s")
        print(f"  Threading: {thread_time:.3f}s (x{benchmark_results['thread_speedup']:.1f})")
        print(f"  Multiprocessing: {process_time:.3f}s (x{benchmark_results['process_speedup']:.1f})")
        print(f"  æ¨å¥¨æ–¹æ³•: {benchmark_results['optimal_method']}")

        return benchmark_results

    def optimize_batch_processing(self, batch_sizes=[10, 50, 100, 200]):
        """ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–"""
        from integrated_performance_optimizer import get_integrated_optimizer

        optimizer = get_integrated_optimizer()
        results = {}

        print("ğŸ”§ ãƒãƒƒãƒã‚µã‚¤ã‚ºæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ:")

        for batch_size in batch_sizes:
            # ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
            optimizer.batch_processor.batch_size = batch_size

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            test_requests = []
            for i in range(batch_size * 3):  # 3ãƒãƒƒãƒåˆ†
                test_requests.append({
                    'id': f'test_{i}',
                    'data': {'features': np.random.randn(10)},
                    'callback': lambda x: None
                })

            # å‡¦ç†æ™‚é–“æ¸¬å®š
            start_time = time.time()
            for req in test_requests:
                optimizer.batch_processor.add_prediction_request(
                    req['id'], req['data'], req['callback']
                )

            # ãƒ¢ãƒƒã‚¯ãƒ—ãƒ¬ãƒ‡ã‚£ã‚¯ã‚¿ãƒ¼
            class MockPredictor:
                def predict_batch(self, batch_data):
                    time.sleep(0.001 * len(batch_data))  # å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                    return [np.random.randn(5) for _ in batch_data]

            predictor = MockPredictor()

            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            batch_results = optimizer.batch_processor.process_batch(predictor)
            processing_time = time.time() - start_time

            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—
            throughput = len(test_requests) / processing_time if processing_time > 0 else 0

            results[batch_size] = {
                'processing_time': processing_time,
                'throughput': throughput,
                'batch_results': len(batch_results)
            }

            print(f"  Batch {batch_size}: {throughput:.1f} req/s")

        # æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚ºæ±ºå®š
        optimal_batch_size = max(results.keys(),
                               key=lambda k: results[k]['throughput'])

        optimizer.batch_processor.batch_size = optimal_batch_size

        print(f"âœ… æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {optimal_batch_size}")

        return results, optimal_batch_size

# ä½¿ç”¨ä¾‹
cpu_optimizer = CPUOptimizer()

# NumPyæœ€é©åŒ–
cpu_optimizer.optimize_numpy_threads()

# ä¸¦åˆ—å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
benchmark_result = cpu_optimizer.benchmark_threading_vs_multiprocessing()

# ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
batch_results, optimal_batch = cpu_optimizer.optimize_batch_processing()
```

## ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–

### é«˜åº¦ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

```python
import hashlib
import pickle
from collections import OrderedDict
from typing import Any, Optional

class AdvancedCacheManager:
    """é«˜åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.cache_strategies = {
            'lru': self._lru_strategy,
            'lfu': self._lfu_strategy,
            'adaptive': self._adaptive_strategy
        }

    def analyze_cache_performance(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ€§èƒ½ã®è©³ç´°åˆ†æ"""
        from integrated_performance_optimizer import get_integrated_optimizer

        optimizer = get_integrated_optimizer()

        # ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ†æ
        model_cache = optimizer.model_cache
        model_stats = model_cache.get_stats()

        # ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ†æ
        feature_cache = optimizer.feature_cache
        feature_stats = feature_cache.get_stats()

        # ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        model_access_patterns = self._analyze_access_patterns(
            model_cache.access_times
        )

        feature_access_patterns = self._analyze_access_patterns(
            feature_cache.last_access
        )

        analysis = {
            'model_cache': {
                'stats': model_stats,
                'access_patterns': model_access_patterns,
                'efficiency_score': self._calculate_efficiency_score(model_stats)
            },
            'feature_cache': {
                'stats': feature_stats,
                'access_patterns': feature_access_patterns,
                'efficiency_score': self._calculate_efficiency_score(feature_stats)
            }
        }

        return analysis

    def _analyze_access_patterns(self, access_times):
        """ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if not access_times:
            return {'pattern': 'no_data'}

        import numpy as np
        current_time = time.time()

        # æœ€çµ‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“ã®åˆ†å¸ƒ
        last_access_delays = [current_time - t for t in access_times.values()]

        patterns = {
            'total_items': len(access_times),
            'avg_delay_hours': np.mean(last_access_delays) / 3600,
            'max_delay_hours': np.max(last_access_delays) / 3600,
            'recent_access_count': sum(1 for d in last_access_delays if d < 3600),  # 1æ™‚é–“ä»¥å†…
            'pattern': 'frequent' if np.mean(last_access_delays) < 3600 else 'sparse'
        }

        return patterns

    def _calculate_efficiency_score(self, stats):
        """åŠ¹ç‡ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        hit_rate = stats.get('hit_rate', 0)
        utilization = stats.get('size', 0) / max(stats.get('max_size', 1), 1)

        # åŠ¹ç‡ã‚¹ã‚³ã‚¢ = ãƒ’ãƒƒãƒˆç‡ * ä½¿ç”¨ç‡ * 100
        efficiency = hit_rate * utilization * 100

        return efficiency

    def optimize_cache_sizes(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®å‹•çš„æœ€é©åŒ–"""
        from integrated_performance_optimizer import get_integrated_optimizer

        optimizer = get_integrated_optimizer()
        analysis = self.analyze_cache_performance()

        # ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºèª¿æ•´
        model_efficiency = analysis['model_cache']['efficiency_score']
        model_pattern = analysis['model_cache']['access_patterns']['pattern']

        if model_efficiency < 30:  # åŠ¹ç‡ãŒä½ã„
            new_model_size = max(20, optimizer.model_cache.max_size * 0.7)
        elif model_efficiency > 80 and model_pattern == 'frequent':
            new_model_size = min(200, optimizer.model_cache.max_size * 1.3)
        else:
            new_model_size = optimizer.model_cache.max_size

        # ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºèª¿æ•´
        feature_efficiency = analysis['feature_cache']['efficiency_score']
        feature_pattern = analysis['feature_cache']['access_patterns']['pattern']

        if feature_efficiency < 30:
            new_feature_size = max(1000, optimizer.feature_cache.max_features * 0.7)
        elif feature_efficiency > 80 and feature_pattern == 'frequent':
            new_feature_size = min(10000, optimizer.feature_cache.max_features * 1.3)
        else:
            new_feature_size = optimizer.feature_cache.max_features

        # ã‚µã‚¤ã‚ºé©ç”¨
        old_model_size = optimizer.model_cache.max_size
        old_feature_size = optimizer.feature_cache.max_features

        optimizer.model_cache.max_size = int(new_model_size)
        optimizer.feature_cache.max_features = int(new_feature_size)

        optimization_result = {
            'model_cache': {
                'old_size': old_model_size,
                'new_size': int(new_model_size),
                'efficiency_before': model_efficiency
            },
            'feature_cache': {
                'old_size': old_feature_size,
                'new_size': int(new_feature_size),
                'efficiency_before': feature_efficiency
            }
        }

        print(f"ğŸ¯ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºæœ€é©åŒ–:")
        print(f"  ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {old_model_size} â†’ {int(new_model_size)}")
        print(f"  ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {old_feature_size} â†’ {int(new_feature_size)}")

        return optimization_result

    def implement_smart_prefetching(self, prediction_patterns):
        """ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ³ã‚°"""
        from integrated_performance_optimizer import get_integrated_optimizer

        optimizer = get_integrated_optimizer()

        # ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š
        frequent_models = self._identify_frequent_patterns(prediction_patterns)

        # ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        preloaded_count = 0
        for model_type in frequent_models:
            if model_type not in optimizer.model_cache.cache:
                # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å®Ÿãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
                dummy_model = {
                    'type': model_type,
                    'weights': np.random.randn(50, 20),
                    'preloaded': True
                }
                optimizer.model_cache.put(f"preload_{model_type}", dummy_model)
                preloaded_count += 1

        print(f"ğŸš€ ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ³ã‚°: {preloaded_count} ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰")

        return preloaded_count

    def _identify_frequent_patterns(self, patterns):
        """é »ç¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‰¹å®š"""
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆç°¡ç•¥ç‰ˆï¼‰
        if not patterns:
            return ['linear', 'tree']  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        # ä½¿ç”¨é »åº¦åˆ†æ
        frequency = {}
        for pattern in patterns:
            model_type = pattern.get('model_type', 'unknown')
            frequency[model_type] = frequency.get(model_type, 0) + 1

        # ä¸Šä½3ã¤ã‚’è¿”ã™
        sorted_patterns = sorted(frequency.items(), key=lambda x: x[1], reverse=True)
        return [pattern[0] for pattern in sorted_patterns[:3]]

# ä½¿ç”¨ä¾‹
cache_manager = AdvancedCacheManager()

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ€§èƒ½åˆ†æ
analysis = cache_manager.analyze_cache_performance()
print("ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ†æçµæœ:")
print(f"  ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡: {analysis['model_cache']['efficiency_score']:.1f}%")
print(f"  ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡: {analysis['feature_cache']['efficiency_score']:.1f}%")

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºæœ€é©åŒ–
optimization_result = cache_manager.optimize_cache_sizes()

# ã‚¹ãƒãƒ¼ãƒˆãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ³ã‚°
sample_patterns = [
    {'model_type': 'linear', 'timestamp': time.time()},
    {'model_type': 'tree', 'timestamp': time.time()},
    {'model_type': 'linear', 'timestamp': time.time()},
]
cache_manager.implement_smart_prefetching(sample_patterns)
```

## çµ±åˆæœ€é©åŒ–æˆ¦ç•¥

### è‡ªå‹•æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

```python
import schedule
from datetime import datetime, time as dt_time

class OptimizationScheduler:
    """æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""

    def __init__(self):
        self.optimization_history = []
        self.active_schedules = []

    def setup_optimization_schedule(self):
        """æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š"""

        # è»½é‡æœ€é©åŒ–ï¼ˆæ¯æ™‚ï¼‰
        schedule.every().hour.do(self._hourly_optimization)

        # ä¸­ç¨‹åº¦æœ€é©åŒ–ï¼ˆ6æ™‚é–“ã”ã¨ï¼‰
        schedule.every(6).hours.do(self._medium_optimization)

        # é‡åº¦æœ€é©åŒ–ï¼ˆæ—¥æ¬¡ï¼‰
        schedule.every().day.at("03:00").do(self._daily_optimization)

        # é€±æ¬¡æœ€é©åŒ–ï¼ˆé€±æœ«ï¼‰
        schedule.every().sunday.at("02:00").do(self._weekly_optimization)

        print("ğŸ“… æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®šå®Œäº†")

    def _hourly_optimization(self):
        """æ¯æ™‚æœ€é©åŒ–"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M')}] è»½é‡æœ€é©åŒ–å®Ÿè¡Œ")

        from integrated_performance_optimizer import get_integrated_optimizer
        optimizer = get_integrated_optimizer()

        # è»½é‡ãªæœ€é©åŒ–ã®ã¿
        results = []

        # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
        report = get_system_performance_report()
        if report['system_state']['memory_usage_mb'] > 600:
            memory_result = optimizer.optimize_memory_usage()
            results.append(('memory', memory_result))

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãƒã‚§ãƒƒã‚¯
        model_hit_rate = report['cache_performance']['model_cache']['hit_rate']
        if model_hit_rate < 0.7:
            cache_result = optimizer.optimize_model_cache()
            results.append(('cache', cache_result))

        self._log_optimization('hourly', results)

    def _medium_optimization(self):
        """ä¸­ç¨‹åº¦æœ€é©åŒ–"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M')}] ä¸­ç¨‹åº¦æœ€é©åŒ–å®Ÿè¡Œ")

        from integrated_performance_optimizer import get_integrated_optimizer
        optimizer = get_integrated_optimizer()

        results = []

        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        memory_result = optimizer.optimize_memory_usage()
        results.append(('memory', memory_result))

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
        cache_result = optimizer.optimize_model_cache()
        results.append(('cache', cache_result))

        # ç‰¹å¾´é‡å‡¦ç†æœ€é©åŒ–
        feature_result = optimizer.optimize_feature_processing()
        results.append(('feature', feature_result))

        self._log_optimization('medium', results)

    def _daily_optimization(self):
        """æ—¥æ¬¡æœ€é©åŒ–"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M')}] åŒ…æ‹¬çš„æœ€é©åŒ–å®Ÿè¡Œ")

        from integrated_performance_optimizer import optimize_system_performance

        # åŒ…æ‹¬çš„æœ€é©åŒ–å®Ÿè¡Œ
        results = optimize_system_performance()

        # é«˜åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
        cache_manager = AdvancedCacheManager()
        cache_optimization = cache_manager.optimize_cache_sizes()

        # CPUæœ€é©åŒ–
        cpu_optimizer = CPUOptimizer()
        cpu_optimizer.optimize_numpy_threads()

        self._log_optimization('daily', results + [('advanced_cache', cache_optimization)])

    def _weekly_optimization(self):
        """é€±æ¬¡æœ€é©åŒ–"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M')}] é€±æ¬¡ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿè¡Œ")

        # å±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._cleanup_history()

        # æ·±åº¦ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        memory_manager = AdvancedMemoryManager()
        cleanup_result = memory_manager.aggressive_cleanup()

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        self._rotate_logs()

        self._log_optimization('weekly', [('deep_cleanup', cleanup_result)])

    def _log_optimization(self, optimization_type, results):
        """æœ€é©åŒ–ãƒ­ã‚°è¨˜éŒ²"""
        log_entry = {
            'timestamp': datetime.now(),
            'type': optimization_type,
            'results': results,
            'total_improvements': sum(
                r[1].get('improvement_percent', 0)
                for r in results
                if isinstance(r[1], dict)
            )
        }

        self.optimization_history.append(log_entry)

        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.optimization_history) > 1000:
            self.optimization_history = self.optimization_history[-500:]

        print(f"  ç·æ”¹å–„: {log_entry['total_improvements']:.1f}%")

    def _cleanup_history(self):
        """å±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # 30æ—¥ä»¥ä¸Šå¤ã„å±¥æ­´ã‚’å‰Šé™¤
        cutoff_date = datetime.now() - timedelta(days=30)
        self.optimization_history = [
            entry for entry in self.optimization_history
            if entry['timestamp'] > cutoff_date
        ]

        print(f"ğŸ“š å±¥æ­´ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {len(self.optimization_history)} ã‚¨ãƒ³ãƒˆãƒªä¿æŒ")

    def _rotate_logs(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†
        print("ğŸ“‹ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†")

    def run_scheduler(self, duration_minutes=None):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œ"""
        print("â° æœ€é©åŒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹")

        start_time = time.time()

        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯

                # åˆ¶é™æ™‚é–“ãƒã‚§ãƒƒã‚¯
                if duration_minutes:
                    elapsed = (time.time() - start_time) / 60
                    if elapsed >= duration_minutes:
                        break

        except KeyboardInterrupt:
            print("\nâ¹ï¸  ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼åœæ­¢")

        print("âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çµ‚äº†")

    def get_optimization_summary(self, days=7):
        """æœ€é©åŒ–ã‚µãƒãƒªãƒ¼å–å¾—"""
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_optimizations = [
            entry for entry in self.optimization_history
            if entry['timestamp'] > cutoff_date
        ]

        if not recent_optimizations:
            return {"message": "æœ€è¿‘ã®æœ€é©åŒ–å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"}

        summary = {
            'total_optimizations': len(recent_optimizations),
            'by_type': {},
            'total_improvement': sum(entry['total_improvements'] for entry in recent_optimizations),
            'avg_improvement': 0,
            'period_days': days
        }

        # ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
        for entry in recent_optimizations:
            opt_type = entry['type']
            summary['by_type'][opt_type] = summary['by_type'].get(opt_type, 0) + 1

        # å¹³å‡æ”¹å–„ç‡
        if recent_optimizations:
            summary['avg_improvement'] = summary['total_improvement'] / len(recent_optimizations)

        return summary

# ä½¿ç”¨ä¾‹
scheduler = OptimizationScheduler()

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
scheduler.setup_optimization_schedule()

# æ‰‹å‹•æœ€é©åŒ–å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
scheduler._hourly_optimization()

# ã‚µãƒãƒªãƒ¼ç¢ºèª
summary = scheduler.get_optimization_summary(days=1)
print("æœ€é©åŒ–ã‚µãƒãƒªãƒ¼:")
print(f"  å®Ÿè¡Œå›æ•°: {summary['total_optimizations']}")
print(f"  ç·æ”¹å–„: {summary['total_improvement']:.1f}%")
print(f"  å¹³å‡æ”¹å–„: {summary['avg_improvement']:.1f}%")

# ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼å®Ÿè¡Œï¼ˆçŸ­æ™‚é–“ãƒ†ã‚¹ãƒˆï¼‰
# scheduler.run_scheduler(duration_minutes=5)
```

## ã¾ã¨ã‚

ã“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ä»¥ä¸‹ã®ä¸»è¦ãªæœ€é©åŒ–é ˜åŸŸã‚’ã‚«ãƒãƒ¼ã—ã¾ã—ãŸï¼š

1. **ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã¨è‡ªå‹•è­¦å‘Šã‚·ã‚¹ãƒ†ãƒ 
2. **ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**: ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
3. **CPUæœ€é©åŒ–**: ä¸¦åˆ—å‡¦ç†ã€NumPyè¨­å®šã€ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
4. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–**: ã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã€å‹•çš„ã‚µã‚¤ã‚ºèª¿æ•´ã€ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒãƒ³ã‚°
5. **çµ±åˆæœ€é©åŒ–**: è‡ªå‹•ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€åŒ…æ‹¬çš„æœ€é©åŒ–æˆ¦ç•¥

ã“ã‚Œã‚‰ã®æœ€é©åŒ–æ‰‹æ³•ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€Issue #870æ‹¡å¼µäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã€30-60%ã®äºˆæ¸¬ç²¾åº¦å‘ä¸Šã¨åŒæ™‚ã«å„ªã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã‚’å®Ÿç¾ã§ãã¾ã™ã€‚

å®šæœŸçš„ãªç›£è¦–ã¨æœ€é©åŒ–ã®å®Ÿè¡Œã«ã‚ˆã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ã¯å¸¸ã«æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ç¶šã‘ã¾ã™ã€‚