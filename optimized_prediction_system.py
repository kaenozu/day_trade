#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Optimized Prediction System - æœ€é©åŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

ç¾å®Ÿçš„ãªãƒ‡ãƒ¼ã‚¿é‡ã§å‹•ä½œã™ã‚‹æ”¹è‰¯ç‰ˆäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
Issue #800-2-1å®Ÿè£…ï¼šäºˆæ¸¬ç²¾åº¦æ”¹å–„è¨ˆç”»
"""

import asyncio
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import sqlite3
from pathlib import Path

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class ModelType(Enum):
    """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"""
    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    LOGISTIC_REGRESSION = "logistic_regression"
    ENSEMBLE_VOTING = "ensemble_voting"

@dataclass
class ModelPerformance:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½"""
    model_type: ModelType
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    roc_auc: float
    cross_val_score: float
    feature_importance: Dict[str, float] = field(default_factory=dict)

@dataclass
class PredictionResult:
    """äºˆæ¸¬çµæœ"""
    symbol: str
    prediction: int  # 0: ä¸‹é™, 1: ä¸Šæ˜‡
    confidence: float
    model_consensus: Dict[str, int]
    timestamp: datetime

class OptimizedFeatureEngineering:
    """æœ€é©åŒ–ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def create_optimized_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """æœ€é©åŒ–ç‰¹å¾´é‡ä½œæˆï¼ˆæœ€å°30ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã§å‹•ä½œï¼‰"""

        if len(data) < 30:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½30ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå¿…è¦ï¼‰")

        features = pd.DataFrame(index=data.index)

        # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        high = data['High']
        low = data['Low']
        close = data['Close']
        volume = data['Volume']
        open_price = data['Open']

        try:
            # 1. åŸºæœ¬ä¾¡æ ¼æŒ‡æ¨™ï¼ˆ10ç¨®é¡ï¼‰
            features['returns'] = close.pct_change()
            features['log_returns'] = np.log(close / close.shift(1))
            features['price_range'] = (high - low) / close
            features['gap'] = (open_price - close.shift(1)) / close.shift(1)
            features['body_size'] = abs(close - open_price) / close
            features['upper_shadow'] = (high - np.maximum(close, open_price)) / close
            features['lower_shadow'] = (np.minimum(close, open_price) - low) / close
            features['hl2'] = (high + low) / 2
            features['hlc3'] = (high + low + close) / 3
            features['ohlc4'] = (open_price + high + low + close) / 4

            # 2. ç§»å‹•å¹³å‡ç³»ï¼ˆ12ç¨®é¡ï¼‰
            for period in [3, 5, 10, 15]:
                if len(data) > period:
                    sma = close.rolling(period).mean()
                    features[f'sma_{period}'] = sma
                    features[f'sma_ratio_{period}'] = close / sma
                    features[f'sma_distance_{period}'] = (close - sma) / sma

            # æŒ‡æ•°ç§»å‹•å¹³å‡
            for period in [5, 12]:
                if len(data) > period:
                    ema = close.ewm(span=period).mean()
                    features[f'ema_{period}'] = ema
                    features[f'ema_ratio_{period}'] = close / ema

            # MACDï¼ˆç°¡å˜ç‰ˆï¼‰
            if len(data) > 12:
                ema_5 = close.ewm(span=5).mean()
                ema_12 = close.ewm(span=12).mean()
                features['macd'] = ema_5 - ema_12
                features['macd_signal'] = features['macd'].ewm(span=3).mean()
                features['macd_histogram'] = features['macd'] - features['macd_signal']

            # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆ8ç¨®é¡ï¼‰
            for period in [3, 5, 10, 15]:
                if len(data) > period:
                    vol = features['returns'].rolling(period).std()
                    features[f'volatility_{period}'] = vol

                    # ATRè¨ˆç®—
                    tr1 = high - low
                    tr2 = abs(high - close.shift(1))
                    tr3 = abs(low - close.shift(1))
                    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
                    features[f'atr_{period}'] = true_range.rolling(period).mean()

            # 4. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™ï¼ˆ10ç¨®é¡ï¼‰
            # RSI
            for period in [5, 10, 14]:
                if len(data) > period:
                    delta = close.diff()
                    gain = (delta.where(delta > 0, 0)).rolling(period).mean()
                    loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
                    rs = gain / loss.replace(0, 1)
                    features[f'rsi_{period}'] = 100 - (100 / (1 + rs))

            # ROC (Rate of Change)
            for period in [3, 5, 10]:
                if len(data) > period:
                    features[f'roc_{period}'] = (close - close.shift(period)) / close.shift(period) * 100
                    features[f'momentum_{period}'] = close / close.shift(period)

            # Stochasticï¼ˆç°¡å˜ç‰ˆï¼‰
            if len(data) > 10:
                period = 10
                lowest_low = low.rolling(period).min()
                highest_high = high.rolling(period).max()
                k_percent = 100 * (close - lowest_low) / (highest_high - lowest_low).replace(0, 1)
                features['stoch_k'] = k_percent
                features['stoch_d'] = k_percent.rolling(3).mean()

            # Williams %R
            if len(data) > 10:
                period = 10
                highest_high = high.rolling(period).max()
                lowest_low = low.rolling(period).min()
                features['williams_r'] = -100 * (highest_high - close) / (highest_high - lowest_low).replace(0, 1)

            # 5. ãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™ï¼ˆ6ç¨®é¡ï¼‰
            for period in [5, 10]:
                if len(data) > period:
                    vol_sma = volume.rolling(period).mean()
                    features[f'volume_sma_{period}'] = vol_sma
                    features[f'volume_ratio_{period}'] = volume / vol_sma

            features['price_volume'] = close * volume
            if len(data) > 10:
                features['vwap'] = (features['price_volume'].rolling(10).sum() / volume.rolling(10).sum())

            # Volume Price Trend
            if len(data) > 5:
                features['vpt'] = ((close - close.shift(1)) / close.shift(1) * volume).rolling(5).sum()

            # On Balance Volumeï¼ˆç°¡å˜ç‰ˆï¼‰
            obv = pd.Series(index=close.index, dtype=float)
            obv.iloc[0] = volume.iloc[0]
            for i in range(1, len(close)):
                if close.iloc[i] > close.iloc[i-1]:
                    obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]
                elif close.iloc[i] < close.iloc[i-1]:
                    obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]
                else:
                    obv.iloc[i] = obv.iloc[i-1]
            features['obv'] = obv

            # 6. ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰ï¼ˆ4ç¨®é¡ï¼‰
            if len(data) > 15:
                period = 15
                sma = close.rolling(period).mean()
                std = close.rolling(period).std()

                upper = sma + (std * 2)
                lower = sma - (std * 2)

                features['bb_upper'] = upper
                features['bb_lower'] = lower
                features['bb_width'] = (upper - lower) / sma
                features['bb_position'] = (close - lower) / (upper - lower).replace(0, 1)

            # 7. ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ï¼ˆ5ç¨®é¡ï¼‰
            # ADXè¨ˆç®—ï¼ˆç°¡å˜ç‰ˆï¼‰
            if len(data) > 10:
                period = 10
                plus_dm = high.diff()
                minus_dm = -low.diff()
                plus_dm[plus_dm < 0] = 0
                minus_dm[minus_dm < 0] = 0

                if f'atr_{period}' in features.columns:
                    atr = features[f'atr_{period}']
                    plus_di = 100 * (plus_dm.rolling(period).mean() / atr)
                    minus_di = 100 * (minus_dm.rolling(period).mean() / atr)

                    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, 1)
                    features['adx'] = dx.rolling(period).mean()
                    features['plus_di'] = plus_di
                    features['minus_di'] = minus_di

            # Aroonï¼ˆç°¡å˜ç‰ˆï¼‰
            if len(data) > 10:
                period = 10
                def find_highest_index(x):
                    return len(x) - 1 - np.argmax(x) if len(x) > 0 else 0
                def find_lowest_index(x):
                    return len(x) - 1 - np.argmin(x) if len(x) > 0 else 0

                aroon_up = high.rolling(period + 1).apply(find_highest_index, raw=True) / period * 100
                aroon_down = low.rolling(period + 1).apply(find_lowest_index, raw=True) / period * 100

                features['aroon_up'] = aroon_up
                features['aroon_down'] = aroon_down

            # 8. çµ±è¨ˆçš„æŒ‡æ¨™ï¼ˆ6ç¨®é¡ï¼‰
            for period in [5, 10]:
                if len(data) > period:
                    ret_window = features['returns'].rolling(period)
                    features[f'skewness_{period}'] = ret_window.skew()
                    features[f'kurtosis_{period}'] = ret_window.kurt()
                    features[f'sharpe_ratio_{period}'] = (ret_window.mean() / ret_window.std() * np.sqrt(252))

            # 9. ã‚«ã‚¹ã‚¿ãƒ è¤‡åˆæŒ‡æ¨™ï¼ˆ8ç¨®é¡ï¼‰
            # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
            if len(data) > 5:
                features['trend_strength'] = (close > close.shift(1)).rolling(5).sum() / 5

            # ä¾¡æ ¼åŠ¹ç‡æ€§
            if len(data) > 10 and 'atr_10' in features.columns:
                features['price_efficiency'] = abs(close - close.shift(10)) / features['atr_10'] / 10

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ
            if 'volatility_5' in features.columns:
                features['vol_breakout'] = (features['volatility_5'] > features['volatility_5'].rolling(10).mean()).astype(int)

            # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ç™ºæ•£
            if 'rsi_10' in features.columns and 'roc_5' in features.columns:
                features['momentum_divergence'] = features['rsi_10'] / 50 - features['roc_5'] / features['roc_5'].rolling(10).std()

            # ãƒœãƒªãƒ¥ãƒ¼ãƒ ãƒ»ä¾¡æ ¼ç™ºæ•£
            if 'volume_ratio_5' in features.columns:
                features['volume_price_divergence'] = (features['volume_ratio_5'] - 1) * (features['returns'] > 0).astype(int)

            # è¤‡åˆå¼·åº¦æŒ‡æ¨™
            if all(col in features.columns for col in ['rsi_10', 'stoch_k', 'williams_r']):
                features['composite_strength'] = (
                    features['rsi_10'] / 100 +
                    features['stoch_k'] / 100 +
                    (features['williams_r'] + 100) / 100
                ) / 3

            # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ»ãƒœãƒªãƒ¥ãƒ¼ãƒ åˆæµ
            if 'adx' in features.columns and 'volume_ratio_5' in features.columns:
                features['trend_volume_confluence'] = features['adx'] * features['volume_ratio_5'] / 100

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ»ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
            if 'volatility_10' in features.columns and 'roc_5' in features.columns:
                features['volatility_momentum'] = features['volatility_10'] * abs(features['roc_5'])

        except Exception as e:
            self.logger.error(f"ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        # NaNå€¤å‡¦ç†ï¼ˆéæ¨å¥¨è­¦å‘Šå¯¾å¿œï¼‰
        features = features.ffill().bfill()

        # ç„¡é™å¤§å€¤å‡¦ç†
        features = features.replace([np.inf, -np.inf], 0)

        # ç•°å¸¸å€¤é™¤å»ï¼ˆ3ã‚·ã‚°ãƒãƒ«ãƒ¼ãƒ«ï¼‰
        for col in features.select_dtypes(include=[np.number]).columns:
            mean = features[col].mean()
            std = features[col].std()
            if std > 0:
                features[col] = features[col].clip(mean - 3*std, mean + 3*std)

        # æœ€åˆã®20è¡Œã‚’å‰Šé™¤ï¼ˆæŒ‡æ¨™è¨ˆç®—ã®ãŸã‚ï¼‰
        if len(features) > 20:
            features = features.iloc[20:].copy()

        self.logger.info(f"æœ€é©åŒ–ç‰¹å¾´é‡ä½œæˆå®Œäº†: {features.shape[1]}ç‰¹å¾´é‡, {features.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")
        return features

class OptimizedPredictionSystem:
    """æœ€é©åŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.feature_engineering = OptimizedFeatureEngineering()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = Path("ml_models_data/optimized_predictions.db")
        self.db_path.parent.mkdir(exist_ok=True)

        # è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        self.trained_models: Dict[str, Any] = {}

        self.logger.info("Optimized prediction system initialized")

    async def train_optimized_models(self, symbol: str, period: str = "6mo") -> Dict[ModelType, ModelPerformance]:
        """æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""

        self.logger.info(f"æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {symbol}")

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, period)

            if data is None or len(data) < 30:
                raise ValueError("è¨“ç·´ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            features = self.feature_engineering.create_optimized_features(data)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆç¿Œæ—¥ã®ä¸Šæ˜‡/ä¸‹é™ï¼‰
            targets = self._create_targets(data.iloc[20:]['Close'])  # ç‰¹å¾´é‡ã¨åŒã˜æœŸé–“

            # ãƒ‡ãƒ¼ã‚¿åŒæœŸ
            min_length = min(len(features), len(targets))
            features = features.iloc[:min_length]
            targets = targets[:min_length]

            if len(features) < 15:
                raise ValueError("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

            # ç‰¹å¾´é¸æŠï¼ˆé‡è¦ãªç‰¹å¾´é‡ã®ã¿ï¼‰
            max_features = min(30, features.shape[1])
            features_selected = self._select_best_features(features, targets, k=max_features)

            # å„ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            performances = {}

            # Random Forest
            rf_performance = await self._train_random_forest(features_selected, targets, symbol)
            performances[ModelType.RANDOM_FOREST] = rf_performance

            # Gradient Boosting
            gb_performance = await self._train_gradient_boosting(features_selected, targets, symbol)
            performances[ModelType.GRADIENT_BOOSTING] = gb_performance

            # Logistic Regression
            lr_performance = await self._train_logistic_regression(features_selected, targets, symbol)
            performances[ModelType.LOGISTIC_REGRESSION] = lr_performance

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
            if len(performances) >= 2:
                ensemble_performance = await self._create_ensemble_model(features_selected, targets, symbol)
                performances[ModelType.ENSEMBLE_VOTING] = ensemble_performance

            # çµæœä¿å­˜
            await self._save_model_performances(symbol, performances)

            self.logger.info(f"æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(performances)}ãƒ¢ãƒ‡ãƒ«")
            return performances

        except Exception as e:
            self.logger.error(f"æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _create_targets(self, prices: pd.Series) -> np.ndarray:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ"""
        # ç¿Œæ—¥ã®ä¾¡æ ¼å¤‰å‹•ç‡
        returns = prices.pct_change().shift(-1)  # ç¿Œæ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³

        # é–¾å€¤ä»¥ä¸Šã®ä¸Šæ˜‡ã‚’1ã€ãã‚Œä»¥å¤–ã‚’0
        threshold = 0.002  # 0.2%ä»¥ä¸Šã®ä¸Šæ˜‡
        targets = (returns > threshold).astype(int)

        return targets.values[:-1]  # æœ€å¾Œã®è¦ç´ ï¼ˆNaNï¼‰ã‚’é™¤å»

    def _select_best_features(self, features: pd.DataFrame, targets: np.ndarray, k: int = 30) -> pd.DataFrame:
        """æœ€é©ç‰¹å¾´é‡é¸æŠ"""

        k = min(k, features.shape[1])
        selector = SelectKBest(score_func=f_classif, k=k)

        features_selected = selector.fit_transform(features, targets)
        selected_columns = features.columns[selector.get_support()]

        return pd.DataFrame(features_selected, columns=selected_columns, index=features.index)

    async def _train_random_forest(self, features: pd.DataFrame, targets: np.ndarray, symbol: str) -> ModelPerformance:
        """Random Forestè¨“ç·´"""

        model = RandomForestClassifier(
            n_estimators=50,
            max_depth=8,
            min_samples_split=3,
            min_samples_leaf=2,
            random_state=42
        )

        return await self._train_and_evaluate_model(model, features, targets, ModelType.RANDOM_FOREST, symbol)

    async def _train_gradient_boosting(self, features: pd.DataFrame, targets: np.ndarray, symbol: str) -> ModelPerformance:
        """Gradient Boostingè¨“ç·´"""

        model = GradientBoostingClassifier(
            n_estimators=50,
            learning_rate=0.1,
            max_depth=5,
            min_samples_split=3,
            random_state=42
        )

        return await self._train_and_evaluate_model(model, features, targets, ModelType.GRADIENT_BOOSTING, symbol)

    async def _train_logistic_regression(self, features: pd.DataFrame, targets: np.ndarray, symbol: str) -> ModelPerformance:
        """Logistic Regressionè¨“ç·´"""

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        features_scaled = pd.DataFrame(features_scaled, columns=features.columns, index=features.index)

        model = LogisticRegression(
            C=1.0,
            penalty='l2',
            max_iter=500,
            random_state=42
        )

        performance = await self._train_and_evaluate_model(model, features_scaled, targets, ModelType.LOGISTIC_REGRESSION, symbol)

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚‚ä¿å­˜
        model_key = f"{symbol}_{ModelType.LOGISTIC_REGRESSION.value}"
        if model_key in self.trained_models:
            self.trained_models[model_key]['scaler'] = scaler

        return performance

    async def _train_and_evaluate_model(self, model, features: pd.DataFrame, targets: np.ndarray,
                                      model_type: ModelType, symbol: str) -> ModelPerformance:
        """ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»è©•ä¾¡"""

        # è¨“ç·´
        model.fit(features, targets)

        # äºˆæ¸¬
        predictions = model.predict(features)
        prediction_proba = model.predict_proba(features)[:, 1] if hasattr(model, 'predict_proba') else predictions

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯2-foldï¼‰
        cv_folds = min(3, len(features) // 5) if len(features) >= 10 else 2
        cv_scores = cross_val_score(model, features, targets, cv=cv_folds)

        # ç‰¹å¾´é‡è¦åº¦
        feature_importance = {}
        if hasattr(model, 'feature_importances_'):
            for i, importance in enumerate(model.feature_importances_):
                feature_importance[features.columns[i]] = float(importance)

        performance = ModelPerformance(
            model_type=model_type,
            accuracy=accuracy_score(targets, predictions),
            precision=precision_score(targets, predictions, average='weighted', zero_division=0),
            recall=recall_score(targets, predictions, average='weighted', zero_division=0),
            f1_score=f1_score(targets, predictions, average='weighted', zero_division=0),
            roc_auc=roc_auc_score(targets, prediction_proba) if len(np.unique(targets)) > 1 else 0.5,
            cross_val_score=float(cv_scores.mean()),
            feature_importance=feature_importance
        )

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_key = f"{symbol}_{model_type.value}"
        self.trained_models[model_key] = {
            'model': model,
            'feature_columns': features.columns.tolist(),
            'performance': performance
        }

        return performance

    async def _create_ensemble_model(self, features: pd.DataFrame, targets: np.ndarray, symbol: str) -> ModelPerformance:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""

        # æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        best_models = []
        for model_key, model_data in self.trained_models.items():
            if symbol in model_key and model_data['performance'].accuracy > 0.45:
                model_name = model_data['performance'].model_type.value
                best_models.append((model_name, model_data['model']))

        if len(best_models) < 2:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
            best_models = [
                (model_data['performance'].model_type.value, model_data['model'])
                for model_key, model_data in self.trained_models.items()
                if symbol in model_key
            ]

        # Voting Classifierä½œæˆ
        voting_classifier = VotingClassifier(
            estimators=best_models,
            voting='soft' if all(hasattr(model, 'predict_proba') for _, model in best_models) else 'hard'
        )

        return await self._train_and_evaluate_model(voting_classifier, features, targets, ModelType.ENSEMBLE_VOTING, symbol)

    async def _save_model_performances(self, symbol: str, performances: Dict[ModelType, ModelPerformance]):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS optimized_performances (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        model_type TEXT NOT NULL,
                        accuracy REAL,
                        precision_score REAL,
                        recall_score REAL,
                        f1_score REAL,
                        roc_auc REAL,
                        cross_val_score REAL,
                        feature_importance TEXT,
                        created_at TEXT,
                        UNIQUE(symbol, model_type)
                    )
                ''')

                # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
                for model_type, performance in performances.items():
                    cursor.execute('''
                        INSERT OR REPLACE INTO optimized_performances
                        (symbol, model_type, accuracy, precision_score, recall_score,
                         f1_score, roc_auc, cross_val_score, feature_importance, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        symbol,
                        model_type.value,
                        performance.accuracy,
                        performance.precision,
                        performance.recall,
                        performance.f1_score,
                        performance.roc_auc,
                        performance.cross_val_score,
                        json.dumps(performance.feature_importance),
                        datetime.now().isoformat()
                    ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"æ€§èƒ½ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    async def predict_with_optimized_models(self, symbol: str) -> PredictionResult:
        """æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬"""

        try:
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, "2mo")

            # ç‰¹å¾´é‡ä½œæˆ
            features = self.feature_engineering.create_optimized_features(data)
            latest_features = features.iloc[-1:].copy()

            # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            model_predictions = {}
            confidences = []

            for model_key, model_data in self.trained_models.items():
                if symbol in model_key:
                    try:
                        model = model_data['model']
                        feature_columns = model_data['feature_columns']

                        # ç‰¹å¾´é‡ã‚’åˆã‚ã›ã‚‹
                        X = latest_features[feature_columns].copy()

                        # Logistic Regressionã®å ´åˆã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
                        if 'scaler' in model_data:
                            X = model_data['scaler'].transform(X)
                            X = pd.DataFrame(X, columns=feature_columns, index=latest_features.index)

                        # äºˆæ¸¬
                        prediction = model.predict(X)[0]
                        confidence = model.predict_proba(X)[0].max() if hasattr(model, 'predict_proba') else 0.6

                        model_type = model_data['performance'].model_type.value
                        model_predictions[model_type] = int(prediction)
                        confidences.append(confidence)

                    except Exception as e:
                        self.logger.warning(f"äºˆæ¸¬å¤±æ•— {model_key}: {e}")
                        continue

            # æœ€çµ‚äºˆæ¸¬ï¼ˆå¤šæ•°æ±ºï¼‰
            if model_predictions:
                final_prediction = int(np.mean(list(model_predictions.values())) >= 0.5)
                final_confidence = float(np.mean(confidences))
            else:
                final_prediction = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬
                final_confidence = 0.5

            return PredictionResult(
                symbol=symbol,
                prediction=final_prediction,
                confidence=final_confidence,
                model_consensus=model_predictions,
                timestamp=datetime.now()
            )

        except Exception as e:
            self.logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬ã‚’è¿”ã™
            return PredictionResult(
                symbol=symbol,
                prediction=1,
                confidence=0.5,
                model_consensus={},
                timestamp=datetime.now()
            )

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
optimized_prediction_system = OptimizedPredictionSystem()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def test_optimized_system():
    """æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""

    print("=== æœ€é©åŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")

    test_symbols = ["7203", "8306", "4751"]

    for symbol in test_symbols:
        print(f"\nğŸ¤– {symbol} æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")

        try:
            performances = await optimized_prediction_system.train_optimized_models(symbol, "6mo")

            print(f"ğŸ“Š {symbol} è¨“ç·´çµæœ:")
            for model_type, performance in performances.items():
                print(f"  {model_type.value}: ç²¾åº¦{performance.accuracy:.3f} F1{performance.f1_score:.3f} CV{performance.cross_val_score:.3f}")

            # äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
            prediction = await optimized_prediction_system.predict_with_optimized_models(symbol)
            print(f"ğŸ”® {symbol} äºˆæ¸¬: {'ä¸Šæ˜‡' if prediction.prediction else 'ä¸‹é™'} (ä¿¡é ¼åº¦: {prediction.confidence:.3f})")

        except Exception as e:
            print(f"âŒ {symbol} ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\nâœ… æœ€é©åŒ–äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_optimized_system())