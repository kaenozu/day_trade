#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Market Data Stability System - å®Ÿå¸‚å ´ãƒ‡ãƒ¼ã‚¿å®‰å®šæ€§ã‚·ã‚¹ãƒ†ãƒ 

Issue #804å®Ÿè£…ï¼šå®Ÿå¸‚å ´ãƒ‡ãƒ¼ã‚¿é€£ç¶šå–å¾—ãƒ»å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§å‘ä¸Š
ãƒ‡ãƒ¼ã‚¿å–å¾—ã®å†—é•·æ€§ã€ã‚¨ãƒ©ãƒ¼å›å¾©ã€æ¥ç¶šå®‰å®šæ€§ã®å‘ä¸Š
"""

import asyncio
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union, Callable
from dataclasses import dataclass, field
from enum import Enum
import json
import sqlite3
from pathlib import Path
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque
import random
import aiohttp
import requests

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class DataSource(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹"""
    YAHOO_FINANCE = "yahoo_finance"
    YAHOO_FINANCE_ALT = "yahoo_finance_alt"
    STOOQ = "stooq"
    LOCAL_CACHE = "local_cache"
    FALLBACK = "fallback"

class DataQuality(Enum):
    """ãƒ‡ãƒ¼ã‚¿å“è³ª"""
    EXCELLENT = "excellent"  # å®Œå…¨ãƒ‡ãƒ¼ã‚¿
    GOOD = "good"           # è»½å¾®ãªæ¬ æ
    FAIR = "fair"           # ä¸­ç¨‹åº¦ã®æ¬ æ
    POOR = "poor"           # å¤§ããªæ¬ æ
    CRITICAL = "critical"   # ä½¿ç”¨ä¸å¯

@dataclass
class DataFetchAttempt:
    """ãƒ‡ãƒ¼ã‚¿å–å¾—è©¦è¡Œè¨˜éŒ²"""
    symbol: str
    source: DataSource
    start_time: datetime
    end_time: Optional[datetime] = None
    success: bool = False
    data_points: int = 0
    error_message: Optional[str] = None
    response_time: float = 0.0
    quality: Optional[DataQuality] = None

@dataclass
class DataReliabilityMetrics:
    """ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    symbol: str
    total_attempts: int
    successful_attempts: int
    avg_response_time: float
    best_source: DataSource
    data_coverage: float  # 0-1
    consistency_score: float  # 0-1
    last_successful_fetch: Optional[datetime] = None

class DataSourceManager:
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å„ªå…ˆé †ä½
        self.source_priority = [
            DataSource.YAHOO_FINANCE,
            DataSource.YAHOO_FINANCE_ALT,
            DataSource.STOOQ,
            DataSource.LOCAL_CACHE,
            DataSource.FALLBACK
        ]

        # å„ã‚½ãƒ¼ã‚¹ã®è¨­å®š
        self.source_config = {
            DataSource.YAHOO_FINANCE: {
                'timeout': 10,
                'retry_count': 3,
                'cooldown': 1
            },
            DataSource.YAHOO_FINANCE_ALT: {
                'timeout': 15,
                'retry_count': 2,
                'cooldown': 2
            },
            DataSource.STOOQ: {
                'timeout': 20,
                'retry_count': 2,
                'cooldown': 3
            }
        }

        # å¤±æ•—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.failure_counts = {source: 0 for source in DataSource}
        self.source_cooldowns = {source: None for source in DataSource}

        # æˆåŠŸç‡è¿½è·¡
        self.success_rates = {source: 1.0 for source in DataSource}
        self.attempt_history = deque(maxlen=1000)

    async def fetch_with_fallback(self, symbol: str, period: str = "5d") -> Optional[pd.DataFrame]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ããƒ‡ãƒ¼ã‚¿å–å¾—"""

        for source in self.source_priority:
            # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒã‚§ãƒƒã‚¯
            if self._is_in_cooldown(source):
                continue

            attempt = DataFetchAttempt(
                symbol=symbol,
                source=source,
                start_time=datetime.now()
            )

            try:
                data = await self._fetch_from_source(source, symbol, period)

                attempt.end_time = datetime.now()
                attempt.response_time = (attempt.end_time - attempt.start_time).total_seconds()

                if data is not None and len(data) > 0:
                    attempt.success = True
                    attempt.data_points = len(data)
                    attempt.quality = self._evaluate_data_quality(data)

                    self._update_success_metrics(source, attempt)
                    self.attempt_history.append(attempt)

                    self.logger.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ {symbol} from {source.value}: {len(data)}ä»¶")
                    return data

            except Exception as e:
                attempt.end_time = datetime.now()
                attempt.error_message = str(e)
                attempt.response_time = (attempt.end_time - attempt.start_time).total_seconds()

                self._update_failure_metrics(source, attempt)
                self.attempt_history.append(attempt)

                self.logger.warning(f"ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•— {symbol} from {source.value}: {e}")

                # æ¬¡ã®ã‚½ãƒ¼ã‚¹ã‚’è©¦è¡Œ
                continue

        self.logger.error(f"å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {symbol}")
        return None

    async def _fetch_from_source(self, source: DataSource, symbol: str, period: str) -> Optional[pd.DataFrame]:
        """ç‰¹å®šã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        if source == DataSource.YAHOO_FINANCE:
            return await self._fetch_yahoo_finance(symbol, period)
        elif source == DataSource.YAHOO_FINANCE_ALT:
            return await self._fetch_yahoo_finance_alt(symbol, period)
        elif source == DataSource.STOOQ:
            return await self._fetch_stooq(symbol, period)
        elif source == DataSource.LOCAL_CACHE:
            return await self._fetch_local_cache(symbol, period)
        elif source == DataSource.FALLBACK:
            return await self._fetch_fallback(symbol, period)

        return None

    async def _fetch_yahoo_finance(self, symbol: str, period: str) -> Optional[pd.DataFrame]:
        """Yahoo Financeï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        try:
            from real_data_provider_v2 import real_data_provider
            return await real_data_provider.get_stock_data(symbol, period)
        except Exception as e:
            raise Exception(f"Yahoo Financeå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    async def _fetch_yahoo_finance_alt(self, symbol: str, period: str) -> Optional[pd.DataFrame]:
        """Yahoo Financeï¼ˆä»£æ›¿æ–¹å¼ï¼‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        try:
            import yfinance as yf

            # æ—¥æœ¬æ ªã®å ´åˆã¯.Tã‚’ä»˜åŠ 
            if symbol.isdigit():
                yahoo_symbol = f"{symbol}.T"
            else:
                yahoo_symbol = symbol

            ticker = yf.Ticker(yahoo_symbol)
            data = ticker.history(period=period)

            if data.empty:
                raise Exception("ãƒ‡ãƒ¼ã‚¿ãŒç©º")

            return data

        except Exception as e:
            raise Exception(f"Yahoo Finance Altå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    async def _fetch_stooq(self, symbol: str, period: str) -> Optional[pd.DataFrame]:
        """Stooqã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        try:
            import pandas_datareader as pdr

            # æœŸé–“å¤‰æ›
            end_date = datetime.now()
            if period == "1d":
                start_date = end_date - timedelta(days=1)
            elif period == "5d":
                start_date = end_date - timedelta(days=5)
            elif period == "1mo":
                start_date = end_date - timedelta(days=30)
            else:
                start_date = end_date - timedelta(days=5)

            # Stooqç”¨ã‚·ãƒ³ãƒœãƒ«å¤‰æ›
            stooq_symbol = f"{symbol}.JP"

            data = pdr.get_data_stooq(stooq_symbol, start_date, end_date)

            if data.empty:
                raise Exception("ãƒ‡ãƒ¼ã‚¿ãŒç©º")

            return data

        except Exception as e:
            raise Exception(f"Stooqå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

    async def _fetch_local_cache(self, symbol: str, period: str) -> Optional[pd.DataFrame]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""

        try:
            cache_path = Path(f"cache_data/{symbol}_{period}.csv")

            if cache_path.exists():
                data = pd.read_csv(cache_path, index_col=0, parse_dates=True)

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®é®®åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆ24æ™‚é–“ä»¥å†…ï¼‰
                cache_time = datetime.fromtimestamp(cache_path.stat().st_mtime)
                if datetime.now() - cache_time < timedelta(hours=24):
                    return data
                else:
                    self.logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå¤ã„: {symbol}")

            return None

        except Exception as e:
            raise Exception(f"ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    async def _fetch_fallback(self, symbol: str, period: str) -> Optional[pd.DataFrame]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰"""

        try:
            self.logger.warning(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: {symbol}")

            # éå»5æ—¥åˆ†ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            dates = pd.date_range(end=datetime.now(), periods=5, freq='D')

            # åŸºæº–ä¾¡æ ¼ï¼ˆ3000å††ï¼‰ã‹ã‚‰ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚¦ã‚©ãƒ¼ã‚¯
            base_price = 3000
            prices = []

            for i in range(len(dates)):
                if i == 0:
                    prices.append(base_price)
                else:
                    change = np.random.normal(0, 0.02)  # 2%ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
                    prices.append(prices[-1] * (1 + change))

            # OHLCV ãƒ‡ãƒ¼ã‚¿ä½œæˆ
            data = pd.DataFrame(index=dates)
            data['Close'] = prices
            data['Open'] = [p * np.random.uniform(0.99, 1.01) for p in prices]
            data['High'] = [max(o, c) * np.random.uniform(1.0, 1.02) for o, c in zip(data['Open'], data['Close'])]
            data['Low'] = [min(o, c) * np.random.uniform(0.98, 1.0) for o, c in zip(data['Open'], data['Close'])]
            data['Volume'] = [np.random.randint(100000, 1000000) for _ in range(len(dates))]

            return data

        except Exception as e:
            raise Exception(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

    def _is_in_cooldown(self, source: DataSource) -> bool:
        """ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ä¸­ã‹ãƒã‚§ãƒƒã‚¯"""

        if self.source_cooldowns[source] is None:
            return False

        cooldown_duration = self.source_config.get(source, {}).get('cooldown', 5)
        return (datetime.now() - self.source_cooldowns[source]).total_seconds() < cooldown_duration

    def _evaluate_data_quality(self, data: pd.DataFrame) -> DataQuality:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡"""

        if data is None or len(data) == 0:
            return DataQuality.CRITICAL

        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))

        # å¿…é ˆã‚«ãƒ©ãƒ ãƒã‚§ãƒƒã‚¯
        required_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
        missing_columns = [col for col in required_columns if col not in data.columns]

        # ä¾¡æ ¼ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        price_validity = True
        if 'Close' in data.columns:
            price_validity = not (data['Close'] <= 0).any()

        # å“è³ªåˆ¤å®š
        if missing_columns or not price_validity:
            return DataQuality.CRITICAL
        elif missing_ratio > 0.2:
            return DataQuality.POOR
        elif missing_ratio > 0.1:
            return DataQuality.FAIR
        elif missing_ratio > 0.01:
            return DataQuality.GOOD
        else:
            return DataQuality.EXCELLENT

    def _update_success_metrics(self, source: DataSource, attempt: DataFetchAttempt):
        """æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""

        self.failure_counts[source] = max(0, self.failure_counts[source] - 1)

        # æˆåŠŸç‡æ›´æ–°ï¼ˆç§»å‹•å¹³å‡ï¼‰
        self.success_rates[source] = self.success_rates[source] * 0.9 + 0.1

    def _update_failure_metrics(self, source: DataSource, attempt: DataFetchAttempt):
        """å¤±æ•—ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""

        self.failure_counts[source] += 1

        # é€£ç¶šå¤±æ•—æ™‚ã¯ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³è¨­å®š
        if self.failure_counts[source] >= 3:
            self.source_cooldowns[source] = datetime.now()
            self.logger.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³: {source.value}")

        # æˆåŠŸç‡æ›´æ–°ï¼ˆç§»å‹•å¹³å‡ï¼‰
        self.success_rates[source] = self.success_rates[source] * 0.9

class ContinuousDataProcessor:
    """é€£ç¶šãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.source_manager = DataSourceManager()

        # å‡¦ç†çŠ¶æ…‹
        self.is_running = False
        self.processing_intervals = {
            '1min': 60,
            '5min': 300,
            '1hour': 3600
        }

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.db_path = Path("stability_data/market_data_stability.db")
        self.db_path.parent.mkdir(exist_ok=True)
        self._init_database()

        # ç›£è¦–å¯¾è±¡éŠ˜æŸ„
        self.watched_symbols = ["7203", "8306", "4751"]

        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡
        self.data_buffer = {}

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.scheduler_thread = None

        self.logger.info("Continuous data processor initialized")

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ãƒ‡ãƒ¼ã‚¿å–å¾—å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS fetch_attempts (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        source TEXT NOT NULL,
                        start_time TEXT NOT NULL,
                        end_time TEXT,
                        success INTEGER DEFAULT 0,
                        data_points INTEGER DEFAULT 0,
                        response_time REAL DEFAULT 0.0,
                        quality TEXT,
                        error_message TEXT
                    )
                ''')

                # ãƒ‡ãƒ¼ã‚¿å“è³ªå±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS data_quality_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        source TEXT NOT NULL,
                        quality_score REAL NOT NULL,
                        completeness REAL NOT NULL,
                        consistency REAL NOT NULL,
                        freshness REAL NOT NULL
                    )
                ''')

                conn.commit()

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    def start_continuous_processing(self):
        """é€£ç¶šå‡¦ç†é–‹å§‹"""

        if self.is_running:
            return

        self.is_running = True
        self.scheduler_thread = threading.Thread(target=self._scheduler_loop, daemon=True)
        self.scheduler_thread.start()

        self.logger.info("é€£ç¶šãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")

    def stop_continuous_processing(self):
        """é€£ç¶šå‡¦ç†åœæ­¢"""

        self.is_running = False
        self.logger.info("é€£ç¶šãƒ‡ãƒ¼ã‚¿å‡¦ç†åœæ­¢")

    def _scheduler_loop(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ«ãƒ¼ãƒ—"""

        while self.is_running:
            try:
                # å„é–“éš”ã§ã®å‡¦ç†ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
                asyncio.create_task(self._process_symbols_batch(self.watched_symbols))

                # 5åˆ†é–“éš”ã§å®Ÿè¡Œ
                time.sleep(300)

            except Exception as e:
                self.logger.error(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(60)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯1åˆ†å¾…æ©Ÿ

    async def _process_symbols_batch(self, symbols: List[str]):
        """éŠ˜æŸ„ãƒãƒƒãƒå‡¦ç†"""

        tasks = []
        for symbol in symbols:
            task = asyncio.create_task(self._process_single_symbol(symbol))
            tasks.append(task)

        # ä¸¦è¡Œå‡¦ç†ã§å®Ÿè¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        success_count = sum(1 for r in results if not isinstance(r, Exception))
        self.logger.info(f"ãƒãƒƒãƒå‡¦ç†å®Œäº†: {success_count}/{len(symbols)} æˆåŠŸ")

    async def _process_single_symbol(self, symbol: str):
        """å˜ä¸€éŠ˜æŸ„å‡¦ç†"""

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            data = await self.source_manager.fetch_with_fallback(symbol, "5d")

            if data is not None:
                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜
                self.data_buffer[symbol] = {
                    'data': data,
                    'timestamp': datetime.now(),
                    'source': 'multiple_fallback'
                }

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
                await self._save_data_quality_record(symbol, data)

                self.logger.debug(f"éŠ˜æŸ„å‡¦ç†å®Œäº†: {symbol}")

        except Exception as e:
            self.logger.error(f"éŠ˜æŸ„å‡¦ç†ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")

    async def _save_data_quality_record(self, symbol: str, data: pd.DataFrame):
        """ãƒ‡ãƒ¼ã‚¿å“è³ªè¨˜éŒ²ä¿å­˜"""

        try:
            quality_metrics = self._calculate_quality_metrics(data)

            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                cursor.execute('''
                    INSERT INTO data_quality_history
                    (symbol, timestamp, source, quality_score, completeness, consistency, freshness)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    symbol,
                    datetime.now().isoformat(),
                    'fallback_system',
                    quality_metrics['quality_score'],
                    quality_metrics['completeness'],
                    quality_metrics['consistency'],
                    quality_metrics['freshness']
                ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"å“è³ªè¨˜éŒ²ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _calculate_quality_metrics(self, data: pd.DataFrame) -> Dict[str, float]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""

        try:
            # å®Œå…¨æ€§ï¼ˆæ¬ æå€¤ã®å°‘ãªã•ï¼‰
            completeness = 1.0 - (data.isnull().sum().sum() / (len(data) * len(data.columns)))

            # ä¸€è²«æ€§ï¼ˆä¾¡æ ¼é–¢ä¿‚ã®å¦¥å½“æ€§ï¼‰
            consistency = 1.0
            if 'High' in data.columns and 'Low' in data.columns and 'Close' in data.columns:
                invalid_prices = ((data['High'] < data['Low']) |
                                (data['Close'] > data['High']) |
                                (data['Close'] < data['Low'])).sum()
                consistency = 1.0 - (invalid_prices / len(data))

            # é®®åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ã®æ–°ã—ã•ï¼‰
            if len(data) > 0:
                latest_date = pd.to_datetime(data.index[-1])
                if isinstance(latest_date, pd.Timestamp):
                    freshness = max(0, 1.0 - (datetime.now() - latest_date.to_pydatetime()).days / 7.0)
                else:
                    freshness = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            else:
                freshness = 0.0

            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
            quality_score = (completeness * 0.4 + consistency * 0.4 + freshness * 0.2)

            return {
                'quality_score': quality_score,
                'completeness': completeness,
                'consistency': consistency,
                'freshness': freshness
            }

        except Exception as e:
            self.logger.error(f"å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'quality_score': 0.5,
                'completeness': 0.5,
                'consistency': 0.5,
                'freshness': 0.5
            }

    def get_data_reliability_report(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # éå»24æ™‚é–“ã®å–å¾—æˆåŠŸç‡
                yesterday = (datetime.now() - timedelta(days=1)).isoformat()
                cursor.execute('''
                    SELECT symbol, COUNT(*) as total, SUM(success) as successful
                    FROM fetch_attempts
                    WHERE start_time > ?
                    GROUP BY symbol
                ''', (yesterday,))

                success_rates = {}
                for symbol, total, successful in cursor.fetchall():
                    success_rates[symbol] = successful / total if total > 0 else 0

                # å¹³å‡å¿œç­”æ™‚é–“
                cursor.execute('''
                    SELECT symbol, AVG(response_time) as avg_response_time
                    FROM fetch_attempts
                    WHERE start_time > ? AND success = 1
                    GROUP BY symbol
                ''', (yesterday,))

                response_times = dict(cursor.fetchall())

                # ãƒ‡ãƒ¼ã‚¿å“è³ªçµ±è¨ˆ
                cursor.execute('''
                    SELECT symbol, AVG(quality_score) as avg_quality
                    FROM data_quality_history
                    WHERE timestamp > ?
                    GROUP BY symbol
                ''', (yesterday,))

                quality_scores = dict(cursor.fetchall())

                return {
                    'success_rates': success_rates,
                    'response_times': response_times,
                    'quality_scores': quality_scores,
                    'buffer_status': {
                        symbol: {
                            'records': len(info['data']) if 'data' in info else 0,
                            'last_update': info['timestamp'].isoformat() if 'timestamp' in info else 'never'
                        }
                        for symbol, info in self.data_buffer.items()
                    }
                }

        except Exception as e:
            self.logger.error(f"ä¿¡é ¼æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {}

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
market_data_stability_system = ContinuousDataProcessor()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def run_stability_system_test():
    """å®‰å®šæ€§ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("=== ğŸ“ˆ å¸‚å ´ãƒ‡ãƒ¼ã‚¿å®‰å®šæ€§ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")

    # ãƒ†ã‚¹ãƒˆå¯¾è±¡éŠ˜æŸ„
    test_symbols = ["7203", "8306", "4751"]

    print(f"\nğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")

    for symbol in test_symbols:
        print(f"\n--- {symbol} ---")

        start_time = time.time()
        data = await market_data_stability_system.source_manager.fetch_with_fallback(symbol, "5d")
        response_time = time.time() - start_time

        if data is not None:
            quality = market_data_stability_system.source_manager._evaluate_data_quality(data)
            print(f"  âœ… å–å¾—æˆåŠŸ: {len(data)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
            print(f"  â±ï¸  å¿œç­”æ™‚é–“: {response_time:.2f}ç§’")
            print(f"  ğŸ† ãƒ‡ãƒ¼ã‚¿å“è³ª: {quality.value}")
        else:
            print(f"  âŒ å–å¾—å¤±æ•—: {response_time:.2f}ç§’")

    # ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
    print(f"\nâš¡ ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ")

    batch_start = time.time()
    await market_data_stability_system._process_symbols_batch(test_symbols)
    batch_time = time.time() - batch_start

    print(f"  ãƒãƒƒãƒå‡¦ç†æ™‚é–“: {batch_time:.2f}ç§’")
    print(f"  ãƒãƒƒãƒ•ã‚¡çŠ¶æ³: {len(market_data_stability_system.data_buffer)}éŠ˜æŸ„")

    # ä¿¡é ¼æ€§ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§ãƒ¬ãƒãƒ¼ãƒˆ")
    report = market_data_stability_system.get_data_reliability_report()

    if 'success_rates' in report:
        print(f"  æˆåŠŸç‡:")
        for symbol, rate in report['success_rates'].items():
            print(f"    {symbol}: {rate:.1%}")

    if 'response_times' in report:
        print(f"  å¹³å‡å¿œç­”æ™‚é–“:")
        for symbol, time_sec in report['response_times'].items():
            print(f"    {symbol}: {time_sec:.2f}ç§’")

    if 'quality_scores' in report:
        print(f"  å“è³ªã‚¹ã‚³ã‚¢:")
        for symbol, score in report['quality_scores'].items():
            print(f"    {symbol}: {score:.1%}")

    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±è¨ˆ
    print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹çµ±è¨ˆ")
    source_manager = market_data_stability_system.source_manager

    print(f"  æˆåŠŸç‡:")
    for source, rate in source_manager.success_rates.items():
        print(f"    {source.value}: {rate:.1%}")

    print(f"  å¤±æ•—ã‚«ã‚¦ãƒ³ãƒˆ:")
    for source, count in source_manager.failure_counts.items():
        print(f"    {source.value}: {count}")

    print(f"\nâœ… å¸‚å ´ãƒ‡ãƒ¼ã‚¿å®‰å®šæ€§ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­")
    print(f"ç¶™ç¶šå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã¯è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ç¶šã‘ã¾ã™ã€‚")

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_stability_system_test())