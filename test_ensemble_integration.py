"""
Issue #850å¯¾å¿œï¼šã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆ
æ”¹è‰¯ã•ã‚ŒãŸã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã®å‹•ä½œç¢ºèª
"""

import asyncio
import logging
import tempfile
import numpy as np
import pandas as pd
from pathlib import Path
from unittest.mock import patch

from ml_prediction_models import MLPredictionModels, ModelType, PredictionTask

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

async def test_ensemble_prediction_integration():
    """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬çµ±åˆãƒ†ã‚¹ãƒˆ"""

    print("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬æ©Ÿèƒ½çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ ===")

    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ãƒ†ã‚¹ãƒˆ
    with tempfile.TemporaryDirectory() as temp_dir:
        models = MLPredictionModels()

        print(f"âœ“ MLPredictionModelsåˆæœŸåŒ–å®Œäº†")

        # ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        dates = pd.date_range('2024-01-01', periods=100, freq='D')
        sample_features = pd.DataFrame({
            'Close': np.random.normal(100, 5, 100),
            'Volume': np.random.randint(1000, 10000, 100),
            'RSI': np.random.uniform(20, 80, 100),
            'MACD': np.random.normal(0, 1, 100),
            'BB_upper': np.random.normal(105, 5, 100),
            'BB_lower': np.random.normal(95, 5, 100),
            'return_1d': np.random.normal(0, 0.02, 100),
            'return_5d': np.random.normal(0, 0.05, 100),
            'volatility_10d': np.random.uniform(0.01, 0.05, 100),
            'lag_1': np.random.normal(100, 5, 100),
            'lag_2': np.random.normal(100, 5, 100),
            'lag_3': np.random.normal(100, 5, 100)
        }, index=dates)

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ä½œæˆ
        targets = {
            PredictionTask.PRICE_DIRECTION: pd.Series(
                np.random.choice(['UP', 'DOWN'], 100), index=dates
            ),
            PredictionTask.PRICE_REGRESSION: sample_features['Close'] * 1.01
        }

        print("âœ“ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†")

        # 1. å‹•çš„é‡ã¿è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        print("\n--- å‹•çš„é‡ã¿è¨ˆç®—ãƒ†ã‚¹ãƒˆ ---")
        quality_scores = {'Random Forest': 0.8, 'XGBoost': 0.7, 'LightGBM': 0.75}
        confidences = {'Random Forest': 0.9, 'XGBoost': 0.8, 'LightGBM': 0.85}

        weights = await models._calculate_dynamic_weights(
            "TEST", PredictionTask.PRICE_DIRECTION, quality_scores, confidences
        )

        print(f"å‹•çš„é‡ã¿: {weights}")
        assert isinstance(weights, dict)
        assert len(weights) == 3

        # é‡ã¿ã®åˆè¨ˆãŒ1ã«è¿‘ã„ã“ã¨ã‚’ç¢ºèª
        total_weight = sum(weights.values())
        assert abs(total_weight - 1.0) < 0.01
        print(f"âœ“ é‡ã¿åˆè¨ˆ: {total_weight:.4f}")

        # 2. åˆ†é¡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ
        print("\n--- åˆ†é¡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ ---")
        predictions = {'Random Forest': 'UP', 'XGBoost': 'UP', 'LightGBM': 'DOWN'}
        confidences = {'Random Forest': 0.8, 'XGBoost': 0.7, 'LightGBM': 0.6}

        result = models._ensemble_classification(predictions, confidences, weights)

        print(f"åˆ†é¡çµæœ: {result}")
        assert 'prediction' in result
        assert 'confidence' in result
        assert 'consensus_strength' in result
        assert 'disagreement_score' in result

        assert result['prediction'] in ['UP', 'DOWN']
        assert 0 <= result['confidence'] <= 1
        assert 0 <= result['consensus_strength'] <= 1
        assert 0 <= result['disagreement_score'] <= 1
        print(f"âœ“ åˆ†é¡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ: {result['prediction']} (ä¿¡é ¼åº¦: {result['confidence']:.3f})")

        # 3. å›å¸°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ
        print("\n--- å›å¸°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ ---")
        reg_predictions = {'Random Forest': 100.5, 'XGBoost': 101.2, 'LightGBM': 99.8}
        reg_confidences = {'Random Forest': 0.8, 'XGBoost': 0.7, 'LightGBM': 0.75}

        reg_result = models._ensemble_regression(reg_predictions, reg_confidences, weights)

        print(f"å›å¸°çµæœ: {reg_result}")
        assert 'prediction' in reg_result
        assert isinstance(reg_result['prediction'], (int, float))
        assert 0 <= reg_result['confidence'] <= 1
        print(f"âœ“ å›å¸°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çµæœ: {reg_result['prediction']:.2f} (ä¿¡é ¼åº¦: {reg_result['confidence']:.3f})")

        # 4. äºˆæ¸¬å®‰å®šæ€§è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        print("\n--- äºˆæ¸¬å®‰å®šæ€§è¨ˆç®—ãƒ†ã‚¹ãƒˆ ---")

        # å®Œå…¨ä¸€è‡´ã®å ´åˆ
        identical_preds = {'model1': 'UP', 'model2': 'UP', 'model3': 'UP'}
        stability = models._calculate_prediction_stability(identical_preds)
        print(f"å®Œå…¨ä¸€è‡´å®‰å®šæ€§: {stability:.3f}")
        assert stability == 1.0

        # æ•°å€¤äºˆæ¸¬ã®å ´åˆ
        numeric_preds = {'model1': 100.0, 'model2': 100.5, 'model3': 99.5}
        stability = models._calculate_prediction_stability(numeric_preds)
        print(f"æ•°å€¤äºˆæ¸¬å®‰å®šæ€§: {stability:.3f}")
        assert 0 <= stability <= 1

        print("âœ“ äºˆæ¸¬å®‰å®šæ€§è¨ˆç®—ãƒ†ã‚¹ãƒˆå®Œäº†")

        # 5. ç‰¹å¾´é‡ä¸ç¢ºå®Ÿæ€§æ¨å®šãƒ†ã‚¹ãƒˆ
        print("\n--- ç‰¹å¾´é‡ä¸ç¢ºå®Ÿæ€§æ¨å®šãƒ†ã‚¹ãƒˆ ---")
        uncertainty = models._estimate_feature_uncertainty(sample_features)
        print(f"ç‰¹å¾´é‡ä¸ç¢ºå®Ÿæ€§: {uncertainty:.3f}")
        assert 0 <= uncertainty <= 1
        print("âœ“ ç‰¹å¾´é‡ä¸ç¢ºå®Ÿæ€§æ¨å®šãƒ†ã‚¹ãƒˆå®Œäº†")

        # 6. ä¿¡é ¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        print("\n--- ä¿¡é ¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ ---")

        # åˆ†é¡ä¿¡é ¼åº¦
        test_proba = np.array([0.7, 0.3])  # 2ã‚¯ãƒ©ã‚¹åˆ†é¡ã®ä¾‹
        quality_score = 0.8
        confidence = models._calculate_classification_confidence(test_proba, quality_score)
        print(f"åˆ†é¡ä¿¡é ¼åº¦: {confidence:.3f}")
        assert 0.1 <= confidence <= 0.95

        print("âœ“ ä¿¡é ¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆå®Œäº†")

        # 7. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒç¢ºèª
        print("\n--- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒç¢ºèª ---")
        import sqlite3

        with sqlite3.connect(models.db_path) as conn:
            cursor = conn.execute("""
                SELECT name FROM sqlite_master
                WHERE type='table' AND name LIKE '%model_%' OR name LIKE '%ensemble%'
            """)
            tables = [row[0] for row in cursor.fetchall()]

            print(f"ä½œæˆã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {tables}")

            expected_tables = [
                'model_metadata',
                'model_performance_history',
                'ensemble_prediction_history',
                'model_weight_history'
            ]

            for table in expected_tables:
                if table in tables:
                    print(f"âœ“ {table} ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆæ¸ˆã¿")
                else:
                    print(f"âš  {table} ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        print("=== ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬æ©Ÿèƒ½çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº† ===")

        # çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼
        print("\n=== ãƒ†ã‚¹ãƒˆã‚µãƒãƒªãƒ¼ ===")
        print("âœ“ å‹•çš„é‡ã¿è¨ˆç®—")
        print("âœ“ åˆ†é¡ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬")
        print("âœ“ å›å¸°ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬")
        print("âœ“ äºˆæ¸¬å®‰å®šæ€§è¨ˆç®—")
        print("âœ“ ç‰¹å¾´é‡ä¸ç¢ºå®Ÿæ€§æ¨å®š")
        print("âœ“ ä¿¡é ¼åº¦è¨ˆç®—")
        print("âœ“ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ")
        print("\nğŸ‰ å…¨ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")

        return True

async def test_quality_score_retrieval():
    """å“è³ªã‚¹ã‚³ã‚¢å–å¾—ãƒ†ã‚¹ãƒˆ"""
    print("\n=== å“è³ªã‚¹ã‚³ã‚¢å–å¾—ãƒ†ã‚¹ãƒˆ ===")

    with tempfile.TemporaryDirectory() as temp_dir:
        models = MLPredictionModels()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå“è³ªã‚¹ã‚³ã‚¢ç¢ºèª
        quality_score = await models._get_model_quality_score(
            "TEST", ModelType.RANDOM_FOREST, PredictionTask.PRICE_DIRECTION
        )

        print(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå“è³ªã‚¹ã‚³ã‚¢: {quality_score}")
        assert quality_score == 0.6  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        print("âœ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå“è³ªã‚¹ã‚³ã‚¢å–å¾—æˆåŠŸ")

        return True

if __name__ == "__main__":
    async def run_all_tests():
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        try:
            await test_ensemble_prediction_integration()
            await test_quality_score_retrieval()
            print("\nğŸš€ Issue #850ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬æ©Ÿèƒ½ã®çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
            return True
        except Exception as e:
            print(f"\nâŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return False

    success = asyncio.run(run_all_tests())
    if success:
        print("\nâœ… å…¨ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
    else:
        print("\nâŒ çµ±åˆãƒ†ã‚¹ãƒˆã§å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")