#!/usr/bin/env python3
"""
Issue #377 È´òÂ∫¶„Ç≠„É£„ÉÉ„Ç∑„É≥„Ç∞Êà¶Áï•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ

„Éû„Ç§„ÇØ„É≠„Çµ„Éº„Éì„ÇπÂØæÂøúÂàÜÊï£„Ç≠„É£„ÉÉ„Ç∑„É≥„Ç∞„Ç∑„Çπ„ÉÜ„É†„Å®
MLÈÅ©ÂøúÁöÑ„Ç≠„É£„ÉÉ„Ç∑„É•Êà¶Áï•„ÅÆÂåÖÊã¨ÁöÑ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ∏¨ÂÆö
"""

import asyncio
import json
import os
import statistics
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

try:
    import matplotlib.pyplot as plt
    import numpy as np
    import psutil

    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False
    print("‚ö†Ô∏è matplotlib/numpy not available. Plotting disabled.")

try:
    from src.day_trade.cache.adaptive_cache_strategies import (
        AdaptiveCacheOptimizer,
        CacheOptimizationContext,
    )
    from src.day_trade.cache.distributed_cache_system import DistributedCacheManager
    from src.day_trade.cache.microservices_cache_orchestrator import (
        CacheConsistencyLevel,
        CacheRegion,
        EventualConsistencyReplication,
        MasterSlaveReplication,
        MicroservicesCacheOrchestrator,
    )
except ImportError:
    # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁî®„É¢„ÉÉ„ÇØÂÆöÁæ©
    class CacheRegion:
        MARKET_DATA = "market_data"
        TRADING_POSITIONS = "positions"
        ANALYSIS_RESULTS = "analysis"

    class CacheConsistencyLevel:
        STRONG = "strong"
        EVENTUAL = "eventual"
        WEAK = "weak"

    class MicroservicesCacheOrchestrator:
        def __init__(self, *args, **kwargs):
            self.stats = {"cache_hits": 0, "cache_misses": 0}

        async def set(self, *args, **kwargs):
            await asyncio.sleep(0.001)  # 1ms „ÅÆ„Ç∑„Éü„É•„É¨„Éº„ÉàÈÅÖÂª∂
            return True

        async def get(self, *args, **kwargs):
            await asyncio.sleep(0.0005)  # 0.5ms „ÅÆ„Ç∑„Éü„É•„É¨„Éº„ÉàÈÅÖÂª∂
            return f"mock_value_{time.time()}"

        def get_stats(self):
            return self.stats


@dataclass
class BenchmarkResult:
    """„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„Éá„Éº„Çø„ÇØ„É©„Çπ"""

    name: str
    operations_per_second: float
    average_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    hit_rate: float
    memory_usage_mb: float
    cpu_usage_percent: float
    error_rate: float
    duration_seconds: float
    total_operations: int
    concurrent_threads: int

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "operations_per_second": self.operations_per_second,
            "average_latency_ms": self.average_latency_ms,
            "p95_latency_ms": self.p95_latency_ms,
            "p99_latency_ms": self.p99_latency_ms,
            "hit_rate": self.hit_rate,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "error_rate": self.error_rate,
            "duration_seconds": self.duration_seconds,
            "total_operations": self.total_operations,
            "concurrent_threads": self.concurrent_threads,
        }


class CachePerformanceBenchmark:
    """„Ç≠„É£„ÉÉ„Ç∑„É•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇØ„É©„Çπ"""

    def __init__(self):
        self.orchestrator = None
        self.optimizer = None
        self.results = []
        self.process = psutil.Process()

    async def setup(self):
        """„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁí∞Â¢É„Çª„ÉÉ„Éà„Ç¢„ÉÉ„Éó"""
        print("=== Issue #377 „Ç≠„É£„ÉÉ„Ç∑„É•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ ===")

        # „Ç™„Éº„Ç±„Çπ„Éà„É¨„Éº„Çø„ÉºÂàùÊúüÂåñ
        cache_nodes = ["redis://localhost:6379", "redis://localhost:6380"]
        replication_strategy = EventualConsistencyReplication(cache_nodes, min_success_ratio=0.5)

        self.orchestrator = MicroservicesCacheOrchestrator(
            cache_nodes=cache_nodes,
            replication_strategy=replication_strategy,
        )

        # ÈÅ©ÂøúÁöÑÊúÄÈÅ©ÂåñÂô®ÂàùÊúüÂåñ
        self.optimizer = AdaptiveCacheOptimizer()

        print("‚úÖ „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁí∞Â¢ÉÂàùÊúüÂåñÂÆå‰∫Ü")

    async def benchmark_basic_operations(self) -> BenchmarkResult:
        """Âü∫Êú¨Êìç‰Ωú„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"""
        print("\nüî• Âü∫Êú¨Êìç‰Ωú„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å‰∏≠...")

        total_operations = 10000
        concurrent_threads = 1

        # ÂàùÊúü„Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖãË®òÈå≤
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        initial_cpu = self.process.cpu_percent()

        latencies = []
        errors = 0
        hits = 0
        total_gets = 0

        start_time = time.perf_counter()

        # „Ç∑„Éº„Ç±„É≥„Ç∑„É£„É´Êìç‰Ωú„ÉÜ„Çπ„Éà
        for i in range(total_operations):
            key = f"basic_benchmark_key_{i % 1000}"  # 1000„Ç≠„Éº„ÅÆÂæ™Áí∞
            value = f"benchmark_value_{i}_{time.time()}"

            # SETÊìç‰Ωú
            set_start = time.perf_counter()
            try:
                await self.orchestrator.set(
                    key,
                    value,
                    CacheRegion.MARKET_DATA,
                    service_name="benchmark-service",
                )
                set_latency = (time.perf_counter() - set_start) * 1000
                latencies.append(set_latency)
            except Exception as e:
                errors += 1

            # GETÊìç‰Ωú
            get_start = time.perf_counter()
            try:
                result = await self.orchestrator.get(
                    key, CacheRegion.MARKET_DATA, service_name="benchmark-service"
                )
                get_latency = (time.perf_counter() - get_start) * 1000
                latencies.append(get_latency)
                total_gets += 1

                if result is not None:
                    hits += 1
            except Exception as e:
                errors += 1

            # ÈÄ≤ÊçóË°®Á§∫
            if i % 1000 == 0 and i > 0:
                elapsed = time.perf_counter() - start_time
                ops_per_sec = (i * 2) / elapsed  # SET + GET per iteration
                print(f"  ÈÄ≤Êçó: {i}/{total_operations} ({ops_per_sec:.0f} ops/sec)")

        end_time = time.perf_counter()
        duration = end_time - start_time

        # ÊúÄÁµÇ„Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖãË®òÈå≤
        final_memory = self.process.memory_info().rss / 1024 / 1024
        final_cpu = self.process.cpu_percent()

        # Áµ±Ë®àË®àÁÆó
        total_ops = total_operations * 2  # SET + GET
        ops_per_second = total_ops / duration
        avg_latency = statistics.mean(latencies) if latencies else 0
        p95_latency = (
            statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else avg_latency
        )
        p99_latency = (
            statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else avg_latency
        )
        hit_rate = hits / total_gets if total_gets > 0 else 0
        error_rate = errors / total_ops if total_ops > 0 else 0

        result = BenchmarkResult(
            name="Âü∫Êú¨Êìç‰Ωú",
            operations_per_second=ops_per_second,
            average_latency_ms=avg_latency,
            p95_latency_ms=p95_latency,
            p99_latency_ms=p99_latency,
            hit_rate=hit_rate,
            memory_usage_mb=final_memory - initial_memory,
            cpu_usage_percent=final_cpu,
            error_rate=error_rate,
            duration_seconds=duration,
            total_operations=total_ops,
            concurrent_threads=concurrent_threads,
        )

        self.results.append(result)

        print("  ‚úÖ Âü∫Êú¨Êìç‰Ωú„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆå‰∫Ü")
        print(f"     „Çπ„É´„Éº„Éó„ÉÉ„Éà: {ops_per_second:.0f} ops/sec")
        print(f"     Âπ≥ÂùáÈÅÖÂª∂: {avg_latency:.2f}ms")
        print(f"     P95ÈÅÖÂª∂: {p95_latency:.2f}ms")
        print(f"     „Éí„ÉÉ„ÉàÁéá: {hit_rate:.2%}")

        return result

    async def benchmark_concurrent_load(self) -> BenchmarkResult:
        """‰∏¶Ë°åË≤†Ëç∑„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"""
        print("\nüöÄ ‰∏¶Ë°åË≤†Ëç∑„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å‰∏≠...")

        total_operations = 50000
        concurrent_threads = 50
        operations_per_thread = total_operations // concurrent_threads

        # ÂàùÊúü„Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖãË®òÈå≤
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        initial_cpu = self.process.cpu_percent()

        async def worker_task(worker_id: int) -> Dict[str, Any]:
            """„ÉØ„Éº„Ç´„Éº„Çø„Çπ„ÇØ"""
            worker_latencies = []
            worker_errors = 0
            worker_hits = 0
            worker_total_gets = 0

            for i in range(operations_per_thread):
                key = f"concurrent_key_{worker_id}_{i % 100}"
                value = f"concurrent_value_{worker_id}_{i}_{time.time()}"

                # SETÊìç‰Ωú
                set_start = time.perf_counter()
                try:
                    await self.orchestrator.set(
                        key,
                        value,
                        CacheRegion.MARKET_DATA,
                        service_name=f"benchmark-service-{worker_id % 5}",
                    )
                    set_latency = (time.perf_counter() - set_start) * 1000
                    worker_latencies.append(set_latency)
                except Exception:
                    worker_errors += 1

                # GETÊìç‰Ωú
                get_start = time.perf_counter()
                try:
                    result = await self.orchestrator.get(
                        key,
                        CacheRegion.MARKET_DATA,
                        service_name=f"benchmark-service-{worker_id % 5}",
                    )
                    get_latency = (time.perf_counter() - get_start) * 1000
                    worker_latencies.append(get_latency)
                    worker_total_gets += 1

                    if result is not None:
                        worker_hits += 1
                except Exception:
                    worker_errors += 1

            return {
                "latencies": worker_latencies,
                "errors": worker_errors,
                "hits": worker_hits,
                "total_gets": worker_total_gets,
            }

        # ‰∏¶Ë°åÂÆüË°å
        start_time = time.perf_counter()

        tasks = [
            asyncio.create_task(worker_task(worker_id)) for worker_id in range(concurrent_threads)
        ]

        worker_results = await asyncio.gather(*tasks)

        end_time = time.perf_counter()
        duration = end_time - start_time

        # ÊúÄÁµÇ„Ç∑„Çπ„ÉÜ„É†Áä∂ÊÖãË®òÈå≤
        final_memory = self.process.memory_info().rss / 1024 / 1024
        final_cpu = self.process.cpu_percent()

        # Áµ±Ë®àÈõÜË®à
        all_latencies = []
        total_errors = 0
        total_hits = 0
        total_gets = 0

        for worker_result in worker_results:
            all_latencies.extend(worker_result["latencies"])
            total_errors += worker_result["errors"]
            total_hits += worker_result["hits"]
            total_gets += worker_result["total_gets"]

        # Áµ±Ë®àË®àÁÆó
        total_ops = total_operations * 2  # SET + GET
        ops_per_second = total_ops / duration
        avg_latency = statistics.mean(all_latencies) if all_latencies else 0
        p95_latency = (
            statistics.quantiles(all_latencies, n=20)[18]
            if len(all_latencies) >= 20
            else avg_latency
        )
        p99_latency = (
            statistics.quantiles(all_latencies, n=100)[98]
            if len(all_latencies) >= 100
            else avg_latency
        )
        hit_rate = total_hits / total_gets if total_gets > 0 else 0
        error_rate = total_errors / total_ops if total_ops > 0 else 0

        result = BenchmarkResult(
            name="‰∏¶Ë°åË≤†Ëç∑",
            operations_per_second=ops_per_second,
            average_latency_ms=avg_latency,
            p95_latency_ms=p95_latency,
            p99_latency_ms=p99_latency,
            hit_rate=hit_rate,
            memory_usage_mb=final_memory - initial_memory,
            cpu_usage_percent=final_cpu,
            error_rate=error_rate,
            duration_seconds=duration,
            total_operations=total_ops,
            concurrent_threads=concurrent_threads,
        )

        self.results.append(result)

        print("  ‚úÖ ‰∏¶Ë°åË≤†Ëç∑„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆå‰∫Ü")
        print(f"     ‰∏¶Ë°å„Çπ„É¨„ÉÉ„Éâ: {concurrent_threads}")
        print(f"     „Çπ„É´„Éº„Éó„ÉÉ„Éà: {ops_per_second:.0f} ops/sec")
        print(f"     Âπ≥ÂùáÈÅÖÂª∂: {avg_latency:.2f}ms")
        print(f"     P99ÈÅÖÂª∂: {p99_latency:.2f}ms")
        print(f"     „Éí„ÉÉ„ÉàÁéá: {hit_rate:.2%}")

        return result

    async def benchmark_memory_scalability(self) -> BenchmarkResult:
        """„É°„É¢„É™„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"""
        print("\nüìä „É°„É¢„É™„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å‰∏≠...")

        # ÊÆµÈöéÁöÑ„Å´„Éá„Éº„Çø„Çµ„Ç§„Ç∫„ÇíÂ¢óÂä†„Åï„Åõ„Å¶„É°„É¢„É™‰ΩøÁî®Èáè„ÇíÊ∏¨ÂÆö
        data_sizes = [1, 10, 100, 1000, 5000]  # KB
        entries_per_size = 1000

        memory_measurements = []
        latency_measurements = []

        for data_size_kb in data_sizes:
            print(f"  „Éá„Éº„Çø„Çµ„Ç§„Ç∫ {data_size_kb}KB „Åß„ÉÜ„Çπ„Éà‰∏≠...")

            # „ÉÜ„Çπ„Éà„Éá„Éº„ÇøÁîüÊàê
            test_data = "x" * (data_size_kb * 1024)

            # ÂàùÊúü„É°„É¢„É™Ê∏¨ÂÆö
            initial_memory = self.process.memory_info().rss / 1024 / 1024

            # „Éá„Éº„ÇøÊåøÂÖ•
            insertion_latencies = []
            start_time = time.perf_counter()

            for i in range(entries_per_size):
                key = f"memory_scale_{data_size_kb}kb_{i}"
                value = {
                    "id": i,
                    "data": test_data,
                    "metadata": f"size_{data_size_kb}kb",
                    "timestamp": time.time(),
                }

                insert_start = time.perf_counter()
                await self.orchestrator.set(
                    key,
                    value,
                    CacheRegion.ANALYSIS_RESULTS,
                    service_name="memory-benchmark-service",
                )
                insert_latency = (time.perf_counter() - insert_start) * 1000
                insertion_latencies.append(insert_latency)

            insertion_duration = time.perf_counter() - start_time

            # ÊúÄÁµÇ„É°„É¢„É™Ê∏¨ÂÆö
            final_memory = self.process.memory_info().rss / 1024 / 1024
            memory_increase = final_memory - initial_memory

            # Ë™≠„ÅøÂèñ„Çä„ÉÜ„Çπ„Éà
            retrieval_latencies = []
            retrieval_start = time.perf_counter()

            for i in range(min(entries_per_size, 100)):  # 100„Ç®„É≥„Éà„É™„ÅÆ„Çµ„É≥„Éó„É´
                key = f"memory_scale_{data_size_kb}kb_{i}"

                retrieve_start = time.perf_counter()
                result = await self.orchestrator.get(
                    key,
                    CacheRegion.ANALYSIS_RESULTS,
                    service_name="memory-benchmark-service",
                )
                retrieve_latency = (time.perf_counter() - retrieve_start) * 1000
                retrieval_latencies.append(retrieve_latency)

            retrieval_duration = time.perf_counter() - retrieval_start

            # Áµ±Ë®àË®àÁÆó
            avg_insertion_latency = statistics.mean(insertion_latencies)
            avg_retrieval_latency = statistics.mean(retrieval_latencies)
            memory_per_entry_mb = memory_increase / entries_per_size

            memory_measurements.append(
                {
                    "data_size_kb": data_size_kb,
                    "memory_increase_mb": memory_increase,
                    "memory_per_entry_mb": memory_per_entry_mb,
                    "insertion_latency_ms": avg_insertion_latency,
                    "retrieval_latency_ms": avg_retrieval_latency,
                    "entries": entries_per_size,
                }
            )

            print(f"    „É°„É¢„É™Â¢óÂä†: {memory_increase:.1f}MB")
            print(f"    „Ç®„É≥„Éà„É™ÂΩì„Åü„Çä„É°„É¢„É™: {memory_per_entry_mb*1024:.2f}KB")
            print(f"    ÊåøÂÖ•ÈÅÖÂª∂: {avg_insertion_latency:.2f}ms")
            print(f"    Ë™≠„ÅøÂèñ„ÇäÈÅÖÂª∂: {avg_retrieval_latency:.2f}ms")

        # ÊúÄÁµÇÁµêÊûúÁµ±Ë®à
        total_memory = sum(m["memory_increase_mb"] for m in memory_measurements)
        total_entries = sum(m["entries"] for m in memory_measurements)
        avg_memory_per_entry = total_memory / total_entries * 1024  # KB

        avg_insertion_latency = statistics.mean(
            [m["insertion_latency_ms"] for m in memory_measurements]
        )
        avg_retrieval_latency = statistics.mean(
            [m["retrieval_latency_ms"] for m in memory_measurements]
        )

        result = BenchmarkResult(
            name="„É°„É¢„É™„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£",
            operations_per_second=0,  # N/A for this benchmark
            average_latency_ms=(avg_insertion_latency + avg_retrieval_latency) / 2,
            p95_latency_ms=0,  # N/A
            p99_latency_ms=0,  # N/A
            hit_rate=1.0,  # All data should be available
            memory_usage_mb=total_memory,
            cpu_usage_percent=self.process.cpu_percent(),
            error_rate=0.0,  # Assuming no errors
            duration_seconds=0,  # N/A
            total_operations=total_entries * 2,  # SET + GET
            concurrent_threads=1,
        )

        self.results.append(result)

        print("  ‚úÖ „É°„É¢„É™„Çπ„Ç±„Éº„É©„Éì„É™„ÉÜ„Ç£„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆå‰∫Ü")
        print(f"     Á∑è„É°„É¢„É™‰ΩøÁî®Èáè: {total_memory:.1f}MB")
        print(f"     Âπ≥Âùá„É°„É¢„É™/„Ç®„É≥„Éà„É™: {avg_memory_per_entry:.2f}KB")

        return result

    async def benchmark_consistency_performance(self) -> BenchmarkResult:
        """Êï¥ÂêàÊÄß„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ"""
        print("\nüîí Êï¥ÂêàÊÄß„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å‰∏≠...")

        consistency_levels = [
            CacheConsistencyLevel.WEAK,
            CacheConsistencyLevel.EVENTUAL,
            CacheConsistencyLevel.STRONG,
        ]

        operations_per_level = 1000
        concurrent_writers = 10

        consistency_results = {}

        for consistency_level in consistency_levels:
            print(f"  Êï¥ÂêàÊÄß„É¨„Éô„É´: {consistency_level} „ÉÜ„Çπ„Éà‰∏≠...")

            # ÂàùÊúüÊôÇÈñìË®òÈå≤
            start_time = time.perf_counter()

            async def consistency_worker(worker_id: int) -> List[float]:
                """Êï¥ÂêàÊÄß„ÉÜ„Çπ„Éà„ÉØ„Éº„Ç´„Éº"""
                worker_latencies = []

                for i in range(operations_per_level // concurrent_writers):
                    key = f"consistency_{consistency_level}_{worker_id}_{i}"
                    value = f"value_{worker_id}_{i}_{time.time()}"

                    # Êõ∏„ÅçËæº„ÅøÊìç‰Ωú
                    write_start = time.perf_counter()
                    await self.orchestrator.set(
                        key,
                        value,
                        CacheRegion.TRADING_POSITIONS,
                        service_name="consistency-benchmark-service",
                        consistency_level=consistency_level,
                    )
                    write_latency = (time.perf_counter() - write_start) * 1000
                    worker_latencies.append(write_latency)

                return worker_latencies

            # ‰∏¶Ë°åÊõ∏„ÅçËæº„ÅøÂÆüË°å
            tasks = [
                asyncio.create_task(consistency_worker(worker_id))
                for worker_id in range(concurrent_writers)
            ]

            worker_results = await asyncio.gather(*tasks)

            end_time = time.perf_counter()
            duration = end_time - start_time

            # Áµ±Ë®àÈõÜË®à
            all_latencies = []
            for worker_latencies in worker_results:
                all_latencies.extend(worker_latencies)

            avg_latency = statistics.mean(all_latencies) if all_latencies else 0
            p95_latency = (
                statistics.quantiles(all_latencies, n=20)[18]
                if len(all_latencies) >= 20
                else avg_latency
            )
            ops_per_second = operations_per_level / duration

            consistency_results[consistency_level] = {
                "avg_latency_ms": avg_latency,
                "p95_latency_ms": p95_latency,
                "ops_per_second": ops_per_second,
                "duration": duration,
            }

            print(f"    Âπ≥ÂùáÈÅÖÂª∂: {avg_latency:.2f}ms")
            print(f"    P95ÈÅÖÂª∂: {p95_latency:.2f}ms")
            print(f"    „Çπ„É´„Éº„Éó„ÉÉ„Éà: {ops_per_second:.0f} ops/sec")

        # ÊúÄËâØ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÊï¥ÂêàÊÄß„É¨„Éô„É´„ÇíÁµêÊûú„Å®„Åó„Å¶‰ΩøÁî®
        best_performance = min(consistency_results.values(), key=lambda x: x["avg_latency_ms"])

        result = BenchmarkResult(
            name="Êï¥ÂêàÊÄß„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ",
            operations_per_second=best_performance["ops_per_second"],
            average_latency_ms=best_performance["avg_latency_ms"],
            p95_latency_ms=best_performance["p95_latency_ms"],
            p99_latency_ms=0,  # N/A
            hit_rate=1.0,  # All writes should succeed
            memory_usage_mb=0,  # N/A for this benchmark
            cpu_usage_percent=self.process.cpu_percent(),
            error_rate=0.0,  # Assuming no errors
            duration_seconds=best_performance["duration"],
            total_operations=operations_per_level,
            concurrent_threads=concurrent_writers,
        )

        self.results.append(result)

        print("  ‚úÖ Êï¥ÂêàÊÄß„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆå‰∫Ü")

        return result

    def generate_performance_report(self) -> Dict[str, Any]:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„É¨„Éù„Éº„ÉàÁîüÊàê"""
        print("\n" + "=" * 80)
        print("üìä Issue #377 „Ç≠„É£„ÉÉ„Ç∑„É•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ ÊúÄÁµÇ„É¨„Éù„Éº„Éà")
        print("=" * 80)

        if not self.results:
            print("‚ùå „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„Åå„ÅÇ„Çä„Åæ„Åõ„Çì")
            return {}

        # Ê¶ÇË¶ÅÁµ±Ë®à
        total_operations = sum(r.total_operations for r in self.results)
        avg_throughput = statistics.mean(
            [r.operations_per_second for r in self.results if r.operations_per_second > 0]
        )
        avg_latency = statistics.mean([r.average_latency_ms for r in self.results])
        total_memory = sum(r.memory_usage_mb for r in self.results)

        print("üìà ÂÖ®‰ΩìÁµ±Ë®à:")
        print(f"  Á∑èÊìç‰ΩúÊï∞: {total_operations:,}")
        print(f"  Âπ≥Âùá„Çπ„É´„Éº„Éó„ÉÉ„Éà: {avg_throughput:.0f} ops/sec")
        print(f"  Âπ≥ÂùáÈÅÖÂª∂: {avg_latency:.2f}ms")
        print(f"  Á∑è„É°„É¢„É™‰ΩøÁî®Èáè: {total_memory:.1f}MB")

        # Ë©≥Á¥∞ÁµêÊûú
        print("\nüìã Ë©≥Á¥∞„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú:")
        print("-" * 80)
        print(
            f"{'„Éô„É≥„ÉÅ„Éû„Éº„ÇØ':<20} {'„Çπ„É´„Éº„Éó„ÉÉ„Éà':<15} {'Âπ≥ÂùáÈÅÖÂª∂':<12} {'P95ÈÅÖÂª∂':<12} {'„Éí„ÉÉ„ÉàÁéá':<10}"
        )
        print("-" * 80)

        for result in self.results:
            throughput_str = (
                f"{result.operations_per_second:.0f} ops/s"
                if result.operations_per_second > 0
                else "N/A"
            )
            print(
                f"{result.name:<20} {throughput_str:<15} {result.average_latency_ms:.2f}ms{'':<4} {result.p95_latency_ms:.2f}ms{'':<4} {result.hit_rate:.1%}{'':<4}"
            )

        # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°
        print("\nüéØ „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπË©ï‰æ°:")

        # „Çπ„É´„Éº„Éó„ÉÉ„ÉàË©ï‰æ°
        if avg_throughput >= 10000:
            throughput_grade = "A (ÂÑ™ÁßÄ)"
        elif avg_throughput >= 5000:
            throughput_grade = "B (ËâØÂ•Ω)"
        elif avg_throughput >= 1000:
            throughput_grade = "C (Ê®ôÊ∫ñ)"
        else:
            throughput_grade = "D (ÊîπÂñÑÂøÖË¶Å)"

        # ÈÅÖÂª∂Ë©ï‰æ°
        if avg_latency <= 1.0:
            latency_grade = "A (ÂÑ™ÁßÄ)"
        elif avg_latency <= 5.0:
            latency_grade = "B (ËâØÂ•Ω)"
        elif avg_latency <= 20.0:
            latency_grade = "C (Ê®ôÊ∫ñ)"
        else:
            latency_grade = "D (ÊîπÂñÑÂøÖË¶Å)"

        # „É°„É¢„É™ÂäπÁéáË©ï‰æ°
        if total_operations > 0:
            memory_per_op = total_memory * 1024 / total_operations  # KB per operation
            if memory_per_op <= 1.0:
                memory_grade = "A (ÂÑ™ÁßÄ)"
            elif memory_per_op <= 10.0:
                memory_grade = "B (ËâØÂ•Ω)"
            elif memory_per_op <= 50.0:
                memory_grade = "C (Ê®ôÊ∫ñ)"
            else:
                memory_grade = "D (ÊîπÂñÑÂøÖË¶Å)"
        else:
            memory_grade = "N/A"

        print(f"  „Çπ„É´„Éº„Éó„ÉÉ„Éà: {throughput_grade}")
        print(f"  ÈÅÖÂª∂: {latency_grade}")
        print(f"  „É°„É¢„É™ÂäπÁéá: {memory_grade}")

        # Êé®Â•®‰∫ãÈ†Ö
        print("\nüí° Êé®Â•®‰∫ãÈ†Ö:")
        recommendations = []

        if avg_throughput < 5000:
            recommendations.append("„Éª‰∏¶Ë°åÂá¶ÁêÜ„Å®„Éê„ÉÉ„ÉÅ„É≥„Ç∞Êà¶Áï•„ÅÆÊúÄÈÅ©Âåñ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        if avg_latency > 10.0:
            recommendations.append("„Éª„Ç≠„É£„ÉÉ„Ç∑„É•„Ç¢„É´„Ç¥„É™„Ç∫„É†„Å®„Éá„Éº„ÇøÊßãÈÄ†„ÅÆÊúÄÈÅ©Âåñ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        if total_memory > 1000:  # 1GB‰ª•‰∏ä
            recommendations.append(
                "„Éª„É°„É¢„É™‰ΩøÁî®Èáè„ÅÆÊúÄÈÅ©Âåñ„Å®„Ç¨„Éô„Éº„Ç∏„Ç≥„É¨„ÇØ„Ç∑„Éß„É≥Ë™øÊï¥„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
            )

        if not recommendations:
            recommendations.append("„ÉªÁèæÂú®„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅØËâØÂ•Ω„Åß„Åô„ÄÇÁ∂ôÁ∂öÁöÑ„Å™Áõ£Ë¶ñ„ÇíÊé®Â•®„Åó„Åæ„Åô")

        for rec in recommendations:
            print(f"  {rec}")

        # JSONÂá∫ÂäõÁî®„Éá„Éº„Çø
        report_data = {
            "summary": {
                "total_operations": total_operations,
                "avg_throughput_ops_per_sec": avg_throughput,
                "avg_latency_ms": avg_latency,
                "total_memory_mb": total_memory,
                "throughput_grade": throughput_grade,
                "latency_grade": latency_grade,
                "memory_grade": memory_grade,
            },
            "benchmarks": [result.to_dict() for result in self.results],
            "recommendations": recommendations,
            "timestamp": time.time(),
        }

        print("=" * 80)
        return report_data

    def save_results(self, filename: str = None):
        """„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„Çí„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò"""
        if filename is None:
            timestamp = int(time.time())
            filename = f"cache_benchmark_results_{timestamp}.json"

        report_data = self.generate_performance_report()

        # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
        results_dir = "benchmark_results"
        os.makedirs(results_dir, exist_ok=True)

        filepath = os.path.join(results_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"üìÅ „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú„Çí‰øùÂ≠ò„Åó„Åæ„Åó„Åü: {filepath}")
        return filepath


async def run_comprehensive_cache_benchmark():
    """ÂåÖÊã¨ÁöÑ„Ç≠„É£„ÉÉ„Ç∑„É•„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å"""
    benchmark = CachePerformanceBenchmark()

    try:
        await benchmark.setup()

        # ÂêÑ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å
        await benchmark.benchmark_basic_operations()
        await benchmark.benchmark_concurrent_load()
        await benchmark.benchmark_memory_scalability()
        await benchmark.benchmark_consistency_performance()

        # ÊúÄÁµÇ„É¨„Éù„Éº„ÉàÁîüÊàê
        report_data = benchmark.generate_performance_report()

        # ÁµêÊûú‰øùÂ≠ò
        benchmark.save_results()

        print("\nüéâ Issue #377 „Ç≠„É£„ÉÉ„Ç∑„É•„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆå‰∫Ü!")
        return report_data

    except Exception as e:
        print(f"‚ùå „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å„Ç®„É©„Éº: {e}")
        return {"error": str(e)}


if __name__ == "__main__":
    # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å
    results = asyncio.run(run_comprehensive_cache_benchmark())

    if "error" not in results:
        print("\n‚úÖ ÂÖ®„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆå‰∫Ü")
    else:
        print(f"\n‚ùå „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂ§±Êïó: {results['error']}")
