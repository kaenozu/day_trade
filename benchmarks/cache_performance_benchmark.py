#!/usr/bin/env python3
"""
Issue #377 é«˜åº¦ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹å¯¾å¿œåˆ†æ•£ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã¨
MLé©å¿œçš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
"""

import asyncio
import json
import os
import statistics
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

try:
    import matplotlib.pyplot as plt
    import numpy as np
    import psutil

    HAS_PLOTTING = True
except ImportError:
    HAS_PLOTTING = False
    print("âš ï¸ matplotlib/numpy not available. Plotting disabled.")

try:
    from src.day_trade.cache.adaptive_cache_strategies import (
        AdaptiveCacheOptimizer,
        CacheOptimizationContext,
    )
    from src.day_trade.cache.distributed_cache_system import DistributedCacheManager
    from src.day_trade.cache.microservices_cache_orchestrator import (
        CacheConsistencyLevel,
        CacheRegion,
        EventualConsistencyReplication,
        MasterSlaveReplication,
        MicroservicesCacheOrchestrator,
    )
except ImportError:
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ãƒ¢ãƒƒã‚¯å®šç¾©
    class CacheRegion:
        MARKET_DATA = "market_data"
        TRADING_POSITIONS = "positions"
        ANALYSIS_RESULTS = "analysis"

    class CacheConsistencyLevel:
        STRONG = "strong"
        EVENTUAL = "eventual"
        WEAK = "weak"

    class MicroservicesCacheOrchestrator:
        def __init__(self, *args, **kwargs):
            self.stats = {"cache_hits": 0, "cache_misses": 0}

        async def set(self, *args, **kwargs):
            await asyncio.sleep(0.001)  # 1ms ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆé…å»¶
            return True

        async def get(self, *args, **kwargs):
            await asyncio.sleep(0.0005)  # 0.5ms ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆé…å»¶
            return f"mock_value_{time.time()}"

        def get_stats(self):
            return self.stats


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""

    name: str
    operations_per_second: float
    average_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    hit_rate: float
    memory_usage_mb: float
    cpu_usage_percent: float
    error_rate: float
    duration_seconds: float
    total_operations: int
    concurrent_threads: int

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "operations_per_second": self.operations_per_second,
            "average_latency_ms": self.average_latency_ms,
            "p95_latency_ms": self.p95_latency_ms,
            "p99_latency_ms": self.p99_latency_ms,
            "hit_rate": self.hit_rate,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "error_rate": self.error_rate,
            "duration_seconds": self.duration_seconds,
            "total_operations": self.total_operations,
            "concurrent_threads": self.concurrent_threads,
        }


class CachePerformanceBenchmark:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.orchestrator = None
        self.optimizer = None
        self.results = []
        self.process = psutil.Process()

    async def setup(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("=== Issue #377 ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")

        # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼åˆæœŸåŒ–
        cache_nodes = ["redis://localhost:6379", "redis://localhost:6380"]
        replication_strategy = EventualConsistencyReplication(cache_nodes, min_success_ratio=0.5)

        self.orchestrator = MicroservicesCacheOrchestrator(
            cache_nodes=cache_nodes,
            replication_strategy=replication_strategy,
        )

        # é©å¿œçš„æœ€é©åŒ–å™¨åˆæœŸåŒ–
        self.optimizer = AdaptiveCacheOptimizer()

        print("âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒåˆæœŸåŒ–å®Œäº†")

    async def benchmark_basic_operations(self) -> BenchmarkResult:
        """åŸºæœ¬æ“ä½œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ”¥ åŸºæœ¬æ“ä½œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")

        total_operations = 10000
        concurrent_threads = 1

        # åˆæœŸã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        initial_cpu = self.process.cpu_percent()

        latencies = []
        errors = 0
        hits = 0
        total_gets = 0

        start_time = time.perf_counter()

        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«æ“ä½œãƒ†ã‚¹ãƒˆ
        for i in range(total_operations):
            key = f"basic_benchmark_key_{i % 1000}"  # 1000ã‚­ãƒ¼ã®å¾ªç’°
            value = f"benchmark_value_{i}_{time.time()}"

            # SETæ“ä½œ
            set_start = time.perf_counter()
            try:
                await self.orchestrator.set(
                    key,
                    value,
                    CacheRegion.MARKET_DATA,
                    service_name="benchmark-service",
                )
                set_latency = (time.perf_counter() - set_start) * 1000
                latencies.append(set_latency)
            except Exception as e:
                errors += 1

            # GETæ“ä½œ
            get_start = time.perf_counter()
            try:
                result = await self.orchestrator.get(
                    key, CacheRegion.MARKET_DATA, service_name="benchmark-service"
                )
                get_latency = (time.perf_counter() - get_start) * 1000
                latencies.append(get_latency)
                total_gets += 1

                if result is not None:
                    hits += 1
            except Exception as e:
                errors += 1

            # é€²æ—è¡¨ç¤º
            if i % 1000 == 0 and i > 0:
                elapsed = time.perf_counter() - start_time
                ops_per_sec = (i * 2) / elapsed  # SET + GET per iteration
                print(f"  é€²æ—: {i}/{total_operations} ({ops_per_sec:.0f} ops/sec)")

        end_time = time.perf_counter()
        duration = end_time - start_time

        # æœ€çµ‚ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
        final_memory = self.process.memory_info().rss / 1024 / 1024
        final_cpu = self.process.cpu_percent()

        # çµ±è¨ˆè¨ˆç®—
        total_ops = total_operations * 2  # SET + GET
        ops_per_second = total_ops / duration
        avg_latency = statistics.mean(latencies) if latencies else 0
        p95_latency = (
            statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else avg_latency
        )
        p99_latency = (
            statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else avg_latency
        )
        hit_rate = hits / total_gets if total_gets > 0 else 0
        error_rate = errors / total_ops if total_ops > 0 else 0

        result = BenchmarkResult(
            name="åŸºæœ¬æ“ä½œ",
            operations_per_second=ops_per_second,
            average_latency_ms=avg_latency,
            p95_latency_ms=p95_latency,
            p99_latency_ms=p99_latency,
            hit_rate=hit_rate,
            memory_usage_mb=final_memory - initial_memory,
            cpu_usage_percent=final_cpu,
            error_rate=error_rate,
            duration_seconds=duration,
            total_operations=total_ops,
            concurrent_threads=concurrent_threads,
        )

        self.results.append(result)

        print("  âœ… åŸºæœ¬æ“ä½œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
        print(f"     ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {ops_per_second:.0f} ops/sec")
        print(f"     å¹³å‡é…å»¶: {avg_latency:.2f}ms")
        print(f"     P95é…å»¶: {p95_latency:.2f}ms")
        print(f"     ãƒ’ãƒƒãƒˆç‡: {hit_rate:.2%}")

        return result

    async def benchmark_concurrent_load(self) -> BenchmarkResult:
        """ä¸¦è¡Œè² è·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸš€ ä¸¦è¡Œè² è·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")

        total_operations = 50000
        concurrent_threads = 50
        operations_per_thread = total_operations // concurrent_threads

        # åˆæœŸã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
        initial_memory = self.process.memory_info().rss / 1024 / 1024
        initial_cpu = self.process.cpu_percent()

        async def worker_task(worker_id: int) -> Dict[str, Any]:
            """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯"""
            worker_latencies = []
            worker_errors = 0
            worker_hits = 0
            worker_total_gets = 0

            for i in range(operations_per_thread):
                key = f"concurrent_key_{worker_id}_{i % 100}"
                value = f"concurrent_value_{worker_id}_{i}_{time.time()}"

                # SETæ“ä½œ
                set_start = time.perf_counter()
                try:
                    await self.orchestrator.set(
                        key,
                        value,
                        CacheRegion.MARKET_DATA,
                        service_name=f"benchmark-service-{worker_id % 5}",
                    )
                    set_latency = (time.perf_counter() - set_start) * 1000
                    worker_latencies.append(set_latency)
                except Exception:
                    worker_errors += 1

                # GETæ“ä½œ
                get_start = time.perf_counter()
                try:
                    result = await self.orchestrator.get(
                        key,
                        CacheRegion.MARKET_DATA,
                        service_name=f"benchmark-service-{worker_id % 5}",
                    )
                    get_latency = (time.perf_counter() - get_start) * 1000
                    worker_latencies.append(get_latency)
                    worker_total_gets += 1

                    if result is not None:
                        worker_hits += 1
                except Exception:
                    worker_errors += 1

            return {
                "latencies": worker_latencies,
                "errors": worker_errors,
                "hits": worker_hits,
                "total_gets": worker_total_gets,
            }

        # ä¸¦è¡Œå®Ÿè¡Œ
        start_time = time.perf_counter()

        tasks = [
            asyncio.create_task(worker_task(worker_id)) for worker_id in range(concurrent_threads)
        ]

        worker_results = await asyncio.gather(*tasks)

        end_time = time.perf_counter()
        duration = end_time - start_time

        # æœ€çµ‚ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
        final_memory = self.process.memory_info().rss / 1024 / 1024
        final_cpu = self.process.cpu_percent()

        # çµ±è¨ˆé›†è¨ˆ
        all_latencies = []
        total_errors = 0
        total_hits = 0
        total_gets = 0

        for worker_result in worker_results:
            all_latencies.extend(worker_result["latencies"])
            total_errors += worker_result["errors"]
            total_hits += worker_result["hits"]
            total_gets += worker_result["total_gets"]

        # çµ±è¨ˆè¨ˆç®—
        total_ops = total_operations * 2  # SET + GET
        ops_per_second = total_ops / duration
        avg_latency = statistics.mean(all_latencies) if all_latencies else 0
        p95_latency = (
            statistics.quantiles(all_latencies, n=20)[18]
            if len(all_latencies) >= 20
            else avg_latency
        )
        p99_latency = (
            statistics.quantiles(all_latencies, n=100)[98]
            if len(all_latencies) >= 100
            else avg_latency
        )
        hit_rate = total_hits / total_gets if total_gets > 0 else 0
        error_rate = total_errors / total_ops if total_ops > 0 else 0

        result = BenchmarkResult(
            name="ä¸¦è¡Œè² è·",
            operations_per_second=ops_per_second,
            average_latency_ms=avg_latency,
            p95_latency_ms=p95_latency,
            p99_latency_ms=p99_latency,
            hit_rate=hit_rate,
            memory_usage_mb=final_memory - initial_memory,
            cpu_usage_percent=final_cpu,
            error_rate=error_rate,
            duration_seconds=duration,
            total_operations=total_ops,
            concurrent_threads=concurrent_threads,
        )

        self.results.append(result)

        print("  âœ… ä¸¦è¡Œè² è·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
        print(f"     ä¸¦è¡Œã‚¹ãƒ¬ãƒƒãƒ‰: {concurrent_threads}")
        print(f"     ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {ops_per_second:.0f} ops/sec")
        print(f"     å¹³å‡é…å»¶: {avg_latency:.2f}ms")
        print(f"     P99é…å»¶: {p99_latency:.2f}ms")
        print(f"     ãƒ’ãƒƒãƒˆç‡: {hit_rate:.2%}")

        return result

    async def benchmark_memory_scalability(self) -> BenchmarkResult:
        """ãƒ¡ãƒ¢ãƒªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ“Š ãƒ¡ãƒ¢ãƒªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")

        # æ®µéšçš„ã«ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’å¢—åŠ ã•ã›ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®š
        data_sizes = [1, 10, 100, 1000, 5000]  # KB
        entries_per_size = 1000

        memory_measurements = []
        latency_measurements = []

        for data_size_kb in data_sizes:
            print(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º {data_size_kb}KB ã§ãƒ†ã‚¹ãƒˆä¸­...")

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            test_data = "x" * (data_size_kb * 1024)

            # åˆæœŸãƒ¡ãƒ¢ãƒªæ¸¬å®š
            initial_memory = self.process.memory_info().rss / 1024 / 1024

            # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
            insertion_latencies = []
            start_time = time.perf_counter()

            for i in range(entries_per_size):
                key = f"memory_scale_{data_size_kb}kb_{i}"
                value = {
                    "id": i,
                    "data": test_data,
                    "metadata": f"size_{data_size_kb}kb",
                    "timestamp": time.time(),
                }

                insert_start = time.perf_counter()
                await self.orchestrator.set(
                    key,
                    value,
                    CacheRegion.ANALYSIS_RESULTS,
                    service_name="memory-benchmark-service",
                )
                insert_latency = (time.perf_counter() - insert_start) * 1000
                insertion_latencies.append(insert_latency)

            insertion_duration = time.perf_counter() - start_time

            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªæ¸¬å®š
            final_memory = self.process.memory_info().rss / 1024 / 1024
            memory_increase = final_memory - initial_memory

            # èª­ã¿å–ã‚Šãƒ†ã‚¹ãƒˆ
            retrieval_latencies = []
            retrieval_start = time.perf_counter()

            for i in range(min(entries_per_size, 100)):  # 100ã‚¨ãƒ³ãƒˆãƒªã®ã‚µãƒ³ãƒ—ãƒ«
                key = f"memory_scale_{data_size_kb}kb_{i}"

                retrieve_start = time.perf_counter()
                result = await self.orchestrator.get(
                    key,
                    CacheRegion.ANALYSIS_RESULTS,
                    service_name="memory-benchmark-service",
                )
                retrieve_latency = (time.perf_counter() - retrieve_start) * 1000
                retrieval_latencies.append(retrieve_latency)

            retrieval_duration = time.perf_counter() - retrieval_start

            # çµ±è¨ˆè¨ˆç®—
            avg_insertion_latency = statistics.mean(insertion_latencies)
            avg_retrieval_latency = statistics.mean(retrieval_latencies)
            memory_per_entry_mb = memory_increase / entries_per_size

            memory_measurements.append(
                {
                    "data_size_kb": data_size_kb,
                    "memory_increase_mb": memory_increase,
                    "memory_per_entry_mb": memory_per_entry_mb,
                    "insertion_latency_ms": avg_insertion_latency,
                    "retrieval_latency_ms": avg_retrieval_latency,
                    "entries": entries_per_size,
                }
            )

            print(f"    ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_increase:.1f}MB")
            print(f"    ã‚¨ãƒ³ãƒˆãƒªå½“ãŸã‚Šãƒ¡ãƒ¢ãƒª: {memory_per_entry_mb*1024:.2f}KB")
            print(f"    æŒ¿å…¥é…å»¶: {avg_insertion_latency:.2f}ms")
            print(f"    èª­ã¿å–ã‚Šé…å»¶: {avg_retrieval_latency:.2f}ms")

        # æœ€çµ‚çµæœçµ±è¨ˆ
        total_memory = sum(m["memory_increase_mb"] for m in memory_measurements)
        total_entries = sum(m["entries"] for m in memory_measurements)
        avg_memory_per_entry = total_memory / total_entries * 1024  # KB

        avg_insertion_latency = statistics.mean(
            [m["insertion_latency_ms"] for m in memory_measurements]
        )
        avg_retrieval_latency = statistics.mean(
            [m["retrieval_latency_ms"] for m in memory_measurements]
        )

        result = BenchmarkResult(
            name="ãƒ¡ãƒ¢ãƒªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£",
            operations_per_second=0,  # N/A for this benchmark
            average_latency_ms=(avg_insertion_latency + avg_retrieval_latency) / 2,
            p95_latency_ms=0,  # N/A
            p99_latency_ms=0,  # N/A
            hit_rate=1.0,  # All data should be available
            memory_usage_mb=total_memory,
            cpu_usage_percent=self.process.cpu_percent(),
            error_rate=0.0,  # Assuming no errors
            duration_seconds=0,  # N/A
            total_operations=total_entries * 2,  # SET + GET
            concurrent_threads=1,
        )

        self.results.append(result)

        print("  âœ… ãƒ¡ãƒ¢ãƒªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
        print(f"     ç·ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {total_memory:.1f}MB")
        print(f"     å¹³å‡ãƒ¡ãƒ¢ãƒª/ã‚¨ãƒ³ãƒˆãƒª: {avg_memory_per_entry:.2f}KB")

        return result

    async def benchmark_consistency_performance(self) -> BenchmarkResult:
        """æ•´åˆæ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸ”’ æ•´åˆæ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")

        consistency_levels = [
            CacheConsistencyLevel.WEAK,
            CacheConsistencyLevel.EVENTUAL,
            CacheConsistencyLevel.STRONG,
        ]

        operations_per_level = 1000
        concurrent_writers = 10

        consistency_results = {}

        for consistency_level in consistency_levels:
            print(f"  æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«: {consistency_level} ãƒ†ã‚¹ãƒˆä¸­...")

            # åˆæœŸæ™‚é–“è¨˜éŒ²
            start_time = time.perf_counter()

            async def consistency_worker(worker_id: int) -> List[float]:
                """æ•´åˆæ€§ãƒ†ã‚¹ãƒˆãƒ¯ãƒ¼ã‚«ãƒ¼"""
                worker_latencies = []

                for i in range(operations_per_level // concurrent_writers):
                    key = f"consistency_{consistency_level}_{worker_id}_{i}"
                    value = f"value_{worker_id}_{i}_{time.time()}"

                    # æ›¸ãè¾¼ã¿æ“ä½œ
                    write_start = time.perf_counter()
                    await self.orchestrator.set(
                        key,
                        value,
                        CacheRegion.TRADING_POSITIONS,
                        service_name="consistency-benchmark-service",
                        consistency_level=consistency_level,
                    )
                    write_latency = (time.perf_counter() - write_start) * 1000
                    worker_latencies.append(write_latency)

                return worker_latencies

            # ä¸¦è¡Œæ›¸ãè¾¼ã¿å®Ÿè¡Œ
            tasks = [
                asyncio.create_task(consistency_worker(worker_id))
                for worker_id in range(concurrent_writers)
            ]

            worker_results = await asyncio.gather(*tasks)

            end_time = time.perf_counter()
            duration = end_time - start_time

            # çµ±è¨ˆé›†è¨ˆ
            all_latencies = []
            for worker_latencies in worker_results:
                all_latencies.extend(worker_latencies)

            avg_latency = statistics.mean(all_latencies) if all_latencies else 0
            p95_latency = (
                statistics.quantiles(all_latencies, n=20)[18]
                if len(all_latencies) >= 20
                else avg_latency
            )
            ops_per_second = operations_per_level / duration

            consistency_results[consistency_level] = {
                "avg_latency_ms": avg_latency,
                "p95_latency_ms": p95_latency,
                "ops_per_second": ops_per_second,
                "duration": duration,
            }

            print(f"    å¹³å‡é…å»¶: {avg_latency:.2f}ms")
            print(f"    P95é…å»¶: {p95_latency:.2f}ms")
            print(f"    ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {ops_per_second:.0f} ops/sec")

        # æœ€è‰¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ã‚’çµæœã¨ã—ã¦ä½¿ç”¨
        best_performance = min(consistency_results.values(), key=lambda x: x["avg_latency_ms"])

        result = BenchmarkResult(
            name="æ•´åˆæ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹",
            operations_per_second=best_performance["ops_per_second"],
            average_latency_ms=best_performance["avg_latency_ms"],
            p95_latency_ms=best_performance["p95_latency_ms"],
            p99_latency_ms=0,  # N/A
            hit_rate=1.0,  # All writes should succeed
            memory_usage_mb=0,  # N/A for this benchmark
            cpu_usage_percent=self.process.cpu_percent(),
            error_rate=0.0,  # Assuming no errors
            duration_seconds=best_performance["duration"],
            total_operations=operations_per_level,
            concurrent_threads=concurrent_writers,
        )

        self.results.append(result)

        print("  âœ… æ•´åˆæ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")

        return result

    def generate_performance_report(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š Issue #377 ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)

        if not self.results:
            print("âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return {}

        # æ¦‚è¦çµ±è¨ˆ
        total_operations = sum(r.total_operations for r in self.results)
        avg_throughput = statistics.mean(
            [r.operations_per_second for r in self.results if r.operations_per_second > 0]
        )
        avg_latency = statistics.mean([r.average_latency_ms for r in self.results])
        total_memory = sum(r.memory_usage_mb for r in self.results)

        print("ğŸ“ˆ å…¨ä½“çµ±è¨ˆ:")
        print(f"  ç·æ“ä½œæ•°: {total_operations:,}")
        print(f"  å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {avg_throughput:.0f} ops/sec")
        print(f"  å¹³å‡é…å»¶: {avg_latency:.2f}ms")
        print(f"  ç·ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {total_memory:.1f}MB")

        # è©³ç´°çµæœ
        print("\nğŸ“‹ è©³ç´°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
        print("-" * 80)
        print(
            f"{'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯':<20} {'ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ':<15} {'å¹³å‡é…å»¶':<12} {'P95é…å»¶':<12} {'ãƒ’ãƒƒãƒˆç‡':<10}"
        )
        print("-" * 80)

        for result in self.results:
            throughput_str = (
                f"{result.operations_per_second:.0f} ops/s"
                if result.operations_per_second > 0
                else "N/A"
            )
            print(
                f"{result.name:<20} {throughput_str:<15} {result.average_latency_ms:.2f}ms{'':<4} {result.p95_latency_ms:.2f}ms{'':<4} {result.hit_rate:.1%}{'':<4}"
            )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        print("\nğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡:")

        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè©•ä¾¡
        if avg_throughput >= 10000:
            throughput_grade = "A (å„ªç§€)"
        elif avg_throughput >= 5000:
            throughput_grade = "B (è‰¯å¥½)"
        elif avg_throughput >= 1000:
            throughput_grade = "C (æ¨™æº–)"
        else:
            throughput_grade = "D (æ”¹å–„å¿…è¦)"

        # é…å»¶è©•ä¾¡
        if avg_latency <= 1.0:
            latency_grade = "A (å„ªç§€)"
        elif avg_latency <= 5.0:
            latency_grade = "B (è‰¯å¥½)"
        elif avg_latency <= 20.0:
            latency_grade = "C (æ¨™æº–)"
        else:
            latency_grade = "D (æ”¹å–„å¿…è¦)"

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡è©•ä¾¡
        if total_operations > 0:
            memory_per_op = total_memory * 1024 / total_operations  # KB per operation
            if memory_per_op <= 1.0:
                memory_grade = "A (å„ªç§€)"
            elif memory_per_op <= 10.0:
                memory_grade = "B (è‰¯å¥½)"
            elif memory_per_op <= 50.0:
                memory_grade = "C (æ¨™æº–)"
            else:
                memory_grade = "D (æ”¹å–„å¿…è¦)"
        else:
            memory_grade = "N/A"

        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput_grade}")
        print(f"  é…å»¶: {latency_grade}")
        print(f"  ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: {memory_grade}")

        # æ¨å¥¨äº‹é …
        print("\nğŸ’¡ æ¨å¥¨äº‹é …:")
        recommendations = []

        if avg_throughput < 5000:
            recommendations.append("ãƒ»ä¸¦è¡Œå‡¦ç†ã¨ãƒãƒƒãƒãƒ³ã‚°æˆ¦ç•¥ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if avg_latency > 10.0:
            recommendations.append("ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if total_memory > 1000:  # 1GBä»¥ä¸Š
            recommendations.append(
                "ãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ã¨ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            )

        if not recommendations:
            recommendations.append("ãƒ»ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯è‰¯å¥½ã§ã™ã€‚ç¶™ç¶šçš„ãªç›£è¦–ã‚’æ¨å¥¨ã—ã¾ã™")

        for rec in recommendations:
            print(f"  {rec}")

        # JSONå‡ºåŠ›ç”¨ãƒ‡ãƒ¼ã‚¿
        report_data = {
            "summary": {
                "total_operations": total_operations,
                "avg_throughput_ops_per_sec": avg_throughput,
                "avg_latency_ms": avg_latency,
                "total_memory_mb": total_memory,
                "throughput_grade": throughput_grade,
                "latency_grade": latency_grade,
                "memory_grade": memory_grade,
            },
            "benchmarks": [result.to_dict() for result in self.results],
            "recommendations": recommendations,
            "timestamp": time.time(),
        }

        print("=" * 80)
        return report_data

    def save_results(self, filename: str = None):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if filename is None:
            timestamp = int(time.time())
            filename = f"cache_benchmark_results_{timestamp}.json"

        report_data = self.generate_performance_report()

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        results_dir = "benchmark_results"
        os.makedirs(results_dir, exist_ok=True)

        filepath = os.path.join(results_dir, filename)

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filepath}")
        return filepath


async def run_comprehensive_cache_benchmark():
    """åŒ…æ‹¬çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    benchmark = CachePerformanceBenchmark()

    try:
        await benchmark.setup()

        # å„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        await benchmark.benchmark_basic_operations()
        await benchmark.benchmark_concurrent_load()
        await benchmark.benchmark_memory_scalability()
        await benchmark.benchmark_consistency_performance()

        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_data = benchmark.generate_performance_report()

        # çµæœä¿å­˜
        benchmark.save_results()

        print("\nğŸ‰ Issue #377 ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")
        return report_data

    except Exception as e:
        print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return {"error": str(e)}


if __name__ == "__main__":
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    results = asyncio.run(run_comprehensive_cache_benchmark())

    if "error" not in results:
        print("\nâœ… å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")
    else:
        print(f"\nâŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¤±æ•—: {results['error']}")
