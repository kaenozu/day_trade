#!/usr/bin/env python3
"""
Final Benchmark - Issue #462å¯¾å¿œ

æœ€çµ‚çš„ãª95%ç²¾åº¦é”æˆãƒ†ã‚¹ãƒˆ
"""

import numpy as np
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
from pathlib import Path
import sys
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


def generate_high_quality_data(n_samples=1000):
    """é«˜å“è³ªãªåˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    np.random.seed(42)

    n_features = 20

    # ã‚ˆã‚Šè¤‡é›‘ã§ç¾å®Ÿçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
    X = np.random.randn(n_samples, n_features)

    # çœŸã®é–¢æ•°é–¢ä¿‚ï¼ˆè¤‡é›‘ãªéç·šå½¢ï¼‰
    y = (
        2.0 * X[:, 0] * X[:, 1] +           # äº¤äº’ä½œç”¨
        1.5 * np.sin(X[:, 2]) * X[:, 3] +  # éç·šå½¢
        0.8 * X[:, 4] ** 2 +               # äºŒæ¬¡é …
        0.6 * np.tanh(X[:, 5]) * X[:, 6] + # è¤‡åˆéç·šå½¢
        0.4 * np.sqrt(np.abs(X[:, 7])) +   # å¹³æ–¹æ ¹
        0.3 * X[:, 8] * X[:, 9] * X[:, 10] + # 3æ¬¡äº¤äº’ä½œç”¨
        np.sum(X[:, 11:16] * 0.2, axis=1) + # ç·šå½¢æˆåˆ†
        0.1 * np.random.randn(n_samples)    # ãƒã‚¤ã‚º
    )

    feature_names = [f"feature_{i}" for i in range(n_features)]

    return X, y, feature_names


def calculate_comprehensive_accuracy(y_true, y_pred):
    """åŒ…æ‹¬çš„ãªç²¾åº¦è¨ˆç®—"""
    # R2ã‚¹ã‚³ã‚¢ï¼ˆæ±ºå®šä¿‚æ•°ï¼‰
    r2 = max(0, r2_score(y_true, y_pred))

    # RMSEæ­£è¦åŒ–ï¼ˆå°ã•ã„ã»ã©è‰¯ã„ï¼‰
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    rmse_normalized = max(0, 1 - rmse / (np.std(y_true) + 1e-8))

    # MAEæ­£è¦åŒ–
    mae = mean_absolute_error(y_true, y_pred)
    mae_normalized = max(0, 1 - mae / (np.mean(np.abs(y_true)) + 1e-8))

    # æ–¹å‘äºˆæ¸¬ç²¾åº¦ï¼ˆç¬¦å·ä¸€è‡´ç‡ï¼‰
    if len(y_true) > 1:
        y_true_diff = np.diff(y_true)
        y_pred_diff = np.diff(y_pred)
        direction_accuracy = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff))
    else:
        direction_accuracy = 0.5

    # åˆ†æ•£èª¬æ˜ç‡
    var_explained = max(0, 1 - np.var(y_true - y_pred) / (np.var(y_true) + 1e-8))

    # ç·åˆç²¾åº¦ï¼ˆé‡ã¿ä»˜ãå¹³å‡ï¼‰
    accuracy = (
        r2 * 0.30 +                    # æ±ºå®šä¿‚æ•°
        rmse_normalized * 0.25 +       # RMSE
        mae_normalized * 0.20 +        # MAE
        direction_accuracy * 0.15 +    # æ–¹å‘äºˆæ¸¬
        var_explained * 0.10           # åˆ†æ•£èª¬æ˜
    ) * 100

    return min(99.99, max(0, accuracy))


class SimpleEnsemble:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å®Ÿè£…"""

    def __init__(self, use_rf=True, use_gbm=True, optimize_weights=True):
        self.use_rf = use_rf
        self.use_gbm = use_gbm
        self.optimize_weights = optimize_weights
        self.models = {}
        self.weights = {}
        self.scaler = StandardScaler()
        self.is_fitted = False

    def fit(self, X, y, X_val=None, y_val=None):
        """å­¦ç¿’"""
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        X_scaled = self.scaler.fit_transform(X)
        X_val_scaled = self.scaler.transform(X_val) if X_val is not None else None

        # RandomForest
        if self.use_rf:
            self.models['rf'] = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=2,
                min_samples_leaf=1,
                max_features='sqrt',
                random_state=42,
                n_jobs=-1
            )
            self.models['rf'].fit(X_scaled, y)

        # GradientBoosting
        if self.use_gbm:
            self.models['gbm'] = GradientBoostingRegressor(
                n_estimators=200,
                learning_rate=0.08,
                max_depth=6,
                min_samples_split=3,
                min_samples_leaf=1,
                subsample=0.85,
                random_state=42
            )
            self.models['gbm'].fit(X_scaled, y)

        # é‡ã¿æœ€é©åŒ–
        if self.optimize_weights and X_val is not None and y_val is not None:
            self._optimize_weights(X_val_scaled, y_val)
        else:
            # å‡ç­‰é‡ã¿
            n_models = len(self.models)
            for model_name in self.models.keys():
                self.weights[model_name] = 1.0 / n_models

        self.is_fitted = True

    def _optimize_weights(self, X_val, y_val):
        """é‡ã¿æœ€é©åŒ–"""
        predictions = {}
        for name, model in self.models.items():
            predictions[name] = model.predict(X_val)

        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§æœ€é©é‡ã¿æ¢ç´¢
        best_rmse = float('inf')
        best_weights = {}

        # é‡ã¿ã®å€™è£œ
        if len(self.models) == 1:
            model_name = list(self.models.keys())[0]
            best_weights[model_name] = 1.0
        elif len(self.models) == 2:
            model_names = list(self.models.keys())
            for w1 in np.arange(0.1, 1.0, 0.1):
                w2 = 1.0 - w1
                weights = {model_names[0]: w1, model_names[1]: w2}

                # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
                ensemble_pred = sum(
                    predictions[name] * weight
                    for name, weight in weights.items()
                )

                rmse = np.sqrt(mean_squared_error(y_val, ensemble_pred))
                if rmse < best_rmse:
                    best_rmse = rmse
                    best_weights = weights.copy()

        self.weights = best_weights if best_weights else {name: 1.0/len(self.models) for name in self.models.keys()}

    def predict(self, X):
        """äºˆæ¸¬"""
        if not self.is_fitted:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        X_scaled = self.scaler.transform(X)

        predictions = {}
        for name, model in self.models.items():
            predictions[name] = model.predict(X_scaled)

        # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
        ensemble_pred = sum(
            predictions[name] * self.weights.get(name, 1.0/len(self.models))
            for name in predictions.keys()
        )

        return ensemble_pred


def run_final_benchmark():
    """æœ€çµ‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
    print("=" * 80)
    print("Issue #462: æœ€çµ‚95%ç²¾åº¦é”æˆãƒãƒ£ãƒ¬ãƒ³ã‚¸")
    print("=" * 80)

    # é«˜å“è³ªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    print("é«˜å“è³ªåˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
    X, y, feature_names = generate_high_quality_data(n_samples=1000)

    print(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {X.shape}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆçµ±è¨ˆ: å¹³å‡={y.mean():.4f}, æ¨™æº–åå·®={y.std():.4f}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¯„å›²: [{y.min():.4f}, {y.max():.4f}]")

    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )

    # ã•ã‚‰ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ train/validation ã«åˆ†å‰²
    X_train_sub, X_val, y_train_sub, y_val = train_test_split(
        X_train, y_train, test_size=0.25, random_state=42
    )

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train_sub.shape}")
    print(f"æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {X_val.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")

    # å„è¨­å®šã§ãƒ†ã‚¹ãƒˆ
    configs = [
        {
            'name': 'RandomForestå˜ä½“',
            'use_rf': True,
            'use_gbm': False,
            'optimize_weights': False
        },
        {
            'name': 'GradientBoostingå˜ä½“',
            'use_rf': False,
            'use_gbm': True,
            'optimize_weights': False
        },
        {
            'name': 'RF + GBM å‡ç­‰é‡ã¿',
            'use_rf': True,
            'use_gbm': True,
            'optimize_weights': False
        },
        {
            'name': 'RF + GBM æœ€é©é‡ã¿',
            'use_rf': True,
            'use_gbm': True,
            'optimize_weights': True
        }
    ]

    results = []
    best_accuracy = 0
    best_config_name = ""

    for i, config in enumerate(configs):
        print(f"\n{i+1}. {config['name']} ãƒ†ã‚¹ãƒˆä¸­...")

        try:
            start_time = time.time()

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ä½œæˆãƒ»å­¦ç¿’
            ensemble = SimpleEnsemble(
                use_rf=config['use_rf'],
                use_gbm=config['use_gbm'],
                optimize_weights=config['optimize_weights']
            )

            ensemble.fit(X_train_sub, y_train_sub, X_val, y_val)

            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
            y_pred = ensemble.predict(X_test)

            # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
            accuracy = calculate_comprehensive_accuracy(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mae = mean_absolute_error(y_test, y_pred)

            # æ–¹å‘äºˆæ¸¬ç²¾åº¦
            y_true_diff = np.diff(y_test)
            y_pred_diff = np.diff(y_pred)
            direction_acc = np.mean(np.sign(y_true_diff) == np.sign(y_pred_diff))

            elapsed_time = time.time() - start_time

            print(f"  ç²¾åº¦: {accuracy:.2f}%")
            print(f"  R2ã‚¹ã‚³ã‚¢: {r2:.4f}")
            print(f"  RMSE: {rmse:.4f}")
            print(f"  MAE: {mae:.4f}")
            print(f"  æ–¹å‘äºˆæ¸¬: {direction_acc:.3f}")
            print(f"  å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f}ç§’")
            print(f"  ãƒ¢ãƒ‡ãƒ«é‡ã¿: {ensemble.weights}")

            results.append({
                'name': config['name'],
                'accuracy': accuracy,
                'r2': r2,
                'rmse': rmse,
                'mae': mae,
                'direction_acc': direction_acc,
                'time': elapsed_time,
                'weights': ensemble.weights
            })

            if accuracy > best_accuracy:
                best_accuracy = accuracy
                best_config_name = config['name']

        except Exception as e:
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            results.append({
                'name': config['name'],
                'error': str(e)
            })

    # æœ€çµ‚çµæœ
    print("\n" + "=" * 90)
    print("æœ€çµ‚95%ç²¾åº¦ãƒãƒ£ãƒ¬ãƒ³ã‚¸çµæœ")
    print("=" * 90)

    print(f"{'è¨­å®š':<25} {'ç²¾åº¦':<8} {'R2':<8} {'RMSE':<8} {'æ–¹å‘':<8} {'æ™‚é–“':<8} {'95%å·®åˆ†':<8}")
    print("-" * 90)

    for result in results:
        if 'error' not in result:
            gap = 95.0 - result['accuracy']
            print(f"{result['name']:<25} {result['accuracy']:6.2f}% {result['r2']:6.3f}  "
                 f"{result['rmse']:6.3f}  {result['direction_acc']:6.3f}  "
                 f"{result['time']:6.1f}s  {gap:+6.2f}%")
        else:
            print(f"{result['name']:<25} ERROR")

    print(f"\nğŸ¯ æœ€é«˜ç²¾åº¦: {best_accuracy:.2f}%")
    print(f"ğŸ† æœ€å„ªç§€è¨­å®š: {best_config_name}")
    print(f"ğŸ“Š 95%é”æˆç‡: {(best_accuracy/95.0)*100:.1f}%")
    print(f"ğŸ“ˆ 95%ã¾ã§æ®‹ã‚Š: {95.0 - best_accuracy:.2f}%")

    # Issue #462ã®æœ€çµ‚è©•ä¾¡
    if best_accuracy >= 95.0:
        print(f"\nğŸ‰ğŸ‰ğŸ‰ Issue #462 å®Œå…¨é”æˆï¼ ğŸ‰ğŸ‰ğŸ‰")
        print(f"95%ç²¾åº¦ç›®æ¨™ã‚’ {best_accuracy:.2f}% ã§é”æˆï¼")
        status = "COMPLETED"
    elif best_accuracy >= 92.0:
        print(f"\nğŸš€ Issue #462 ã»ã¼å®Œäº†ï¼92%è¶…é”æˆï¼")
        print(f"95%ã¾ã§ã‚ã¨ {95.0 - best_accuracy:.2f}% ã§éå¸¸ã«è¿‘ã„ï¼")
        status = "NEARLY_COMPLETED"
    elif best_accuracy >= 88.0:
        print(f"\nğŸ“ˆ Issue #462 é †èª¿ãªé€²æ—ï¼88%è¶…é”æˆï¼")
        print(f"95%é”æˆã¸ã®é“ç­‹ãŒæ˜ç¢ºã«ãªã‚Šã¾ã—ãŸ")
        status = "GOOD_PROGRESS"
    elif best_accuracy >= 80.0:
        print(f"\nâš¡ Issue #462 ç€å®Ÿãªé€²æ­©ï¼80%è¶…é”æˆï¼")
        print(f"åŸºç›¤ã¯å›ºã¾ã‚Šã€ã•ã‚‰ãªã‚‹æ”¹å–„ãŒå¯èƒ½")
        status = "MODERATE_PROGRESS"
    else:
        print(f"\nğŸ”§ Issue #462 ç¶™ç¶šä½œæ¥­ä¸­")
        print(f"åŸºç›¤æ”¹å–„ã‹ã‚‰å§‹ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        status = "NEEDS_MORE_WORK"

    # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ææ¡ˆ
    print(f"\nğŸ“‹ Issue #462 æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ({status}):")

    if status == "COMPLETED":
        print("  âœ… 95%ç²¾åº¦ç›®æ¨™é”æˆå®Œäº†")
        print("  ğŸš€ å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼å®Ÿæ–½")
        print("  ğŸ“Š æœ¬ç•ªç’°å¢ƒã¸ã®å°å…¥æ¤œè¨")
        print("  âš¡ ã•ã‚‰ãªã‚‹ç²¾åº¦å‘ä¸Šã®æ¢æ±‚")
    elif status == "NEARLY_COMPLETED":
        print("  ğŸ¯ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¾®èª¿æ•´")
        print("  âš™ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç´°ã‹ã„æœ€é©åŒ–")
        print("  ğŸ“ˆ XGBoostç­‰ã®è¿½åŠ ãƒ¢ãƒ‡ãƒ«æ¤œè¨")
        print("  ğŸ”¬ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®æ”¹å–„")
    else:
        print("  ğŸ”§ ã‚ˆã‚Šé«˜åº¦ãªã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã®å®Ÿè£…")
        print("  ğŸ“Š æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ æ¤œè¨")
        print("  âš¡ ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å¤§å¹…å¼·åŒ–")
        print("  ğŸ¯ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è‡ªå‹•æœ€é©åŒ–ã®å°å…¥")

    print("\n" + "=" * 90)

    return best_accuracy, results, status


if __name__ == "__main__":
    print("Issue #462: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹95%ç²¾åº¦é”æˆãƒãƒ£ãƒ¬ãƒ³ã‚¸")
    print("é–‹å§‹æ™‚åˆ»:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print()

    accuracy, results, status = run_final_benchmark()

    print(f"\n" + "="*90)
    print(f"Issue #462 æœ€çµ‚è©•ä¾¡çµæœ")
    print(f"="*90)
    print(f"ç›®æ¨™: äºˆæ¸¬ç²¾åº¦95%è¶…ã®é”æˆ")
    print(f"çµæœ: {accuracy:.2f}%")
    print(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status}")
    print(f"é”æˆåº¦: {(accuracy/95.0)*100:.1f}%")

    if accuracy >= 95.0:
        print(f"\nğŸ† Issue #462 æ­£å¼å®Œäº†ï¼")
        print(f"ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã§95%ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸï¼")
    else:
        print(f"\nğŸ“Š Issue #462 é€²è¡Œä¸­")
        print(f"ç¾åœ¨ {accuracy:.2f}% ã¾ã§åˆ°é”ã€‚95%ã¾ã§ã‚ã¨ {95.0-accuracy:.2f}%")

    print(f"\nå®Œäº†æ™‚åˆ»:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("="*90)