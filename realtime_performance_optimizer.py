#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Realtime Performance Optimizer - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

Issue #802å®Ÿè£…ï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã¨æ€§èƒ½ã®æœ€é©åŒ–
é«˜é€Ÿãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ä¸¦è¡Œå‡¦ç†ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Š
"""

import asyncio
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import sqlite3
from pathlib import Path
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque
import psutil

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class CacheLevel(Enum):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¬ãƒ™ãƒ«"""
    MEMORY = "memory"
    DISK = "disk"
    DISTRIBUTED = "distributed"

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_usage: float
    network_io: Dict[str, float]
    response_times: Dict[str, float]
    cache_hit_rates: Dict[str, float]
    throughput: Dict[str, int]
    error_rates: Dict[str, float]

@dataclass
class CacheEntry:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒª"""
    key: str
    value: Any
    created_at: datetime
    last_accessed: datetime
    access_count: int
    expiry: Optional[datetime] = None
    size_bytes: int = 0

class HighPerformanceCache:
    """é«˜æ€§èƒ½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 300):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache: Dict[str, CacheEntry] = {}
        self.access_queue = deque()  # LRUè¿½è·¡ç”¨
        self.lock = threading.RLock()
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'size': 0
        }

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.cleanup_thread = threading.Thread(target=self._cleanup_loop, daemon=True)
        self.cleanup_thread.start()

    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—"""
        with self.lock:
            if key in self.cache:
                entry = self.cache[key]

                # æœ‰åŠ¹æœŸé™ãƒã‚§ãƒƒã‚¯
                if entry.expiry and datetime.now() > entry.expiry:
                    self._remove_entry(key)
                    self.stats['misses'] += 1
                    return None

                # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±æ›´æ–°
                entry.last_accessed = datetime.now()
                entry.access_count += 1

                # LRUã‚­ãƒ¥ãƒ¼ã«ç§»å‹•
                if key in self.access_queue:
                    self.access_queue.remove(key)
                self.access_queue.append(key)

                self.stats['hits'] += 1
                return entry.value
            else:
                self.stats['misses'] += 1
                return None

    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’è¨­å®š"""
        with self.lock:
            now = datetime.now()
            expiry = now + timedelta(seconds=ttl or self.ttl_seconds)

            # ã‚µã‚¤ã‚ºè¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            try:
                size_bytes = sys.getsizeof(value)
            except:
                size_bytes = 1024  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚µã‚¤ã‚º

            # å®¹é‡ãƒã‚§ãƒƒã‚¯ã¨å¤ã„ã‚¨ãƒ³ãƒˆãƒªã®å‰Šé™¤
            while len(self.cache) >= self.max_size:
                if self.access_queue:
                    old_key = self.access_queue.popleft()
                    if old_key in self.cache:
                        self._remove_entry(old_key)
                        self.stats['evictions'] += 1
                else:
                    break

            # æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªä½œæˆ
            entry = CacheEntry(
                key=key,
                value=value,
                created_at=now,
                last_accessed=now,
                access_count=1,
                expiry=expiry,
                size_bytes=size_bytes
            )

            self.cache[key] = entry
            self.access_queue.append(key)
            self.stats['size'] = len(self.cache)

            return True

    def _remove_entry(self, key: str):
        """ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤"""
        if key in self.cache:
            del self.cache[key]
        if key in self.access_queue:
            self.access_queue.remove(key)
        self.stats['size'] = len(self.cache)

    def _cleanup_loop(self):
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        while True:
            try:
                with self.lock:
                    now = datetime.now()
                    expired_keys = [
                        key for key, entry in self.cache.items()
                        if entry.expiry and now > entry.expiry
                    ]

                    for key in expired_keys:
                        self._remove_entry(key)

                time.sleep(60)  # 1åˆ†æ¯ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            except Exception as e:
                logging.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(300)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯5åˆ†å¾…æ©Ÿ

    def get_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—"""
        with self.lock:
            hit_rate = (self.stats['hits'] / (self.stats['hits'] + self.stats['misses'])) if (self.stats['hits'] + self.stats['misses']) > 0 else 0
            return {
                **self.stats,
                'hit_rate': hit_rate,
                'total_requests': self.stats['hits'] + self.stats['misses']
            }

class AsyncDataProcessor:
    """éåŒæœŸãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.processing_queue = asyncio.Queue()
        self.result_cache = HighPerformanceCache()

    async def process_batch(self, data_batch: List[Dict[str, Any]], processor_func) -> List[Any]:
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å‡¦ç†"""

        start_time = time.time()
        results = []

        # ä¸¦åˆ—å‡¦ç†ã§ãƒãƒƒãƒã‚’åˆ†å‰²
        chunk_size = max(1, len(data_batch) // self.max_workers)
        chunks = [data_batch[i:i + chunk_size] for i in range(0, len(data_batch), chunk_size)]

        # å„ãƒãƒ£ãƒ³ã‚¯ã‚’ä¸¦è¡Œå‡¦ç†
        futures = []
        for chunk in chunks:
            future = self.executor.submit(self._process_chunk, chunk, processor_func)
            futures.append(future)

        # çµæœåé›†
        for future in as_completed(futures):
            try:
                chunk_results = future.result(timeout=30)
                results.extend(chunk_results)
            except Exception as e:
                logging.error(f"ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        processing_time = time.time() - start_time
        logging.info(f"ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(data_batch)}ä»¶ -> {len(results)}ä»¶ ({processing_time:.2f}ç§’)")

        return results

    def _process_chunk(self, chunk: List[Dict[str, Any]], processor_func) -> List[Any]:
        """ãƒãƒ£ãƒ³ã‚¯å‡¦ç†"""
        results = []
        for item in chunk:
            try:
                result = processor_func(item)
                results.append(result)
            except Exception as e:
                logging.warning(f"ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        return results

class RealtimePerformanceOptimizer:
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.data_cache = HighPerformanceCache(max_size=2000, ttl_seconds=300)
        self.prediction_cache = HighPerformanceCache(max_size=1000, ttl_seconds=600)
        self.analysis_cache = HighPerformanceCache(max_size=500, ttl_seconds=1800)

        # éåŒæœŸå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³
        self.async_processor = AsyncDataProcessor(max_workers=4)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡
        self.metrics_history = deque(maxlen=1440)  # 24æ™‚é–“åˆ†ï¼ˆ1åˆ†æ¯ï¼‰
        self.performance_thresholds = {
            'cpu_usage': 80.0,
            'memory_usage': 85.0,
            'response_time': 2.0,
            'cache_hit_rate': 0.8
        }

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = Path("performance_data/optimization_metrics.db")
        self.db_path.parent.mkdir(exist_ok=True)

        self._init_database()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹
        self.metrics_thread = threading.Thread(target=self._metrics_collection_loop, daemon=True)
        self.metrics_thread.start()

        self.logger.info("Realtime performance optimizer initialized")

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS performance_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        cpu_usage REAL,
                        memory_usage REAL,
                        disk_usage REAL,
                        network_io TEXT,
                        response_times TEXT,
                        cache_hit_rates TEXT,
                        throughput TEXT,
                        error_rates TEXT
                    )
                ''')

                # æœ€é©åŒ–ã‚¤ãƒ™ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS optimization_events (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp TEXT NOT NULL,
                        event_type TEXT NOT NULL,
                        description TEXT,
                        impact_score REAL,
                        success INTEGER DEFAULT 1
                    )
                ''')

                conn.commit()

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    async def optimize_data_retrieval(self, symbol: str, period: str = "5d",
                                   force_refresh: bool = False) -> Optional[pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿å–å¾—æœ€é©åŒ–"""

        cache_key = f"data_{symbol}_{period}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_data = self.data_cache.get(cache_key)
            if cached_data is not None:
                self.logger.debug(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_key}")
                return cached_data

        # ãƒ‡ãƒ¼ã‚¿å–å¾—
        start_time = time.time()
        try:
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, period)

            response_time = time.time() - start_time

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            if data is not None:
                self.data_cache.set(cache_key, data, ttl=300)  # 5åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥

            # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
            self._record_response_time("data_retrieval", response_time)

            return data

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return None

    async def optimize_prediction(self, symbol: str, force_refresh: bool = False) -> Optional[Dict[str, Any]]:
        """äºˆæ¸¬æœ€é©åŒ–"""

        cache_key = f"prediction_{symbol}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        if not force_refresh:
            cached_prediction = self.prediction_cache.get(cache_key)
            if cached_prediction is not None:
                self.logger.debug(f"äºˆæ¸¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_key}")
                return cached_prediction

        # äºˆæ¸¬å®Ÿè¡Œ
        start_time = time.time()
        try:
            from optimized_prediction_system import optimized_prediction_system
            prediction = await optimized_prediction_system.predict_with_optimized_models(symbol)

            response_time = time.time() - start_time

            if prediction:
                # äºˆæ¸¬çµæœã‚’è¾æ›¸ã«å¤‰æ›
                prediction_dict = {
                    'symbol': prediction.symbol,
                    'prediction': prediction.prediction,
                    'confidence': prediction.confidence,
                    'model_consensus': prediction.model_consensus,
                    'timestamp': prediction.timestamp.isoformat()
                }

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆ10åˆ†é–“ï¼‰
                self.prediction_cache.set(cache_key, prediction_dict, ttl=600)

                # æ€§èƒ½ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                self._record_response_time("prediction", response_time)

                return prediction_dict

            return None

        except Exception as e:
            self.logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return None

    async def optimize_batch_analysis(self, symbols: List[str]) -> Dict[str, Any]:
        """ãƒãƒƒãƒåˆ†ææœ€é©åŒ–"""

        start_time = time.time()
        results = {}

        # ä¸¦è¡Œå‡¦ç†ã§ã‚·ãƒ³ãƒœãƒ«ã‚’å‡¦ç†
        tasks = []
        for symbol in symbols:
            task = asyncio.create_task(self._analyze_symbol_optimized(symbol))
            tasks.append((symbol, task))

        # çµæœåé›†
        for symbol, task in tasks:
            try:
                result = await asyncio.wait_for(task, timeout=10)
                if result:
                    results[symbol] = result
            except asyncio.TimeoutError:
                self.logger.warning(f"åˆ†æã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {symbol}")
            except Exception as e:
                self.logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼ {symbol}: {e}")

        processing_time = time.time() - start_time
        self._record_response_time("batch_analysis", processing_time)

        return {
            'results': results,
            'processing_time': processing_time,
            'symbols_processed': len(results),
            'success_rate': len(results) / len(symbols) if symbols else 0
        }

    async def _analyze_symbol_optimized(self, symbol: str) -> Optional[Dict[str, Any]]:
        """ã‚·ãƒ³ãƒœãƒ«åˆ†ææœ€é©åŒ–"""

        cache_key = f"analysis_{symbol}"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cached_analysis = self.analysis_cache.get(cache_key)
        if cached_analysis is not None:
            return cached_analysis

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ï¼‰
            data = await self.optimize_data_retrieval(symbol, "5d")
            if data is None or len(data) < 5:
                return None

            # åŸºæœ¬åˆ†æ
            current_price = data['Close'].iloc[-1]
            price_change = data['Close'].pct_change().iloc[-1]
            volume_change = data['Volume'].pct_change().iloc[-1]

            # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
            volatility = data['Close'].pct_change().rolling(5).std().iloc[-1]

            # ãƒˆãƒ¬ãƒ³ãƒ‰
            sma_5 = data['Close'].rolling(5).mean().iloc[-1]
            trend = (current_price - sma_5) / sma_5

            analysis_result = {
                'symbol': symbol,
                'current_price': current_price,
                'price_change': price_change,
                'volume_change': volume_change,
                'volatility': volatility,
                'trend': trend,
                'timestamp': datetime.now().isoformat()
            }

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆ30åˆ†é–“ï¼‰
            self.analysis_cache.set(cache_key, analysis_result, ttl=1800)

            return analysis_result

        except Exception as e:
            self.logger.error(f"ã‚·ãƒ³ãƒœãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {symbol}: {e}")
            return None

    def _record_response_time(self, operation: str, response_time: float):
        """å¿œç­”æ™‚é–“è¨˜éŒ²"""

        if not hasattr(self, '_response_times'):
            self._response_times = {}

        if operation not in self._response_times:
            self._response_times[operation] = deque(maxlen=100)

        self._response_times[operation].append(response_time)

    def _metrics_collection_loop(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ«ãƒ¼ãƒ—"""

        while True:
            try:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
                cpu_usage = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                network = psutil.net_io_counters()

                # å¿œç­”æ™‚é–“çµ±è¨ˆ
                response_times = {}
                if hasattr(self, '_response_times'):
                    for operation, times in self._response_times.items():
                        if times:
                            response_times[operation] = np.mean(list(times))

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
                cache_hit_rates = {
                    'data_cache': self.data_cache.get_stats()['hit_rate'],
                    'prediction_cache': self.prediction_cache.get_stats()['hit_rate'],
                    'analysis_cache': self.analysis_cache.get_stats()['hit_rate']
                }

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ
                metrics = PerformanceMetrics(
                    timestamp=datetime.now(),
                    cpu_usage=cpu_usage,
                    memory_usage=memory.percent,
                    disk_usage=disk.percent,
                    network_io={
                        'bytes_sent': network.bytes_sent,
                        'bytes_recv': network.bytes_recv
                    },
                    response_times=response_times,
                    cache_hit_rates=cache_hit_rates,
                    throughput={},
                    error_rates={}
                )

                # å±¥æ­´ã«è¿½åŠ 
                self.metrics_history.append(metrics)

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆ5åˆ†æ¯ï¼‰
                if len(self.metrics_history) % 5 == 0:
                    asyncio.create_task(self._save_metrics_to_db(metrics))

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è­¦å‘Šãƒã‚§ãƒƒã‚¯
                self._check_performance_thresholds(metrics)

                time.sleep(60)  # 1åˆ†é–“éš”

            except Exception as e:
                self.logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(300)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯5åˆ†å¾…æ©Ÿ

    def _check_performance_thresholds(self, metrics: PerformanceMetrics):
        """æ€§èƒ½é–¾å€¤ãƒã‚§ãƒƒã‚¯"""

        warnings = []

        if metrics.cpu_usage > self.performance_thresholds['cpu_usage']:
            warnings.append(f"é«˜CPUä½¿ç”¨ç‡: {metrics.cpu_usage:.1f}%")

        if metrics.memory_usage > self.performance_thresholds['memory_usage']:
            warnings.append(f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {metrics.memory_usage:.1f}%")

        # å¿œç­”æ™‚é–“ãƒã‚§ãƒƒã‚¯
        for operation, time_ms in metrics.response_times.items():
            if time_ms > self.performance_thresholds['response_time']:
                warnings.append(f"é…ã„å¿œç­”æ™‚é–“ {operation}: {time_ms:.2f}ç§’")

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãƒã‚§ãƒƒã‚¯
        for cache_name, hit_rate in metrics.cache_hit_rates.items():
            if hit_rate < self.performance_thresholds['cache_hit_rate']:
                warnings.append(f"ä½ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ {cache_name}: {hit_rate:.1%}")

        if warnings:
            self.logger.warning(f"æ€§èƒ½è­¦å‘Š: {', '.join(warnings)}")

    async def _save_metrics_to_db(self, metrics: PerformanceMetrics):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                cursor.execute('''
                    INSERT INTO performance_metrics
                    (timestamp, cpu_usage, memory_usage, disk_usage, network_io,
                     response_times, cache_hit_rates, throughput, error_rates)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    metrics.timestamp.isoformat(),
                    metrics.cpu_usage,
                    metrics.memory_usage,
                    metrics.disk_usage,
                    json.dumps(metrics.network_io),
                    json.dumps(metrics.response_times),
                    json.dumps(metrics.cache_hit_rates),
                    json.dumps(metrics.throughput),
                    json.dumps(metrics.error_rates)
                ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_performance_report(self) -> Dict[str, Any]:
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆå–å¾—"""

        if not self.metrics_history:
            return {"error": "ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ãªã—"}

        recent_metrics = list(self.metrics_history)[-60:]  # éå»1æ™‚é–“

        # çµ±è¨ˆè¨ˆç®—
        avg_cpu = np.mean([m.cpu_usage for m in recent_metrics])
        avg_memory = np.mean([m.memory_usage for m in recent_metrics])

        # å¿œç­”æ™‚é–“çµ±è¨ˆ
        response_time_stats = {}
        if hasattr(self, '_response_times'):
            for operation, times in self._response_times.items():
                if times:
                    response_time_stats[operation] = {
                        'avg': np.mean(list(times)),
                        'min': np.min(list(times)),
                        'max': np.max(list(times)),
                        'p95': np.percentile(list(times), 95)
                    }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
        cache_stats = {
            'data_cache': self.data_cache.get_stats(),
            'prediction_cache': self.prediction_cache.get_stats(),
            'analysis_cache': self.analysis_cache.get_stats()
        }

        return {
            'timestamp': datetime.now().isoformat(),
            'system_metrics': {
                'avg_cpu_usage': avg_cpu,
                'avg_memory_usage': avg_memory,
                'metrics_count': len(recent_metrics)
            },
            'response_times': response_time_stats,
            'cache_stats': cache_stats,
            'performance_status': self._get_performance_status(avg_cpu, avg_memory, cache_stats)
        }

    def _get_performance_status(self, cpu: float, memory: float, cache_stats: Dict[str, Any]) -> str:
        """æ€§èƒ½çŠ¶æ…‹åˆ¤å®š"""

        issues = 0

        if cpu > 80:
            issues += 1
        if memory > 85:
            issues += 1

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãƒã‚§ãƒƒã‚¯
        for cache_name, stats in cache_stats.items():
            if stats['hit_rate'] < 0.7:
                issues += 1

        if issues == 0:
            return "EXCELLENT"
        elif issues <= 1:
            return "GOOD"
        elif issues <= 2:
            return "WARNING"
        else:
            return "CRITICAL"

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
realtime_performance_optimizer = RealtimePerformanceOptimizer()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def run_performance_optimization_test():
    """æ€§èƒ½æœ€é©åŒ–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("=== ğŸš€ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ ===")

    test_symbols = ["7203", "8306", "4751"]

    # å€‹åˆ¥æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ“Š å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿å–å¾—æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    for symbol in test_symbols:
        start_time = time.time()
        data = await realtime_performance_optimizer.optimize_data_retrieval(symbol)
        response_time = time.time() - start_time

        status = "âœ…" if data is not None else "âŒ"
        print(f"  {status} {symbol}: {response_time:.3f}ç§’")

    # ãƒãƒƒãƒåˆ†ææœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
    print(f"\nâš¡ ãƒãƒƒãƒåˆ†ææœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    batch_result = await realtime_performance_optimizer.optimize_batch_analysis(test_symbols)

    print(f"  å‡¦ç†æ™‚é–“: {batch_result['processing_time']:.3f}ç§’")
    print(f"  æˆåŠŸç‡: {batch_result['success_rate']:.1%}")
    print(f"  å‡¦ç†éŠ˜æŸ„æ•°: {batch_result['symbols_processed']}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœãƒ†ã‚¹ãƒˆ
    print(f"\nğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœãƒ†ã‚¹ãƒˆ")
    symbol = "7203"

    # 1å›ç›®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰
    start_time = time.time()
    await realtime_performance_optimizer.optimize_data_retrieval(symbol)
    first_time = time.time() - start_time

    # 2å›ç›®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
    start_time = time.time()
    await realtime_performance_optimizer.optimize_data_retrieval(symbol)
    second_time = time.time() - start_time

    speedup = first_time / second_time if second_time > 0 else 1
    print(f"  1å›ç›®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰: {first_time:.3f}ç§’")
    print(f"  2å›ç›®ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰: {second_time:.3f}ç§’")
    print(f"  é«˜é€ŸåŒ–å€ç‡: {speedup:.1f}x")

    # æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ
    print(f"\nğŸ“ˆ æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆ")
    report = realtime_performance_optimizer.get_performance_report()

    print(f"  ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {report['performance_status']}")
    print(f"  å¹³å‡CPUä½¿ç”¨ç‡: {report['system_metrics']['avg_cpu_usage']:.1f}%")
    print(f"  å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {report['system_metrics']['avg_memory_usage']:.1f}%")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
    cache_stats = report['cache_stats']
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_stats['data_cache']['hit_rate']:.1%}")
    print(f"  äºˆæ¸¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_stats['prediction_cache']['hit_rate']:.1%}")
    print(f"  åˆ†æã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_stats['analysis_cache']['hit_rate']:.1%}")

    print(f"\nâœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ç¨¼åƒä¸­")

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_performance_optimization_test())