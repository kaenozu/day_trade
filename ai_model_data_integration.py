#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI Model Data Integration System - AIãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

Issue #801å®Ÿè£…ï¼šAIãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿çµ±åˆã®å®Ÿæ–½
è¤‡æ•°ã®AIãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®çµ±åˆã«ã‚ˆã‚‹é«˜åº¦äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import sqlite3
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.metrics import accuracy_score, classification_report
import joblib

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class ModelType(Enum):
    """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"""
    TREND_FOLLOWING = "trend_following"
    MOMENTUM = "momentum"
    VOLATILITY = "volatility"
    SENTIMENT = "sentiment"
    TECHNICAL = "technical"
    FUNDAMENTAL = "fundamental"
    ENSEMBLE = "ensemble"

class DataSource(Enum):
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹"""
    PRICE_DATA = "price_data"
    VOLUME_DATA = "volume_data"
    TECHNICAL_INDICATORS = "technical_indicators"
    MARKET_SENTIMENT = "market_sentiment"
    ECONOMIC_DATA = "economic_data"
    NEWS_DATA = "news_data"

@dataclass
class ModelPrediction:
    """ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬çµæœ"""
    model_type: ModelType
    symbol: str
    prediction: int  # 0: ä¸‹é™, 1: ä¸Šæ˜‡
    confidence: float
    feature_importance: Dict[str, float]
    prediction_details: Dict[str, Any]
    timestamp: datetime

@dataclass
class IntegratedPrediction:
    """çµ±åˆäºˆæ¸¬çµæœ"""
    symbol: str
    final_prediction: int
    confidence: float
    model_consensus: Dict[str, Dict[str, Any]]
    feature_contributions: Dict[str, float]
    risk_assessment: Dict[str, float]
    explanation: List[str]
    timestamp: datetime

class AdvancedFeatureEngineering:
    """é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.scalers = {}

    def create_comprehensive_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """åŒ…æ‹¬çš„ç‰¹å¾´é‡ä½œæˆ"""

        if len(data) < 50:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½50ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå¿…è¦ï¼‰")

        features = pd.DataFrame(index=data.index)

        # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        high = data['High']
        low = data['Low']
        close = data['Close']
        volume = data['Volume']
        open_price = data['Open']

        try:
            # 1. ä¾¡æ ¼ç³»ç‰¹å¾´é‡
            features = self._add_price_features(features, open_price, high, low, close)

            # 2. ãƒœãƒªãƒ¥ãƒ¼ãƒ ç³»ç‰¹å¾´é‡
            features = self._add_volume_features(features, volume, close)

            # 3. æŠ€è¡“æŒ‡æ¨™ç‰¹å¾´é‡
            features = self._add_technical_features(features, high, low, close, volume)

            # 4. çµ±è¨ˆçš„ç‰¹å¾´é‡
            features = self._add_statistical_features(features, close)

            # 5. ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡
            features = self._add_trend_features(features, close)

            # 6. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡
            features = self._add_volatility_features(features, high, low, close)

            # 7. æ™‚é–“ç³»ç‰¹å¾´é‡
            features = self._add_temporal_features(features, data.index)

            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            features = self._clean_features(features)

            self.logger.info(f"åŒ…æ‹¬ç‰¹å¾´é‡ä½œæˆå®Œäº†: {features.shape[1]}ç‰¹å¾´é‡, {features.shape[0]}ã‚µãƒ³ãƒ—ãƒ«")

        except Exception as e:
            self.logger.error(f"ç‰¹å¾´é‡ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise

        return features

    def _add_price_features(self, features: pd.DataFrame, open_price: pd.Series,
                          high: pd.Series, low: pd.Series, close: pd.Series) -> pd.DataFrame:
        """ä¾¡æ ¼ç³»ç‰¹å¾´é‡è¿½åŠ """

        # åŸºæœ¬ä¾¡æ ¼ç‰¹å¾´é‡
        features['returns'] = close.pct_change()
        features['log_returns'] = np.log(close / close.shift(1))
        features['price_range'] = (high - low) / close
        features['gap'] = (open_price - close.shift(1)) / close.shift(1)
        features['body_size'] = abs(close - open_price) / close

        # ä¾¡æ ¼ä½ç½®ç‰¹å¾´é‡
        features['high_close_ratio'] = (high - close) / (high - low + 1e-10)
        features['low_close_ratio'] = (close - low) / (high - low + 1e-10)
        features['open_close_ratio'] = (close - open_price) / (high - low + 1e-10)

        # è¤‡æ•°æœŸé–“ã®ãƒªã‚¿ãƒ¼ãƒ³
        for period in [2, 3, 5, 10]:
            if len(close) > period:
                features[f'return_{period}d'] = close.pct_change(period)
                features[f'return_vol_{period}d'] = features['returns'].rolling(period).std()

        return features

    def _add_volume_features(self, features: pd.DataFrame, volume: pd.Series, close: pd.Series) -> pd.DataFrame:
        """ãƒœãƒªãƒ¥ãƒ¼ãƒ ç³»ç‰¹å¾´é‡è¿½åŠ """

        # ãƒœãƒªãƒ¥ãƒ¼ãƒ åŸºæœ¬çµ±è¨ˆ
        for period in [5, 10, 20]:
            if len(volume) > period:
                vol_ma = volume.rolling(period).mean()
                features[f'volume_ratio_{period}'] = volume / vol_ma
                features[f'volume_zscore_{period}'] = (volume - vol_ma) / volume.rolling(period).std()

        # ä¾¡æ ¼ãƒ»ãƒœãƒªãƒ¥ãƒ¼ãƒ é–¢ä¿‚
        features['price_volume'] = close * volume
        features['volume_price_trend'] = (features['returns'] * (volume / volume.rolling(10).mean())).rolling(5).mean()

        # On-Balance Volume
        obv = pd.Series(index=close.index, dtype=float)
        obv.iloc[0] = volume.iloc[0]
        for i in range(1, len(close)):
            if close.iloc[i] > close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] + volume.iloc[i]
            elif close.iloc[i] < close.iloc[i-1]:
                obv.iloc[i] = obv.iloc[i-1] - volume.iloc[i]
            else:
                obv.iloc[i] = obv.iloc[i-1]
        features['obv'] = obv
        features['obv_trend'] = obv.pct_change(5)

        return features

    def _add_technical_features(self, features: pd.DataFrame, high: pd.Series,
                              low: pd.Series, close: pd.Series, volume: pd.Series) -> pd.DataFrame:
        """æŠ€è¡“æŒ‡æ¨™ç‰¹å¾´é‡è¿½åŠ """

        # ç§»å‹•å¹³å‡ç³»
        for period in [5, 10, 20, 50]:
            if len(close) > period:
                sma = close.rolling(period).mean()
                ema = close.ewm(span=period).mean()

                features[f'sma_{period}'] = sma
                features[f'sma_distance_{period}'] = (close - sma) / sma
                features[f'ema_distance_{period}'] = (close - ema) / ema
                features[f'price_position_{period}'] = (close - sma) / close.rolling(period).std()

        # MACD
        if len(close) > 26:
            ema_12 = close.ewm(span=12).mean()
            ema_26 = close.ewm(span=26).mean()
            macd = ema_12 - ema_26
            macd_signal = macd.ewm(span=9).mean()
            features['macd'] = macd
            features['macd_signal'] = macd_signal
            features['macd_histogram'] = macd - macd_signal
            features['macd_trend'] = macd.pct_change(3)

        # RSI
        for period in [9, 14, 21]:
            if len(close) > period:
                delta = close.diff()
                gain = (delta.where(delta > 0, 0)).rolling(period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
                rs = gain / (loss + 1e-10)
                rsi = 100 - (100 / (1 + rs))
                features[f'rsi_{period}'] = rsi
                features[f'rsi_trend_{period}'] = rsi.pct_change(3)

        # ãƒœãƒªãƒ³ã‚¸ãƒ£ãƒ¼ãƒãƒ³ãƒ‰
        if len(close) > 20:
            sma_20 = close.rolling(20).mean()
            std_20 = close.rolling(20).std()
            bb_upper = sma_20 + (std_20 * 2)
            bb_lower = sma_20 - (std_20 * 2)

            features['bb_width'] = (bb_upper - bb_lower) / sma_20
            features['bb_position'] = (close - bb_lower) / (bb_upper - bb_lower + 1e-10)
            features['bb_squeeze'] = features['bb_width'] < features['bb_width'].rolling(10).mean()

        # ATR
        for period in [10, 14, 20]:
            if len(high) > period:
                tr1 = high - low
                tr2 = abs(high - close.shift(1))
                tr3 = abs(low - close.shift(1))
                true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
                atr = true_range.rolling(period).mean()
                features[f'atr_{period}'] = atr
                features[f'atr_ratio_{period}'] = atr / close

        # Stochastic
        if len(high) > 14:
            period = 14
            lowest_low = low.rolling(period).min()
            highest_high = high.rolling(period).max()
            k_percent = 100 * (close - lowest_low) / (highest_high - lowest_low + 1e-10)
            d_percent = k_percent.rolling(3).mean()
            features['stoch_k'] = k_percent
            features['stoch_d'] = d_percent
            features['stoch_divergence'] = k_percent - d_percent

        return features

    def _add_statistical_features(self, features: pd.DataFrame, close: pd.Series) -> pd.DataFrame:
        """çµ±è¨ˆçš„ç‰¹å¾´é‡è¿½åŠ """

        returns = close.pct_change()

        for period in [5, 10, 20]:
            if len(returns) > period:
                ret_window = returns.rolling(period)
                features[f'skewness_{period}'] = ret_window.skew()
                features[f'kurtosis_{period}'] = ret_window.kurt()
                features[f'var_{period}'] = ret_window.var()
                features[f'sharpe_{period}'] = ret_window.mean() / (ret_window.std() + 1e-10) * np.sqrt(252)

        # ä¾¡æ ¼åˆ†å¸ƒç‰¹å¾´é‡
        for period in [10, 20]:
            if len(close) > period:
                close_window = close.rolling(period)
                features[f'price_percentile_{period}'] = close.rolling(period).rank(pct=True)
                features[f'price_zscore_{period}'] = (close - close_window.mean()) / close_window.std()

        return features

    def _add_trend_features(self, features: pd.DataFrame, close: pd.Series) -> pd.DataFrame:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ç‰¹å¾´é‡è¿½åŠ """

        # å˜ç´”ãƒˆãƒ¬ãƒ³ãƒ‰
        for period in [5, 10, 20]:
            if len(close) > period:
                features[f'trend_strength_{period}'] = (close > close.shift(period)).rolling(5).mean()
                features[f'trend_consistency_{period}'] = (close.diff() > 0).rolling(period).mean()

        # ç·šå½¢å›å¸°ãƒˆãƒ¬ãƒ³ãƒ‰
        for period in [10, 20]:
            if len(close) > period:
                def calc_trend_slope(x):
                    if len(x) < 3:
                        return 0
                    y = np.arange(len(x))
                    slope = np.polyfit(y, x, 1)[0]
                    return slope

                features[f'trend_slope_{period}'] = close.rolling(period).apply(calc_trend_slope, raw=True)

        # ADXï¼ˆãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦ï¼‰
        if len(close) > 14:
            high_diff = close.shift(1).rolling(2).max() - close.shift(1)
            low_diff = close.shift(1) - close.shift(1).rolling(2).min()

            plus_dm = high_diff.where(high_diff > low_diff, 0)
            minus_dm = low_diff.where(low_diff > high_diff, 0)

            # ç°¡æ˜“ç‰ˆADX
            dm_sum = plus_dm.rolling(14).sum() + minus_dm.rolling(14).sum()
            features['trend_intensity'] = dm_sum / close * 100

        return features

    def _add_volatility_features(self, features: pd.DataFrame, high: pd.Series,
                               low: pd.Series, close: pd.Series) -> pd.DataFrame:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ç‰¹å¾´é‡è¿½åŠ """

        returns = close.pct_change()

        # å®Ÿç¾ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£
        for period in [5, 10, 20]:
            if len(returns) > period:
                vol = returns.rolling(period).std() * np.sqrt(252)
                features[f'volatility_{period}'] = vol
                features[f'vol_rank_{period}'] = vol.rolling(50).rank(pct=True)

        # Parkinsonæ¨å®šé‡ï¼ˆé«˜å€¤ãƒ»å®‰å€¤ã‚’ä½¿ç”¨ï¼‰
        for period in [10, 20]:
            if len(high) > period:
                parkinson = np.log(high / low) ** 2
                features[f'parkinson_vol_{period}'] = parkinson.rolling(period).mean()

        # Garman-Klassæ¨å®šé‡
        if len(high) > 10:
            gk = 0.5 * (np.log(high / low)) ** 2 - (2 * np.log(2) - 1) * (np.log(close / close.shift(1))) ** 2
            features['garman_klass_vol'] = gk.rolling(10).mean()

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼
        vol_5 = returns.rolling(5).std()
        features['vol_cluster'] = (vol_5 > vol_5.rolling(20).mean()).astype(int)

        return features

    def _add_temporal_features(self, features: pd.DataFrame, index: pd.DatetimeIndex) -> pd.DataFrame:
        """æ™‚é–“ç³»ç‰¹å¾´é‡è¿½åŠ """

        if isinstance(index, pd.DatetimeIndex):
            features['day_of_week'] = index.dayofweek
            features['month'] = index.month
            features['quarter'] = index.quarter
            features['is_month_end'] = index.is_month_end.astype(int)
            features['is_quarter_end'] = index.is_quarter_end.astype(int)

        # å­£ç¯€æ€§
        features['seasonal_trend'] = np.sin(2 * np.pi * np.arange(len(features)) / 252)  # å¹´æ¬¡å­£ç¯€æ€§
        features['monthly_cycle'] = np.sin(2 * np.pi * np.arange(len(features)) / 21)   # æœˆæ¬¡ã‚µã‚¤ã‚¯ãƒ«

        return features

    def _clean_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾´é‡ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""

        # ç„¡é™å¤§å€¤å‡¦ç†
        features = features.replace([np.inf, -np.inf], np.nan)

        # æ¬ æå€¤å‡¦ç†
        features = features.fillna(method='ffill').fillna(method='bfill')
        features = features.fillna(0)

        # ç•°å¸¸å€¤å‡¦ç†ï¼ˆ3ã‚·ã‚°ãƒãƒ«ãƒ¼ãƒ«ï¼‰
        for col in features.select_dtypes(include=[np.number]).columns:
            mean = features[col].mean()
            std = features[col].std()
            if std > 0:
                features[col] = features[col].clip(mean - 3*std, mean + 3*std)

        return features

class SpecializedAIModels:
    """å°‚é–€AIãƒ¢ãƒ‡ãƒ«ç¾¤"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.models = {}
        self.scalers = {}

    def create_trend_model(self) -> RandomForestClassifier:
        """ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“ãƒ¢ãƒ‡ãƒ«"""
        return RandomForestClassifier(
            n_estimators=100,
            max_depth=8,
            min_samples_split=5,
            min_samples_leaf=3,
            random_state=42,
            class_weight='balanced'
        )

    def create_momentum_model(self) -> RandomForestClassifier:
        """ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«"""
        return RandomForestClassifier(
            n_estimators=80,
            max_depth=10,
            min_samples_split=3,
            min_samples_leaf=2,
            random_state=43,
            class_weight='balanced'
        )

    def create_volatility_model(self) -> RandomForestClassifier:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«"""
        return RandomForestClassifier(
            n_estimators=60,
            max_depth=6,
            min_samples_split=4,
            min_samples_leaf=2,
            random_state=44,
            class_weight='balanced'
        )

    async def train_specialized_models(self, symbol: str, features: pd.DataFrame,
                                     targets: pd.Series) -> Dict[ModelType, Any]:
        """å°‚é–€ãƒ¢ãƒ‡ãƒ«ç¾¤è¨“ç·´"""

        if len(features) < 30:
            raise ValueError("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

        trained_models = {}

        # ç‰¹å¾´é‡é¸æŠã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scaler = StandardScaler()
        features_scaled = pd.DataFrame(
            scaler.fit_transform(features),
            columns=features.columns,
            index=features.index
        )

        self.scalers[symbol] = scaler

        # ãƒˆãƒ¬ãƒ³ãƒ‰è¿½å¾“ãƒ¢ãƒ‡ãƒ«
        trend_features = self._select_trend_features(features_scaled)
        trend_model = self.create_trend_model()
        trend_model.fit(trend_features, targets)
        trained_models[ModelType.TREND_FOLLOWING] = {
            'model': trend_model,
            'features': trend_features.columns.tolist(),
            'accuracy': self._calculate_accuracy(trend_model, trend_features, targets)
        }

        # ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«
        momentum_features = self._select_momentum_features(features_scaled)
        momentum_model = self.create_momentum_model()
        momentum_model.fit(momentum_features, targets)
        trained_models[ModelType.MOMENTUM] = {
            'model': momentum_model,
            'features': momentum_features.columns.tolist(),
            'accuracy': self._calculate_accuracy(momentum_model, momentum_features, targets)
        }

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«
        vol_features = self._select_volatility_features(features_scaled)
        vol_model = self.create_volatility_model()
        vol_model.fit(vol_features, targets)
        trained_models[ModelType.VOLATILITY] = {
            'model': vol_model,
            'features': vol_features.columns.tolist(),
            'accuracy': self._calculate_accuracy(vol_model, vol_features, targets)
        }

        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«
        ensemble_model = VotingClassifier([
            ('trend', trend_model),
            ('momentum', momentum_model),
            ('volatility', vol_model)
        ], voting='soft')

        ensemble_model.fit(features_scaled, targets)
        trained_models[ModelType.ENSEMBLE] = {
            'model': ensemble_model,
            'features': features_scaled.columns.tolist(),
            'accuracy': self._calculate_accuracy(ensemble_model, features_scaled, targets)
        }

        self.models[symbol] = trained_models

        return trained_models

    def _select_trend_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ãƒˆãƒ¬ãƒ³ãƒ‰é–¢é€£ç‰¹å¾´é‡é¸æŠ"""
        trend_columns = [col for col in features.columns if any(
            keyword in col.lower() for keyword in [
                'sma', 'ema', 'trend', 'macd', 'adx', 'slope'
            ]
        )]
        return features[trend_columns[:20]]  # ä¸Šä½20ç‰¹å¾´é‡

    def _select_momentum_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ é–¢é€£ç‰¹å¾´é‡é¸æŠ"""
        momentum_columns = [col for col in features.columns if any(
            keyword in col.lower() for keyword in [
                'rsi', 'stoch', 'momentum', 'roc', 'return'
            ]
        )]
        return features[momentum_columns[:20]]

    def _select_volatility_features(self, features: pd.DataFrame) -> pd.DataFrame:
        """ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢é€£ç‰¹å¾´é‡é¸æŠ"""
        vol_columns = [col for col in features.columns if any(
            keyword in col.lower() for keyword in [
                'vol', 'atr', 'bb', 'range', 'parkinson', 'garman'
            ]
        )]
        return features[vol_columns[:20]]

    def _calculate_accuracy(self, model, features: pd.DataFrame, targets: pd.Series) -> float:
        """ãƒ¢ãƒ‡ãƒ«ç²¾åº¦è¨ˆç®—"""
        try:
            cv_scores = cross_val_score(model, features, targets, cv=3, scoring='accuracy')
            return cv_scores.mean()
        except:
            return 0.5

    async def predict_with_models(self, symbol: str, current_features: pd.DataFrame) -> Dict[ModelType, ModelPrediction]:
        """ãƒ¢ãƒ‡ãƒ«ç¾¤ã§äºˆæ¸¬"""

        if symbol not in self.models:
            raise ValueError(f"ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“: {symbol}")

        predictions = {}
        scaler = self.scalers.get(symbol)

        if scaler is None:
            raise ValueError(f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {symbol}")

        # ç‰¹å¾´é‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        features_scaled = pd.DataFrame(
            scaler.transform(current_features),
            columns=current_features.columns,
            index=current_features.index
        )

        for model_type, model_info in self.models[symbol].items():
            try:
                model = model_info['model']
                required_features = model_info['features']

                # å¿…è¦ãªç‰¹å¾´é‡ã‚’é¸æŠ
                X = features_scaled[required_features]

                # äºˆæ¸¬å®Ÿè¡Œ
                prediction = model.predict(X.iloc[-1:].values)[0]
                confidence = model.predict_proba(X.iloc[-1:].values)[0].max()

                # ç‰¹å¾´é‡é‡è¦åº¦
                if hasattr(model, 'feature_importances_'):
                    importance = dict(zip(required_features, model.feature_importances_))
                else:
                    importance = {}

                predictions[model_type] = ModelPrediction(
                    model_type=model_type,
                    symbol=symbol,
                    prediction=int(prediction),
                    confidence=float(confidence),
                    feature_importance=importance,
                    prediction_details={
                        'accuracy': model_info['accuracy'],
                        'feature_count': len(required_features)
                    },
                    timestamp=datetime.now()
                )

            except Exception as e:
                self.logger.error(f"ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ {model_type}: {e}")
                continue

        return predictions

class AIModelDataIntegration:
    """AIãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.feature_engineering = AdvancedFeatureEngineering()
        self.ai_models = SpecializedAIModels()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = Path("ai_integration_data/integrated_predictions.db")
        self.db_path.parent.mkdir(exist_ok=True)

        self._init_database()
        self.logger.info("AI model data integration system initialized")

    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # çµ±åˆäºˆæ¸¬ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS integrated_predictions (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        final_prediction INTEGER NOT NULL,
                        confidence REAL NOT NULL,
                        model_consensus TEXT,
                        feature_contributions TEXT,
                        risk_assessment TEXT,
                        explanation TEXT,
                        timestamp TEXT NOT NULL
                    )
                ''')

                # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ãƒ¼ãƒ–ãƒ«
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS model_performance (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        model_type TEXT NOT NULL,
                        accuracy REAL,
                        training_samples INTEGER,
                        feature_count INTEGER,
                        timestamp TEXT NOT NULL
                    )
                ''')

                conn.commit()

        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")

    async def train_integrated_system(self, symbol: str, period: str = "6mo") -> Dict[str, Any]:
        """çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨“ç·´"""

        self.logger.info(f"çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨“ç·´é–‹å§‹: {symbol}")

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, period)

            if data is None or len(data) < 100:
                raise ValueError("ååˆ†ãªè¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # åŒ…æ‹¬çš„ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            features = self.feature_engineering.create_comprehensive_features(data)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
            targets = self._create_targets(data['Close'].iloc[-len(features):])

            # ãƒ‡ãƒ¼ã‚¿åŒæœŸ
            min_length = min(len(features), len(targets))
            features = features.iloc[:min_length]
            targets = targets[:min_length]

            if len(features) < 50:
                raise ValueError("è¨“ç·´ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # å°‚é–€ãƒ¢ãƒ‡ãƒ«ç¾¤è¨“ç·´
            trained_models = await self.ai_models.train_specialized_models(symbol, features, targets)

            # è¨“ç·´çµæœè¨˜éŒ²
            await self._save_model_performance(symbol, trained_models, len(targets))

            self.logger.info(f"çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨“ç·´å®Œäº†: {len(trained_models)}ãƒ¢ãƒ‡ãƒ«")

            return {
                'symbol': symbol,
                'models_trained': len(trained_models),
                'training_samples': len(targets),
                'feature_count': len(features.columns),
                'model_accuracies': {
                    str(model_type): model_info['accuracy']
                    for model_type, model_info in trained_models.items()
                }
            }

        except Exception as e:
            self.logger.error(f"çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _create_targets(self, prices: pd.Series) -> np.ndarray:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ"""
        # ç¿Œæ—¥ã®ä¾¡æ ¼å¤‰å‹•
        returns = prices.pct_change().shift(-1)

        # é–¾å€¤ä»¥ä¸Šã®ä¸Šæ˜‡ã‚’1ã€ãã‚Œä»¥å¤–ã‚’0
        threshold = 0.001  # 0.1%ä»¥ä¸Šã®å¤‰å‹•
        targets = (returns > threshold).astype(int)

        return targets.values[:-1]  # æœ€å¾Œã®è¦ç´ ï¼ˆNaNï¼‰ã‚’é™¤å»

    async def predict_integrated(self, symbol: str, force_retrain: bool = False) -> IntegratedPrediction:
        """çµ±åˆäºˆæ¸¬å®Ÿè¡Œ"""

        try:
            # å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«å†è¨“ç·´
            if force_retrain or symbol not in self.ai_models.models:
                await self.train_integrated_system(symbol)

            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, "3mo")

            if data is None or len(data) < 50:
                raise ValueError("äºˆæ¸¬ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            features = self.feature_engineering.create_comprehensive_features(data)

            # å„å°‚é–€ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            model_predictions = await self.ai_models.predict_with_models(symbol, features)

            # çµ±åˆäºˆæ¸¬è¨ˆç®—
            integrated_result = self._integrate_predictions(symbol, model_predictions)

            # çµæœä¿å­˜
            await self._save_integrated_prediction(integrated_result)

            return integrated_result

        except Exception as e:
            self.logger.error(f"çµ±åˆäºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬ã‚’è¿”ã™
            return IntegratedPrediction(
                symbol=symbol,
                final_prediction=1,
                confidence=0.5,
                model_consensus={},
                feature_contributions={},
                risk_assessment={'risk_level': 'medium'},
                explanation=['äºˆæ¸¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ'],
                timestamp=datetime.now()
            )

    def _integrate_predictions(self, symbol: str, model_predictions: Dict[ModelType, ModelPrediction]) -> IntegratedPrediction:
        """äºˆæ¸¬çµ±åˆ"""

        if not model_predictions:
            return IntegratedPrediction(
                symbol=symbol,
                final_prediction=1,
                confidence=0.5,
                model_consensus={},
                feature_contributions={},
                risk_assessment={'risk_level': 'high'},
                explanation=['åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ãŒã‚ã‚Šã¾ã›ã‚“'],
                timestamp=datetime.now()
            )

        # ãƒ¢ãƒ‡ãƒ«é‡ã¿ï¼ˆç²¾åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        model_weights = {}
        total_accuracy = sum(pred.prediction_details.get('accuracy', 0.5) for pred in model_predictions.values())

        for model_type, prediction in model_predictions.items():
            accuracy = prediction.prediction_details.get('accuracy', 0.5)
            model_weights[model_type] = accuracy / total_accuracy if total_accuracy > 0 else 1.0 / len(model_predictions)

        # é‡ã¿ä»˜ãæŠ•ç¥¨
        weighted_sum = sum(
            prediction.prediction * model_weights[model_type]
            for model_type, prediction in model_predictions.items()
        )

        final_prediction = 1 if weighted_sum >= 0.5 else 0

        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence_scores = [pred.confidence for pred in model_predictions.values()]
        final_confidence = np.mean(confidence_scores)

        # ãƒ¢ãƒ‡ãƒ«åˆæ„åº¦è¨ˆç®—
        predictions_list = [pred.prediction for pred in model_predictions.values()]
        consensus_rate = predictions_list.count(final_prediction) / len(predictions_list)

        # ç‰¹å¾´é‡è²¢çŒ®åº¦çµ±åˆ
        feature_contributions = {}
        for prediction in model_predictions.values():
            for feature, importance in prediction.feature_importance.items():
                if feature not in feature_contributions:
                    feature_contributions[feature] = 0
                feature_contributions[feature] += importance * model_weights[prediction.model_type]

        # ãƒªã‚¹ã‚¯è©•ä¾¡
        risk_assessment = self._assess_prediction_risk(
            final_confidence, consensus_rate, len(model_predictions)
        )

        # èª¬æ˜ç”Ÿæˆ
        explanation = self._generate_explanation(
            final_prediction, final_confidence, model_predictions, feature_contributions
        )

        # ãƒ¢ãƒ‡ãƒ«åˆæ„æƒ…å ±
        model_consensus = {}
        for model_type, prediction in model_predictions.items():
            model_consensus[model_type.value] = {
                'prediction': prediction.prediction,
                'confidence': prediction.confidence,
                'weight': model_weights[model_type],
                'accuracy': prediction.prediction_details.get('accuracy', 0.5)
            }

        return IntegratedPrediction(
            symbol=symbol,
            final_prediction=final_prediction,
            confidence=final_confidence,
            model_consensus=model_consensus,
            feature_contributions=feature_contributions,
            risk_assessment=risk_assessment,
            explanation=explanation,
            timestamp=datetime.now()
        )

    def _assess_prediction_risk(self, confidence: float, consensus_rate: float, model_count: int) -> Dict[str, float]:
        """äºˆæ¸¬ãƒªã‚¹ã‚¯è©•ä¾¡"""

        # ä¿¡é ¼åº¦ãƒªã‚¹ã‚¯
        confidence_risk = 1.0 - confidence

        # åˆæ„åº¦ãƒªã‚¹ã‚¯
        consensus_risk = 1.0 - consensus_rate

        # ãƒ¢ãƒ‡ãƒ«æ•°ãƒªã‚¹ã‚¯
        model_diversity_risk = max(0, (5 - model_count) / 5)

        # ç·åˆãƒªã‚¹ã‚¯
        overall_risk = (confidence_risk * 0.5 + consensus_risk * 0.3 + model_diversity_risk * 0.2)

        risk_level = "low" if overall_risk < 0.3 else "medium" if overall_risk < 0.6 else "high"

        return {
            'overall_risk': overall_risk,
            'confidence_risk': confidence_risk,
            'consensus_risk': consensus_risk,
            'model_diversity_risk': model_diversity_risk,
            'risk_level': risk_level
        }

    def _generate_explanation(self, final_prediction: int, confidence: float,
                            model_predictions: Dict[ModelType, ModelPrediction],
                            feature_contributions: Dict[str, float]) -> List[str]:
        """èª¬æ˜ç”Ÿæˆ"""

        explanation = []

        # åŸºæœ¬äºˆæ¸¬æƒ…å ±
        direction = "ä¸Šæ˜‡" if final_prediction == 1 else "ä¸‹é™"
        explanation.append(f"çµ±åˆäºˆæ¸¬: {direction} (ä¿¡é ¼åº¦: {confidence:.1%})")

        # ãƒ¢ãƒ‡ãƒ«åˆæ„æƒ…å ±
        agree_count = sum(1 for pred in model_predictions.values() if pred.prediction == final_prediction)
        total_count = len(model_predictions)
        explanation.append(f"ãƒ¢ãƒ‡ãƒ«åˆæ„: {agree_count}/{total_count}ãƒ¢ãƒ‡ãƒ«ãŒ{direction}äºˆæ¸¬")

        # ä¸»è¦ãƒ¢ãƒ‡ãƒ«æƒ…å ±
        best_model = max(model_predictions.values(), key=lambda x: x.confidence)
        explanation.append(f"æœ€é«˜ä¿¡é ¼åº¦ãƒ¢ãƒ‡ãƒ«: {best_model.model_type.value} ({best_model.confidence:.1%})")

        # ä¸»è¦ç‰¹å¾´é‡
        if feature_contributions:
            top_features = sorted(feature_contributions.items(), key=lambda x: abs(x[1]), reverse=True)[:3]
            feature_names = [f"{name}({contribution:.3f})" for name, contribution in top_features]
            explanation.append(f"ä¸»è¦è¦å› : {', '.join(feature_names)}")

        # ãƒªã‚¹ã‚¯è­¦å‘Š
        if confidence < 0.6:
            explanation.append("âš ï¸ ä½ä¿¡é ¼åº¦äºˆæ¸¬ã®ãŸã‚æ³¨æ„ãŒå¿…è¦")

        return explanation

    async def _save_model_performance(self, symbol: str, trained_models: Dict[ModelType, Any], sample_count: int):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                for model_type, model_info in trained_models.items():
                    cursor.execute('''
                        INSERT INTO model_performance
                        (symbol, model_type, accuracy, training_samples, feature_count, timestamp)
                        VALUES (?, ?, ?, ?, ?, ?)
                    ''', (
                        symbol,
                        model_type.value,
                        model_info['accuracy'],
                        sample_count,
                        len(model_info['features']),
                        datetime.now().isoformat()
                    ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    async def _save_integrated_prediction(self, prediction: IntegratedPrediction):
        """çµ±åˆäºˆæ¸¬ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                cursor.execute('''
                    INSERT INTO integrated_predictions
                    (symbol, final_prediction, confidence, model_consensus,
                     feature_contributions, risk_assessment, explanation, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    prediction.symbol,
                    prediction.final_prediction,
                    prediction.confidence,
                    json.dumps(prediction.model_consensus),
                    json.dumps(prediction.feature_contributions),
                    json.dumps(prediction.risk_assessment),
                    json.dumps(prediction.explanation),
                    prediction.timestamp.isoformat()
                ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"çµ±åˆäºˆæ¸¬ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
ai_model_data_integration = AIModelDataIntegration()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def run_ai_integration_test():
    """AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("=== ğŸ¤– AIãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")

    test_symbols = ["7203", "8306"]

    for symbol in test_symbols:
        print(f"\nğŸ”¬ {symbol} çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆ")

        try:
            # çµ±åˆã‚·ã‚¹ãƒ†ãƒ è¨“ç·´
            training_result = await ai_model_data_integration.train_integrated_system(symbol)

            print(f"  âœ… è¨“ç·´å®Œäº†:")
            print(f"    è¨“ç·´ãƒ¢ãƒ‡ãƒ«æ•°: {training_result['models_trained']}")
            print(f"    è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«æ•°: {training_result['training_samples']}")
            print(f"    ç‰¹å¾´é‡æ•°: {training_result['feature_count']}")

            # ãƒ¢ãƒ‡ãƒ«ç²¾åº¦è¡¨ç¤º
            for model_type, accuracy in training_result['model_accuracies'].items():
                print(f"    {model_type}: {accuracy:.3f}")

            # çµ±åˆäºˆæ¸¬ãƒ†ã‚¹ãƒˆ
            prediction = await ai_model_data_integration.predict_integrated(symbol)

            print(f"  ğŸ¯ çµ±åˆäºˆæ¸¬çµæœ:")
            print(f"    äºˆæ¸¬: {'ä¸Šæ˜‡' if prediction.final_prediction else 'ä¸‹é™'}")
            print(f"    ä¿¡é ¼åº¦: {prediction.confidence:.1%}")
            print(f"    ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {prediction.risk_assessment['risk_level']}")
            print(f"    ãƒ¢ãƒ‡ãƒ«æ•°: {len(prediction.model_consensus)}")

            # èª¬æ˜è¡¨ç¤º
            print(f"    èª¬æ˜:")
            for explanation in prediction.explanation[:3]:
                print(f"      - {explanation}")

        except Exception as e:
            print(f"  âŒ {symbol} ã‚¨ãƒ©ãƒ¼: {e}")

    print(f"\nâœ… AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(run_ai_integration_test())