name: Performance Regression Testing

on:
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'pyproject.toml'
      - 'requirements.txt'
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # 毎日2時に実行
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  performance-baseline:
    runs-on: ubuntu-latest
    outputs:
      baseline-results: ${{ steps.baseline.outputs.results }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark psutil memory_profiler

      - name: Run baseline benchmarks
        id: baseline
        run: |
          python -c "
          import json
          import time
          import psutil
          import sys
          sys.path.insert(0, 'src')

          # パフォーマンステスト
          results = {}
          process = psutil.Process()

          # 1. 特徴量生成速度テスト
          start_time = time.time()
          start_memory = process.memory_info().rss / 1024 / 1024

          try:
              from typing import List
              from day_trade.ml.feature_pipeline import FeaturePipeline
              from day_trade.analysis.feature_engineering_unified import FeatureConfig
              pipeline = FeaturePipeline()
              # テストデータでのベンチマーク
              import pandas as pd
              test_data = pd.DataFrame({
                  'timestamp': pd.date_range('2024-01-01', periods=1000, freq='1min'),
                  'price': [100 + i * 0.1 for i in range(1000)],
                  'volume': [1000 + i * 10 for i in range(1000)]
              })
              # 適切なメソッド引数でテスト
              symbols_data = {'TEST': test_data}
              feature_config = FeatureConfig(
                  lookback_periods=[5, 10, 20],
                  volatility_windows=[5, 10],
                  momentum_periods=[5, 10, 20]
              )
              features = pipeline.batch_generate_features(symbols_data, feature_config)
              feature_gen_time = time.time() - start_time
              results['feature_generation_time'] = feature_gen_time
              results['feature_count'] = len(features) if features else 0
          except Exception as e:
              results['feature_generation_time'] = -1
              results['feature_generation_error'] = str(e)

          # 2. メモリ使用量テスト
          end_memory = process.memory_info().rss / 1024 / 1024
          results['memory_usage_mb'] = end_memory - start_memory
          results['peak_memory_mb'] = end_memory

          # 3. データI/O性能テスト
          start_time = time.time()
          try:
              import sqlite3
              import tempfile
              import os
              with tempfile.NamedTemporaryFile(delete=False) as tmp:
                  conn = sqlite3.connect(tmp.name)
                  cursor = conn.cursor()
                  cursor.execute('CREATE TABLE test (id INTEGER PRIMARY KEY, data TEXT)')
                  for i in range(1000):
                      cursor.execute('INSERT INTO test (data) VALUES (?)', (f'test_data_{i}',))
                  conn.commit()
                  conn.close()
                  os.unlink(tmp.name)
              results['db_io_time'] = time.time() - start_time
          except Exception as e:
              results['db_io_time'] = -1
              results['db_io_error'] = str(e)

          # 4. API応答時間シミュレーション
          start_time = time.time()
          try:
              # 軽量な計算処理での応答時間測定
              for i in range(100):
                  _ = sum(j for j in range(100))
              results['api_response_time'] = (time.time() - start_time) / 100
          except Exception as e:
              results['api_response_time'] = -1
              results['api_response_error'] = str(e)

          print(json.dumps(results, indent=2))
          with open('baseline_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          "

      - name: Upload baseline results
        uses: actions/upload-artifact@v4
        with:
          name: baseline-performance-results
          path: baseline_results.json

  performance-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: performance-baseline
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark psutil memory_profiler

      - name: Download baseline results
        uses: actions/download-artifact@v4
        with:
          name: baseline-performance-results
          path: baseline/

      - name: Run current benchmarks
        id: current
        run: |
          python -c "
          import json
          import time
          import psutil
          import sys
          sys.path.insert(0, 'src')

          # パフォーマンステスト（ベースラインと同じテスト）
          results = {}
          process = psutil.Process()

          # 1. 特徴量生成速度テスト
          start_time = time.time()
          start_memory = process.memory_info().rss / 1024 / 1024

          try:
              from typing import List
              from day_trade.ml.feature_pipeline import FeaturePipeline
              from day_trade.analysis.feature_engineering_unified import FeatureConfig
              pipeline = FeaturePipeline()
              import pandas as pd
              test_data = pd.DataFrame({
                  'timestamp': pd.date_range('2024-01-01', periods=1000, freq='1min'),
                  'price': [100 + i * 0.1 for i in range(1000)],
                  'volume': [1000 + i * 10 for i in range(1000)]
              })
              # 適切なメソッド引数でテスト
              symbols_data = {'TEST': test_data}
              feature_config = FeatureConfig(
                  lookback_periods=[5, 10, 20],
                  volatility_windows=[5, 10],
                  momentum_periods=[5, 10, 20]
              )
              features = pipeline.batch_generate_features(symbols_data, feature_config)
              feature_gen_time = time.time() - start_time
              results['feature_generation_time'] = feature_gen_time
              results['feature_count'] = len(features) if features else 0
          except Exception as e:
              results['feature_generation_time'] = -1
              results['feature_generation_error'] = str(e)

          # 2. メモリ使用量テスト
          end_memory = process.memory_info().rss / 1024 / 1024
          results['memory_usage_mb'] = end_memory - start_memory
          results['peak_memory_mb'] = end_memory

          # 3. データI/O性能テスト
          start_time = time.time()
          try:
              import sqlite3
              import tempfile
              import os
              with tempfile.NamedTemporaryFile(delete=False) as tmp:
                  conn = sqlite3.connect(tmp.name)
                  cursor = conn.cursor()
                  cursor.execute('CREATE TABLE test (id INTEGER PRIMARY KEY, data TEXT)')
                  for i in range(1000):
                      cursor.execute('INSERT INTO test (data) VALUES (?)', (f'test_data_{i}',))
                  conn.commit()
                  conn.close()
                  os.unlink(tmp.name)
              results['db_io_time'] = time.time() - start_time
          except Exception as e:
              results['db_io_time'] = -1
              results['db_io_error'] = str(e)

          # 4. API応答時間シミュレーション
          start_time = time.time()
          try:
              for i in range(100):
                  _ = sum(j for j in range(100))
              results['api_response_time'] = (time.time() - start_time) / 100
          except Exception as e:
              results['api_response_time'] = -1
              results['api_response_error'] = str(e)

          print(json.dumps(results, indent=2))
          with open('current_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          "

      - name: Compare performance and create report
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // ベースラインと現在の結果を読み込み
            const baseline = JSON.parse(fs.readFileSync('baseline/baseline_results.json', 'utf8'));
            const current = JSON.parse(fs.readFileSync('current_results.json', 'utf8'));

            // パフォーマンス比較
            function compareMetrics(baselineValue, currentValue, metricName, lowerIsBetter = true) {
              if (baselineValue <= 0 || currentValue <= 0) {
                return `❓ ${metricName}: データ不足（baseline: ${baselineValue}, current: ${currentValue})`;
              }

              const change = ((currentValue - baselineValue) / baselineValue) * 100;
              const absChange = Math.abs(change);

              let emoji = '✅';
              let status = 'GOOD';

              // 閾値設定
              const warningThreshold = 5;  // 5%の変化で警告
              const criticalThreshold = 20; // 20%の変化でクリティカル

              if (lowerIsBetter) {
                if (change > criticalThreshold) {
                  emoji = '🚨';
                  status = 'CRITICAL';
                } else if (change > warningThreshold) {
                  emoji = '⚠️';
                  status = 'WARNING';
                } else if (change < -warningThreshold) {
                  emoji = '🎉';
                  status = 'IMPROVED';
                }
              } else {
                if (change < -criticalThreshold) {
                  emoji = '🚨';
                  status = 'CRITICAL';
                } else if (change < -warningThreshold) {
                  emoji = '⚠️';
                  status = 'WARNING';
                } else if (change > warningThreshold) {
                  emoji = '🎉';
                  status = 'IMPROVED';
                }
              }

              const changeStr = change >= 0 ? `+${change.toFixed(2)}%` : `${change.toFixed(2)}%`;
              return `${emoji} ${metricName}: ${currentValue.toFixed(4)}s (${changeStr}) - ${status}`;
            }

            // レポート生成
            let report = `## 📊 パフォーマンス回帰テスト結果\n\n`;
            report += `### 📈 メトリクス比較\n\n`;

            // 各メトリクスの比較
            if (baseline.feature_generation_time > 0 && current.feature_generation_time > 0) {
              report += compareMetrics(baseline.feature_generation_time, current.feature_generation_time, '特徴量生成時間') + '\n';
            }

            if (baseline.db_io_time > 0 && current.db_io_time > 0) {
              report += compareMetrics(baseline.db_io_time, current.db_io_time, 'データI/O時間') + '\n';
            }

            if (baseline.api_response_time > 0 && current.api_response_time > 0) {
              report += compareMetrics(baseline.api_response_time, current.api_response_time, 'API応答時間') + '\n';
            }

            if (baseline.memory_usage_mb > 0 && current.memory_usage_mb > 0) {
              report += compareMetrics(baseline.memory_usage_mb, current.memory_usage_mb, 'メモリ使用量 (MB)') + '\n';
            }

            report += `\n### 📋 詳細データ\n\n`;
            report += `| メトリクス | ベースライン | 現在 | 変化率 |\n`;
            report += `|-----------|------------|------|--------|\n`;

            const metrics = [
              ['特徴量生成時間 (s)', 'feature_generation_time'],
              ['データI/O時間 (s)', 'db_io_time'],
              ['API応答時間 (s)', 'api_response_time'],
              ['メモリ使用量 (MB)', 'memory_usage_mb'],
              ['ピークメモリ (MB)', 'peak_memory_mb'],
              ['特徴量数', 'feature_count']
            ];

            metrics.forEach(([name, key]) => {
              const baseVal = baseline[key] || 'N/A';
              const currVal = current[key] || 'N/A';
              let change = 'N/A';

              if (typeof baseVal === 'number' && typeof currVal === 'number' && baseVal > 0) {
                const changeVal = ((currVal - baseVal) / baseVal) * 100;
                change = `${changeVal >= 0 ? '+' : ''}${changeVal.toFixed(2)}%`;
              }

              report += `| ${name} | ${baseVal} | ${currVal} | ${change} |\n`;
            });

            report += `\n### 🎯 推奨アクション\n\n`;

            // クリティカルな劣化がある場合の推奨アクション
            let hasRegression = false;
            ['feature_generation_time', 'db_io_time', 'api_response_time', 'memory_usage_mb'].forEach(key => {
              if (baseline[key] > 0 && current[key] > 0) {
                const change = ((current[key] - baseline[key]) / baseline[key]) * 100;
                if (change > 20) {
                  hasRegression = true;
                }
              }
            });

            if (hasRegression) {
              report += `🚨 **重大なパフォーマンス劣化を検出しました**\n`;
              report += `- プロファイリングツールでボトルネックを特定してください\n`;
              report += `- メモリリークの可能性を調査してください\n`;
              report += `- アルゴリズムの最適化を検討してください\n\n`;
            } else {
              report += `✅ **パフォーマンスは許容範囲内です**\n`;
              report += `- 継続的な監視を維持してください\n`;
              report += `- さらなる最適化の機会を探してください\n\n`;
            }

            report += `---\n*このレポートは自動生成されました。${new Date().toISOString()}*`;

            // PRにコメントを投稿
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

            // 重大な劣化がある場合はチェックを失敗させる
            if (hasRegression) {
              core.setFailed('重大なパフォーマンス劣化が検出されました。');
            }

  store-results:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: performance-baseline
    steps:
      - uses: actions/checkout@v4

      - name: Download results
        uses: actions/download-artifact@v4
        with:
          name: baseline-performance-results

      - name: Store performance history
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // 結果を読み込み
            const results = JSON.parse(fs.readFileSync('baseline_results.json', 'utf8'));

            // GitHubのissueやPRにパフォーマンス履歴を記録
            const historyComment = `## 📊 パフォーマンス履歴記録

            **実行日時**: ${new Date().toISOString()}
            **コミット**: ${context.sha.substring(0, 7)}

            ### メトリクス
            - 特徴量生成時間: ${results.feature_generation_time?.toFixed(4)}s
            - データI/O時間: ${results.db_io_time?.toFixed(4)}s
            - API応答時間: ${results.api_response_time?.toFixed(6)}s
            - メモリ使用量: ${results.memory_usage_mb?.toFixed(2)}MB
            - ピークメモリ: ${results.peak_memory_mb?.toFixed(2)}MB
            - 特徴量数: ${results.feature_count}

            *自動記録 - Performance Regression Testing*`;

            // パフォーマンス履歴用のissueを検索
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: ['performance-history'],
              state: 'open'
            });

            if (issues.data.length > 0) {
              // 既存の履歴issueにコメント追加
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: historyComment
              });
            } else {
              // 履歴記録用のissueを作成
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: '📊 Performance History Tracking',
                body: `このイシューはパフォーマンス回帰テストの履歴を記録するために使用されます。\n\n${historyComment}`,
                labels: ['performance-history', 'automated']
              });
            }
