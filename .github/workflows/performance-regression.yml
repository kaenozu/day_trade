name: Performance Regression Testing

on:
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/**'
      - 'pyproject.toml'
      - 'requirements.txt'
  push:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # æ¯æ—¥2æ™‚ã«å®Ÿè¡Œ
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  performance-baseline:
    runs-on: ubuntu-latest
    outputs:
      baseline-results: ${{ steps.baseline.outputs.results }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: main

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark psutil memory_profiler

      - name: Run baseline benchmarks
        id: baseline
        run: |
          python -c "
          import json
          import time
          import psutil
          import sys
          sys.path.insert(0, 'src')

          # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
          results = {}
          process = psutil.Process()

          # 1. ç‰¹å¾´é‡ç”Ÿæˆé€Ÿåº¦ãƒ†ã‚¹ãƒˆ
          start_time = time.time()
          start_memory = process.memory_info().rss / 1024 / 1024

          try:
              from typing import List
              from day_trade.ml.feature_pipeline import FeaturePipeline
              from day_trade.analysis.feature_engineering_unified import FeatureConfig
              pipeline = FeaturePipeline()
              # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
              import pandas as pd
              test_data = pd.DataFrame({
                  'timestamp': pd.date_range('2024-01-01', periods=1000, freq='1min'),
                  'price': [100 + i * 0.1 for i in range(1000)],
                  'volume': [1000 + i * 10 for i in range(1000)]
              })
              # é©åˆ‡ãªãƒ¡ã‚½ãƒƒãƒ‰å¼•æ•°ã§ãƒ†ã‚¹ãƒˆ
              symbols_data = {'TEST': test_data}
              feature_config = FeatureConfig(
                  lookback_periods=[5, 10, 20],
                  volatility_windows=[5, 10],
                  momentum_periods=[5, 10, 20]
              )
              features = pipeline.batch_generate_features(symbols_data, feature_config)
              feature_gen_time = time.time() - start_time
              results['feature_generation_time'] = feature_gen_time
              results['feature_count'] = len(features) if features else 0
          except Exception as e:
              results['feature_generation_time'] = -1
              results['feature_generation_error'] = str(e)

          # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
          end_memory = process.memory_info().rss / 1024 / 1024
          results['memory_usage_mb'] = end_memory - start_memory
          results['peak_memory_mb'] = end_memory

          # 3. ãƒ‡ãƒ¼ã‚¿I/Oæ€§èƒ½ãƒ†ã‚¹ãƒˆ
          start_time = time.time()
          try:
              import sqlite3
              import tempfile
              import os
              with tempfile.NamedTemporaryFile(delete=False) as tmp:
                  conn = sqlite3.connect(tmp.name)
                  cursor = conn.cursor()
                  cursor.execute('CREATE TABLE test (id INTEGER PRIMARY KEY, data TEXT)')
                  for i in range(1000):
                      cursor.execute('INSERT INTO test (data) VALUES (?)', (f'test_data_{i}',))
                  conn.commit()
                  conn.close()
                  os.unlink(tmp.name)
              results['db_io_time'] = time.time() - start_time
          except Exception as e:
              results['db_io_time'] = -1
              results['db_io_error'] = str(e)

          # 4. APIå¿œç­”æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
          start_time = time.time()
          try:
              # è»½é‡ãªè¨ˆç®—å‡¦ç†ã§ã®å¿œç­”æ™‚é–“æ¸¬å®š
              for i in range(100):
                  _ = sum(j for j in range(100))
              results['api_response_time'] = (time.time() - start_time) / 100
          except Exception as e:
              results['api_response_time'] = -1
              results['api_response_error'] = str(e)

          print(json.dumps(results, indent=2))
          with open('baseline_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          "

      - name: Upload baseline results
        uses: actions/upload-artifact@v4
        with:
          name: baseline-performance-results
          path: baseline_results.json

  performance-test:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: performance-baseline
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark psutil memory_profiler

      - name: Download baseline results
        uses: actions/download-artifact@v4
        with:
          name: baseline-performance-results
          path: baseline/

      - name: Run current benchmarks
        id: current
        run: |
          python -c "
          import json
          import time
          import psutil
          import sys
          sys.path.insert(0, 'src')

          # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒã˜ãƒ†ã‚¹ãƒˆï¼‰
          results = {}
          process = psutil.Process()

          # 1. ç‰¹å¾´é‡ç”Ÿæˆé€Ÿåº¦ãƒ†ã‚¹ãƒˆ
          start_time = time.time()
          start_memory = process.memory_info().rss / 1024 / 1024

          try:
              from typing import List
              from day_trade.ml.feature_pipeline import FeaturePipeline
              from day_trade.analysis.feature_engineering_unified import FeatureConfig
              pipeline = FeaturePipeline()
              import pandas as pd
              test_data = pd.DataFrame({
                  'timestamp': pd.date_range('2024-01-01', periods=1000, freq='1min'),
                  'price': [100 + i * 0.1 for i in range(1000)],
                  'volume': [1000 + i * 10 for i in range(1000)]
              })
              # é©åˆ‡ãªãƒ¡ã‚½ãƒƒãƒ‰å¼•æ•°ã§ãƒ†ã‚¹ãƒˆ
              symbols_data = {'TEST': test_data}
              feature_config = FeatureConfig(
                  lookback_periods=[5, 10, 20],
                  volatility_windows=[5, 10],
                  momentum_periods=[5, 10, 20]
              )
              features = pipeline.batch_generate_features(symbols_data, feature_config)
              feature_gen_time = time.time() - start_time
              results['feature_generation_time'] = feature_gen_time
              results['feature_count'] = len(features) if features else 0
          except Exception as e:
              results['feature_generation_time'] = -1
              results['feature_generation_error'] = str(e)

          # 2. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
          end_memory = process.memory_info().rss / 1024 / 1024
          results['memory_usage_mb'] = end_memory - start_memory
          results['peak_memory_mb'] = end_memory

          # 3. ãƒ‡ãƒ¼ã‚¿I/Oæ€§èƒ½ãƒ†ã‚¹ãƒˆ
          start_time = time.time()
          try:
              import sqlite3
              import tempfile
              import os
              with tempfile.NamedTemporaryFile(delete=False) as tmp:
                  conn = sqlite3.connect(tmp.name)
                  cursor = conn.cursor()
                  cursor.execute('CREATE TABLE test (id INTEGER PRIMARY KEY, data TEXT)')
                  for i in range(1000):
                      cursor.execute('INSERT INTO test (data) VALUES (?)', (f'test_data_{i}',))
                  conn.commit()
                  conn.close()
                  os.unlink(tmp.name)
              results['db_io_time'] = time.time() - start_time
          except Exception as e:
              results['db_io_time'] = -1
              results['db_io_error'] = str(e)

          # 4. APIå¿œç­”æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
          start_time = time.time()
          try:
              for i in range(100):
                  _ = sum(j for j in range(100))
              results['api_response_time'] = (time.time() - start_time) / 100
          except Exception as e:
              results['api_response_time'] = -1
              results['api_response_error'] = str(e)

          print(json.dumps(results, indent=2))
          with open('current_results.json', 'w') as f:
              json.dump(results, f, indent=2)
          "

      - name: Compare performance and create report
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç¾åœ¨ã®çµæœã‚’èª­ã¿è¾¼ã¿
            const baseline = JSON.parse(fs.readFileSync('baseline/baseline_results.json', 'utf8'));
            const current = JSON.parse(fs.readFileSync('current_results.json', 'utf8'));

            // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
            function compareMetrics(baselineValue, currentValue, metricName, lowerIsBetter = true) {
              if (baselineValue <= 0 || currentValue <= 0) {
                return `â“ ${metricName}: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ï¼ˆbaseline: ${baselineValue}, current: ${currentValue})`;
              }

              const change = ((currentValue - baselineValue) / baselineValue) * 100;
              const absChange = Math.abs(change);

              let emoji = 'âœ…';
              let status = 'GOOD';

              // é–¾å€¤è¨­å®š
              const warningThreshold = 5;  // 5%ã®å¤‰åŒ–ã§è­¦å‘Š
              const criticalThreshold = 20; // 20%ã®å¤‰åŒ–ã§ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«

              if (lowerIsBetter) {
                if (change > criticalThreshold) {
                  emoji = 'ğŸš¨';
                  status = 'CRITICAL';
                } else if (change > warningThreshold) {
                  emoji = 'âš ï¸';
                  status = 'WARNING';
                } else if (change < -warningThreshold) {
                  emoji = 'ğŸ‰';
                  status = 'IMPROVED';
                }
              } else {
                if (change < -criticalThreshold) {
                  emoji = 'ğŸš¨';
                  status = 'CRITICAL';
                } else if (change < -warningThreshold) {
                  emoji = 'âš ï¸';
                  status = 'WARNING';
                } else if (change > warningThreshold) {
                  emoji = 'ğŸ‰';
                  status = 'IMPROVED';
                }
              }

              const changeStr = change >= 0 ? `+${change.toFixed(2)}%` : `${change.toFixed(2)}%`;
              return `${emoji} ${metricName}: ${currentValue.toFixed(4)}s (${changeStr}) - ${status}`;
            }

            // ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            let report = `## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆçµæœ\n\n`;
            report += `### ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ\n\n`;

            // å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¯”è¼ƒ
            if (baseline.feature_generation_time > 0 && current.feature_generation_time > 0) {
              report += compareMetrics(baseline.feature_generation_time, current.feature_generation_time, 'ç‰¹å¾´é‡ç”Ÿæˆæ™‚é–“') + '\n';
            }

            if (baseline.db_io_time > 0 && current.db_io_time > 0) {
              report += compareMetrics(baseline.db_io_time, current.db_io_time, 'ãƒ‡ãƒ¼ã‚¿I/Oæ™‚é–“') + '\n';
            }

            if (baseline.api_response_time > 0 && current.api_response_time > 0) {
              report += compareMetrics(baseline.api_response_time, current.api_response_time, 'APIå¿œç­”æ™‚é–“') + '\n';
            }

            if (baseline.memory_usage_mb > 0 && current.memory_usage_mb > 0) {
              report += compareMetrics(baseline.memory_usage_mb, current.memory_usage_mb, 'ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)') + '\n';
            }

            report += `\n### ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿\n\n`;
            report += `| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | ç¾åœ¨ | å¤‰åŒ–ç‡ |\n`;
            report += `|-----------|------------|------|--------|\n`;

            const metrics = [
              ['ç‰¹å¾´é‡ç”Ÿæˆæ™‚é–“ (s)', 'feature_generation_time'],
              ['ãƒ‡ãƒ¼ã‚¿I/Oæ™‚é–“ (s)', 'db_io_time'],
              ['APIå¿œç­”æ™‚é–“ (s)', 'api_response_time'],
              ['ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (MB)', 'memory_usage_mb'],
              ['ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª (MB)', 'peak_memory_mb'],
              ['ç‰¹å¾´é‡æ•°', 'feature_count']
            ];

            metrics.forEach(([name, key]) => {
              const baseVal = baseline[key] || 'N/A';
              const currVal = current[key] || 'N/A';
              let change = 'N/A';

              if (typeof baseVal === 'number' && typeof currVal === 'number' && baseVal > 0) {
                const changeVal = ((currVal - baseVal) / baseVal) * 100;
                change = `${changeVal >= 0 ? '+' : ''}${changeVal.toFixed(2)}%`;
              }

              report += `| ${name} | ${baseVal} | ${currVal} | ${change} |\n`;
            });

            report += `\n### ğŸ¯ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³\n\n`;

            // ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªåŠ£åŒ–ãŒã‚ã‚‹å ´åˆã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            let hasRegression = false;
            ['feature_generation_time', 'db_io_time', 'api_response_time', 'memory_usage_mb'].forEach(key => {
              if (baseline[key] > 0 && current[key] > 0) {
                const change = ((current[key] - baseline[key]) / baseline[key]) * 100;
                if (change > 20) {
                  hasRegression = true;
                }
              }
            });

            if (hasRegression) {
              report += `ğŸš¨ **é‡å¤§ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’æ¤œå‡ºã—ã¾ã—ãŸ**\n`;
              report += `- ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦ãã ã•ã„\n`;
              report += `- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„\n`;
              report += `- ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„\n\n`;
            } else {
              report += `âœ… **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯è¨±å®¹ç¯„å›²å†…ã§ã™**\n`;
              report += `- ç¶™ç¶šçš„ãªç›£è¦–ã‚’ç¶­æŒã—ã¦ãã ã•ã„\n`;
              report += `- ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã®æ©Ÿä¼šã‚’æ¢ã—ã¦ãã ã•ã„\n\n`;
            }

            report += `---\n*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚${new Date().toISOString()}*`;

            // PRã«ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ•ç¨¿
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

            // é‡å¤§ãªåŠ£åŒ–ãŒã‚ã‚‹å ´åˆã¯ãƒã‚§ãƒƒã‚¯ã‚’å¤±æ•—ã•ã›ã‚‹
            if (hasRegression) {
              core.setFailed('é‡å¤§ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚');
            }

  store-results:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: performance-baseline
    steps:
      - uses: actions/checkout@v4

      - name: Download results
        uses: actions/download-artifact@v4
        with:
          name: baseline-performance-results

      - name: Store performance history
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // çµæœã‚’èª­ã¿è¾¼ã¿
            const results = JSON.parse(fs.readFileSync('baseline_results.json', 'utf8'));

            // GitHubã®issueã‚„PRã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ã‚’è¨˜éŒ²
            const historyComment = `## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´è¨˜éŒ²

            **å®Ÿè¡Œæ—¥æ™‚**: ${new Date().toISOString()}
            **ã‚³ãƒŸãƒƒãƒˆ**: ${context.sha.substring(0, 7)}

            ### ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            - ç‰¹å¾´é‡ç”Ÿæˆæ™‚é–“: ${results.feature_generation_time?.toFixed(4)}s
            - ãƒ‡ãƒ¼ã‚¿I/Oæ™‚é–“: ${results.db_io_time?.toFixed(4)}s
            - APIå¿œç­”æ™‚é–“: ${results.api_response_time?.toFixed(6)}s
            - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: ${results.memory_usage_mb?.toFixed(2)}MB
            - ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: ${results.peak_memory_mb?.toFixed(2)}MB
            - ç‰¹å¾´é‡æ•°: ${results.feature_count}

            *è‡ªå‹•è¨˜éŒ² - Performance Regression Testing*`;

            // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å±¥æ­´ç”¨ã®issueã‚’æ¤œç´¢
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: ['performance-history'],
              state: 'open'
            });

            if (issues.data.length > 0) {
              // æ—¢å­˜ã®å±¥æ­´issueã«ã‚³ãƒ¡ãƒ³ãƒˆè¿½åŠ 
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues.data[0].number,
                body: historyComment
              });
            } else {
              // å±¥æ­´è¨˜éŒ²ç”¨ã®issueã‚’ä½œæˆ
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'ğŸ“Š Performance History Tracking',
                body: `ã“ã®ã‚¤ã‚·ãƒ¥ãƒ¼ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆã®å±¥æ­´ã‚’è¨˜éŒ²ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n\n${historyComment}`,
                labels: ['performance-history', 'automated']
              });
            }
