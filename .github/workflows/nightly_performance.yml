name: Nightly Performance Testing

on:
  schedule:
    # æ¯æ—¥åˆå‰3æ™‚ã«å®Ÿè¡Œ
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      benchmark_suite:
        description: 'Benchmark Suite'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - inference
        - memory
        - concurrency
        - throughput
      comparison_baseline:
        description: 'Comparison Baseline'
        required: false
        default: 'previous'
        type: choice
        options:
        - previous
        - release
        - custom

env:
  PYTHON_VERSION: '3.9'
  BENCHMARK_TIMEOUT: 7200  # 2 hours
  PERFORMANCE_BASELINE_DIR: tests/baselines

jobs:
  # å¤œé–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  nightly-benchmarks:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        benchmark_type: [inference, memory, concurrency, throughput, system]
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # å±¥æ­´ã‚’å–å¾—ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark pytest-xdist

    - name: Setup performance testing environment
      run: |
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®š
        echo "Setting up performance testing environment"

        # CPU governorè¨­å®š
        echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor || true

        # ã‚¹ãƒ¯ãƒƒãƒ—ç„¡åŠ¹åŒ–
        sudo swapoff -a || true

        # ãƒ¡ãƒ¢ãƒªè¨­å®š
        echo 3 | sudo tee /proc/sys/vm/drop_caches

    - name: Download previous baseline
      run: |
        # å‰å›ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å–å¾—
        mkdir -p ${{ env.PERFORMANCE_BASELINE_DIR }}

        # æœ€æ–°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        gh run list --workflow="Nightly Performance Testing" --status=success --limit=1 --json=databaseId \
          | jq -r '.[0].databaseId' \
          | xargs -I {} gh run download {} --pattern="*baseline*" --dir=${{ env.PERFORMANCE_BASELINE_DIR }} || true
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Run ${{ matrix.benchmark_type }} benchmarks
      env:
        BENCHMARK_TYPE: ${{ matrix.benchmark_type }}
        BENCHMARK_SUITE: ${{ github.event.inputs.benchmark_suite || 'all' }}
        PYTEST_WORKERS: 1  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¯å˜ä¸€ãƒ—ãƒ­ã‚»ã‚¹
      run: |
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        pytest tests/performance/test_performance_benchmarks.py \
          --benchmark-only \
          --benchmark-json=benchmark-${{ matrix.benchmark_type }}.json \
          --benchmark-histogram=histogram-${{ matrix.benchmark_type }} \
          --benchmark-save=${{ matrix.benchmark_type }}-$(date +%Y%m%d) \
          --timeout=${{ env.BENCHMARK_TIMEOUT }} \
          -v -k "${{ matrix.benchmark_type }}" \
          -m "performance and ${{ matrix.benchmark_type }}"

    - name: Generate performance analysis
      run: |
        python scripts/analyze_performance.py \
          --benchmark-file benchmark-${{ matrix.benchmark_type }}.json \
          --baseline-dir ${{ env.PERFORMANCE_BASELINE_DIR }} \
          --output-dir performance-analysis-${{ matrix.benchmark_type }} \
          --type ${{ matrix.benchmark_type }}

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.benchmark_type }}
        path: |
          benchmark-${{ matrix.benchmark_type }}.json
          histogram-${{ matrix.benchmark_type }}.*
          performance-analysis-${{ matrix.benchmark_type }}/
        retention-days: 90

  # Issue #761 æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ç‰¹åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
  inference-deep-benchmark:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run comprehensive inference benchmarks
      env:
        DEEP_BENCHMARK_MODE: true
        INFERENCE_BENCHMARK_ITERATIONS: 1000
        INFERENCE_BATCH_SIZES: "1,8,16,32,64,128"
        INFERENCE_CONCURRENCY_LEVELS: "1,5,10,20,50,100"
      run: |
        # æ¨è«–ã‚·ã‚¹ãƒ†ãƒ è©³ç´°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        pytest tests/performance/test_inference_deep_benchmarks.py \
          --benchmark-only \
          --benchmark-json=inference-deep-benchmark.json \
          --benchmark-histogram=inference-deep-histogram \
          --timeout=${{ env.BENCHMARK_TIMEOUT }} \
          -v -m "inference and deep"

    - name: Verify Issue #761 targets
      run: |
        python scripts/verify_issue_761_targets.py \
          --benchmark-file inference-deep-benchmark.json \
          --latency-target 5.0 \
          --throughput-target 10000.0 \
          --memory-reduction-target 0.5 \
          --accuracy-retention-target 0.97 \
          --output-report inference-target-verification.json

    - name: Generate inference performance report
      run: |
        python scripts/generate_inference_report.py \
          --benchmark-data inference-deep-benchmark.json \
          --verification-data inference-target-verification.json \
          --output-dir inference-performance-report \
          --format html,pdf,json

    - name: Upload inference benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: inference-deep-benchmark-results
        path: |
          inference-deep-benchmark.json
          inference-deep-histogram.*
          inference-target-verification.json
          inference-performance-report/
        retention-days: 90

  # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
  memory-leak-detection:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install memory-profiler psutil tracemalloc

    - name: Run memory leak detection
      run: |
        # é•·æ™‚é–“å®Ÿè¡Œã§ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
        python scripts/memory_leak_detector.py \
          --duration 3600 \
          --interval 30 \
          --threshold 100 \
          --output memory-leak-report.json

    - name: Memory profile inference system
      run: |
        # æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        python -m memory_profiler scripts/profile_inference_memory.py \
          --output inference-memory-profile.txt

    - name: Upload memory analysis
      uses: actions/upload-artifact@v3
      with:
        name: memory-analysis-results
        path: |
          memory-leak-report.json
          inference-memory-profile.txt
        retention-days: 90

  # é•·æ™‚é–“å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
  stability-testing:
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4æ™‚é–“
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run stability tests
      env:
        STABILITY_TEST_DURATION: 14400  # 4æ™‚é–“
        STABILITY_TEST_INTERVAL: 60     # 1åˆ†é–“éš”
      run: |
        # é•·æ™‚é–“å®‰å®šæ€§ãƒ†ã‚¹ãƒˆ
        pytest tests/stability/ \
          --duration=${{ env.STABILITY_TEST_DURATION }} \
          --interval=${{ env.STABILITY_TEST_INTERVAL }} \
          --junit-xml=stability-results.xml \
          -v -m "stability"

    - name: Generate stability report
      run: |
        python scripts/generate_stability_report.py \
          --test-results stability-results.xml \
          --output-dir stability-report \
          --format html,json

    - name: Upload stability results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: stability-test-results
        path: |
          stability-results.xml
          stability-report/
        retention-days: 90

  # çµæœé›†ç´„ã¨åˆ†æ
  performance-analysis:
    runs-on: ubuntu-latest
    needs: [nightly-benchmarks, inference-deep-benchmark, memory-leak-detection, stability-testing]
    if: always()
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all benchmark artifacts
      uses: actions/download-artifact@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install analysis dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
        pip install matplotlib seaborn plotly pandas numpy

    - name: Comprehensive performance analysis
      run: |
        python scripts/comprehensive_performance_analysis.py \
          --benchmark-dirs benchmark-results-* \
          --inference-dir inference-deep-benchmark-results \
          --memory-dir memory-analysis-results \
          --stability-dir stability-test-results \
          --output-dir comprehensive-performance-analysis \
          --format html,pdf,json

    - name: Performance regression detection
      run: |
        python scripts/detect_performance_regression.py \
          --current-benchmarks comprehensive-performance-analysis \
          --baseline-dir ${{ env.PERFORMANCE_BASELINE_DIR }} \
          --threshold 5.0 \
          --output regression-analysis.json

    - name: Generate performance dashboard
      run: |
        python scripts/generate_performance_dashboard.py \
          --data-dir comprehensive-performance-analysis \
          --output-dir performance-dashboard \
          --include-trends true \
          --include-comparisons true

    - name: Update performance baselines
      run: |
        # æˆåŠŸã—ãŸå ´åˆã®ã¿ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°
        python scripts/update_performance_baselines.py \
          --benchmark-dirs benchmark-results-* \
          --output-dir updated-baselines \
          --strategy conservative

    - name: Upload comprehensive analysis
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-performance-analysis
        path: |
          comprehensive-performance-analysis/
          regression-analysis.json
          performance-dashboard/
          updated-baselines/
        retention-days: 365  # 1å¹´ä¿æŒ

    - name: Upload new baselines
      uses: actions/upload-artifact@v3
      with:
        name: performance-baselines-${{ github.run_number }}
        path: updated-baselines/
        retention-days: 365

  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœé€šçŸ¥
  performance-notification:
    runs-on: ubuntu-latest
    needs: [performance-analysis]
    if: always()
    steps:
    - name: Download analysis results
      uses: actions/download-artifact@v3
      with:
        name: comprehensive-performance-analysis

    - name: Create performance issue on regression
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          // å›å¸°åˆ†æçµæœèª­ã¿è¾¼ã¿
          let regressionData = {};
          try {
            regressionData = JSON.parse(fs.readFileSync('regression-analysis.json', 'utf8'));
          } catch (e) {
            console.log('No regression analysis found');
            return;
          }

          if (regressionData.has_regression) {
            const title = `ğŸŒ Performance Regression Detected - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Performance Regression Report

            **Detection Date**: ${new Date().toISOString()}
            **Commit**: ${context.sha}
            **Run**: ${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}

            ### Regression Summary

            ${regressionData.regressions.map(r =>
              `- **${r.metric}**: ${r.change}% change (threshold: ${r.threshold}%)`
            ).join('\n')}

            ### Affected Areas

            ${regressionData.affected_areas.join(', ')}

            ### Recommended Actions

            1. Review recent changes in affected areas
            2. Run local performance tests
            3. Consider reverting problematic changes
            4. Update performance baselines if intentional

            ğŸ“Š [View Detailed Analysis](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})
            `;

            // æ—¢å­˜ã®å›å¸°Issueã‚’ãƒã‚§ãƒƒã‚¯
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'performance-regression'
            });

            const existingIssue = issues.data.find(issue =>
              issue.title.includes('Performance Regression Detected') &&
              issue.created_at > new Date(Date.now() - 24*60*60*1000).toISOString()
            );

            if (!existingIssue) {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['performance-regression', 'bug', 'priority-high']
              });
            }
          }

    - name: Comment on related PRs
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          // ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµæœã‚µãƒãƒªãƒ¼èª­ã¿è¾¼ã¿
          let performanceData = {};
          try {
            performanceData = JSON.parse(fs.readFileSync('comprehensive-performance-analysis/summary.json', 'utf8'));
          } catch (e) {
            console.log('No performance summary found');
            return;
          }

          // æœ€è¿‘ã®PRã‚’å–å¾—
          const prs = await github.rest.pulls.list({
            owner: context.repo.owner,
            repo: context.repo.repo,
            state: 'open',
            per_page: 10
          });

          const comment = `## ğŸš€ Nightly Performance Report

          **Test Date**: ${new Date().toISOString().split('T')[0]}

          ### Key Metrics

          | Metric | Current | Target | Status |
          |--------|---------|--------|--------|
          | Inference Latency | ${performanceData.inference?.avg_latency || 'N/A'}ms | <5ms | ${performanceData.inference?.latency_status || 'â“'} |
          | Throughput | ${performanceData.inference?.throughput || 'N/A'}/sec | >10,000/sec | ${performanceData.inference?.throughput_status || 'â“'} |
          | Memory Efficiency | ${performanceData.memory?.efficiency || 'N/A'}% | >50% | ${performanceData.memory?.efficiency_status || 'â“'} |

          ğŸ“Š [View Full Report](${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID})
          `;

          // é–¢é€£PRã«ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆperformanceé–¢é€£ã®å¤‰æ›´ãŒã‚ã‚‹å ´åˆï¼‰
          for (const pr of prs.data) {
            const files = await github.rest.pulls.listFiles({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: pr.number
            });

            const hasPerformanceChanges = files.data.some(file =>
              file.filename.includes('inference') ||
              file.filename.includes('performance') ||
              file.filename.includes('optimization')
            );

            if (hasPerformanceChanges) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                body: comment
              });
            }
          }