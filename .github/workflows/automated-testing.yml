name: Advanced Automated Testing

on:
  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/**'
  push:
    branches: [main]
  schedule:
    - cron: '0 4 * * 0'  # æ¯é€±æ—¥æ›œæ—¥4æ™‚
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Test type to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - mutation
        - property
        - fuzz
        - chaos

permissions:
  contents: read
  issues: write
  pull-requests: write

jobs:
  mutation-testing:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'mutation' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install mutpy pytest pytest-cov coverage

      - name: Run mutation testing
        run: |
          echo "## ğŸ§¬ Mutation Testing Results" > mutation_report.md
          echo "" >> mutation_report.md

          # ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
          mkdir -p mutation_results

          # ä¸»è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å¯¾ã—ã¦ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
          modules=(
            "src/day_trade/ml/feature_pipeline.py"
            "src/day_trade/ml/feature_store.py"
            "src/day_trade/core/optimization_strategy.py"
            "src/day_trade/utils/data_optimization.py"
          )

          for module in "${modules[@]}"; do
            if [ -f "$module" ]; then
              echo "### $(basename $module)" >> mutation_report.md
              echo "" >> mutation_report.md

              # MutPyã‚’ä½¿ç”¨ã—ã¦ãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
              if mut.py --target "$module" --unit-test tests/ --runner pytest --report-html "mutation_results/$(basename $module .py)" --timeout-factor 2.0 > "mutation_output_$(basename $module .py).txt" 2>&1; then

                # çµæœã‚’è§£æ
                if grep -q "Mutation score" "mutation_output_$(basename $module .py).txt"; then
                  score=$(grep "Mutation score" "mutation_output_$(basename $module .py).txt" | tail -1)
                  echo "âœ… **$score**" >> mutation_report.md
                else
                  echo "âš ï¸ **Mutation score could not be determined**" >> mutation_report.md
                fi

                # è©³ç´°ã‚’ãƒ¬ãƒãƒ¼ãƒˆã«è¿½åŠ 
                echo "" >> mutation_report.md
                echo '```' >> mutation_report.md
                tail -10 "mutation_output_$(basename $module .py).txt" >> mutation_report.md
                echo '```' >> mutation_report.md
                echo "" >> mutation_report.md

              else
                echo "âŒ **Mutation testing failed**" >> mutation_report.md
                echo "" >> mutation_report.md
                echo '```' >> mutation_report.md
                cat "mutation_output_$(basename $module .py).txt" >> mutation_report.md
                echo '```' >> mutation_report.md
                echo "" >> mutation_report.md
              fi
            fi
          done

          # ä»£æ›¿çš„ãªç°¡å˜ãªãƒŸãƒ¥ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ
          echo "### Code Quality Analysis" >> mutation_report.md
          echo "" >> mutation_report.md

          # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®æ¸¬å®š
          if pytest tests/ --cov=src/day_trade --cov-report=term-missing > coverage_output.txt 2>&1; then
            coverage=$(grep "TOTAL" coverage_output.txt | awk '{print $4}' | tail -1)
            echo "ğŸ“Š **Test Coverage**: $coverage" >> mutation_report.md
          fi

          # è¤‡é›‘åº¦ã®æ¸¬å®š
          pip install radon
          complexity=$(radon cc src/ -a -s | grep "Average complexity" | tail -1)
          echo "ğŸ“ˆ **Cyclomatic Complexity**: $complexity" >> mutation_report.md

      - name: Upload mutation results
        uses: actions/upload-artifact@v4
        with:
          name: mutation-testing-results
          path: |
            mutation_report.md
            mutation_results/
            mutation_output_*.txt
            coverage_output.txt

  property-based-testing:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'property' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install hypothesis pytest

      - name: Create property-based tests
        run: |
          mkdir -p property_tests

          # ç‰¹å¾´é‡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆã‚’ç”Ÿæˆ
          cat > property_tests/test_feature_pipeline_properties.py << 'EOF'
          import pytest
          import pandas as pd
          import numpy as np
          from hypothesis import given, strategies as st, settings, Verbosity
          import sys
          import os
          sys.path.insert(0, 'src')

          try:
              from day_trade.ml.feature_pipeline import FeaturePipeline
              FEATURE_PIPELINE_AVAILABLE = True
          except ImportError:
              FEATURE_PIPELINE_AVAILABLE = False

          @pytest.mark.skipif(not FEATURE_PIPELINE_AVAILABLE, reason="FeaturePipeline not available")
          class TestFeaturePipelineProperties:

              @given(
                  data_size=st.integers(min_value=10, max_value=1000),
                  batch_size=st.integers(min_value=1, max_value=100)
              )
              @settings(max_examples=20, verbosity=Verbosity.verbose)
              def test_batch_processing_consistency(self, data_size, batch_size):
                  """ãƒãƒƒãƒå‡¦ç†ã®ä¸€è²«æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
                  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                  data = pd.DataFrame({
                      'timestamp': pd.date_range('2024-01-01', periods=data_size, freq='1min'),
                      'price': np.random.uniform(100, 200, data_size),
                      'volume': np.random.randint(100, 1000, data_size)
                  })

                  pipeline = FeaturePipeline()

                  try:
                      # ãƒãƒƒãƒå‡¦ç†ã§ç‰¹å¾´é‡ç”Ÿæˆ
                      features = pipeline.batch_generate_features(data, batch_size=batch_size)

                      # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: ç‰¹å¾´é‡ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨
                      if features is not None:
                          assert len(features) >= 0, "ç‰¹å¾´é‡ã¯0å€‹ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"

                      # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒçµæœã«å½±éŸ¿ã—ãªã„ã“ã¨ï¼ˆä¸€è²«æ€§ï¼‰
                      features2 = pipeline.batch_generate_features(data, batch_size=min(batch_size*2, data_size))

                      if features is not None and features2 is not None:
                          # ç‰¹å¾´é‡ã®æ•°ã¯åŒã˜ã§ã‚ã‚‹ã¹ã
                          assert len(features) == len(features2), "ç•°ãªã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚‚åŒã˜æ•°ã®ç‰¹å¾´é‡ãŒç”Ÿæˆã•ã‚Œã‚‹ã¹ã"

                  except Exception as e:
                      # äºˆæœŸã—ãªã„ä¾‹å¤–ã¯å¤±æ•—ã¨ã™ã‚‹
                      pytest.fail(f"Unexpected exception: {e}")

              @given(
                  price_range=st.floats(min_value=1.0, max_value=10000.0, allow_nan=False, allow_infinity=False),
                  volume_range=st.integers(min_value=1, max_value=100000)
              )
              @settings(max_examples=10)
              def test_input_range_handling(self, price_range, volume_range):
                  """å…¥åŠ›å€¤ã®ç¯„å›²ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ã‚’ãƒ†ã‚¹ãƒˆ"""
                  data = pd.DataFrame({
                      'timestamp': pd.date_range('2024-01-01', periods=50, freq='1min'),
                      'price': [price_range] * 50,
                      'volume': [volume_range] * 50
                  })

                  pipeline = FeaturePipeline()

                  try:
                      features = pipeline.batch_generate_features(data, batch_size=10)

                      # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: æœ‰åŠ¹ãªå…¥åŠ›ã«å¯¾ã—ã¦ä¾‹å¤–ãŒç™ºç”Ÿã—ãªã„ã“ã¨
                      if features is not None:
                          assert len(features) >= 0

                  except Exception as e:
                      # ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¦ç¶™ç¶šï¼ˆå®Œå…¨ã«å¤±æ•—ã•ã›ãªã„ï¼‰
                      print(f"Warning: Exception with price={price_range}, volume={volume_range}: {e}")

          # ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
          try:
              from day_trade.utils.data_optimization import DataOptimizer
              DATA_OPTIMIZER_AVAILABLE = True
          except ImportError:
              DATA_OPTIMIZER_AVAILABLE = False

          @pytest.mark.skipif(not DATA_OPTIMIZER_AVAILABLE, reason="DataOptimizer not available")
          class TestDataOptimizerProperties:

              @given(
                  data_size=st.integers(min_value=5, max_value=500),
                  chunk_size=st.integers(min_value=1, max_value=50)
              )
              @settings(max_examples=15)
              def test_chunking_preserves_data(self, data_size, chunk_size):
                  """ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†ã§ãƒ‡ãƒ¼ã‚¿ãŒå¤±ã‚ã‚Œãªã„ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""
                  original_data = list(range(data_size))

                  optimizer = DataOptimizer()

                  try:
                      chunks = optimizer.create_chunks(original_data, chunk_size)

                      # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£: ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²å¾Œã€å…¨ãƒ‡ãƒ¼ã‚¿ãŒä¿æŒã•ã‚Œã‚‹ã“ã¨
                      flattened = [item for chunk in chunks for item in chunk]
                      assert len(flattened) == len(original_data), "ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å¾Œã‚‚ãƒ‡ãƒ¼ã‚¿æ•°ã¯ä¿æŒã•ã‚Œã‚‹"
                      assert sorted(flattened) == sorted(original_data), "ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å¾Œã‚‚ãƒ‡ãƒ¼ã‚¿å†…å®¹ã¯ä¿æŒã•ã‚Œã‚‹"

                  except Exception as e:
                      print(f"Warning: Chunking failed with size={data_size}, chunk_size={chunk_size}: {e}")
          EOF

          # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
          echo "## ğŸ² Property-Based Testing Results" > property_report.md
          echo "" >> property_report.md

          if pytest property_tests/ -v --tb=short > property_output.txt 2>&1; then
            echo "âœ… **Property-based tests passed**" >> property_report.md
            echo "" >> property_report.md
            echo "### Test Results" >> property_report.md
            echo '```' >> property_report.md
            grep -E "(PASSED|FAILED|ERROR)" property_output.txt >> property_report.md
            echo '```' >> property_report.md
          else
            echo "âš ï¸ **Some property-based tests failed**" >> property_report.md
            echo "" >> property_report.md
            echo '```' >> property_report.md
            cat property_output.txt >> property_report.md
            echo '```' >> property_report.md
          fi

      - name: Upload property testing results
        uses: actions/upload-artifact@v4
        with:
          name: property-testing-results
          path: |
            property_report.md
            property_tests/
            property_output.txt

  fuzz-testing:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'fuzz' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install atheris coverage

      - name: Create fuzz testing targets
        run: |
          mkdir -p fuzz_tests

          # æ–‡å­—åˆ—å‡¦ç†ã®ãƒ•ã‚¡ã‚ºãƒ†ã‚¹ãƒˆ
          cat > fuzz_tests/fuzz_string_processing.py << 'EOF'
          import atheris
          import sys
          import os
          sys.path.insert(0, 'src')

          def TestStringProcessing(data):
              """æ–‡å­—åˆ—å‡¦ç†ã®ãƒ•ã‚¡ã‚ºãƒ†ã‚¹ãƒˆ"""
              fdp = atheris.FuzzedDataProvider(data)

              try:
                  # ãƒ©ãƒ³ãƒ€ãƒ ãªæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
                  test_string = fdp.ConsumeUnicodeNoSurrogates(fdp.ConsumeIntInRange(0, 1000))

                  # åŸºæœ¬çš„ãªæ–‡å­—åˆ—æ“ä½œãŒä¾‹å¤–ã‚’èµ·ã“ã•ãªã„ã“ã¨ã‚’ç¢ºèª
                  result = test_string.strip().lower().replace('\n', ' ')

                  # JSONãƒ‘ãƒ¼ã‚¹ã®ãƒ†ã‚¹ãƒˆ
                  import json
                  if test_string.startswith('{') and test_string.endswith('}'):
                      try:
                          json.loads(test_string)
                      except (json.JSONDecodeError, ValueError):
                          pass  # æœŸå¾…ã•ã‚Œã‚‹ä¾‹å¤–

                  # SQL injection patterns detection
                  dangerous_patterns = ['DROP TABLE', 'DELETE FROM', '; --', 'UNION SELECT']
                  for pattern in dangerous_patterns:
                      if pattern.lower() in test_string.lower():
                          print(f"Potential SQL injection pattern detected: {pattern}")

              except Exception as e:
                  # äºˆæœŸã—ãªã„ä¾‹å¤–ã¯è¨˜éŒ²
                  print(f"Unexpected exception in string processing: {e}")
                  return  # ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã•ã›ãªã„

          def TestNumericProcessing(data):
              """æ•°å€¤å‡¦ç†ã®ãƒ•ã‚¡ã‚ºãƒ†ã‚¹ãƒˆ"""
              fdp = atheris.FuzzedDataProvider(data)

              try:
                  # ãƒ©ãƒ³ãƒ€ãƒ ãªæ•°å€¤ã‚’ç”Ÿæˆ
                  float_val = fdp.ConsumeFloat()
                  int_val = fdp.ConsumeInt()

                  # æ•°å€¤æ“ä½œãŒä¾‹å¤–ã‚’èµ·ã“ã•ãªã„ã“ã¨ã‚’ç¢ºèª
                  if not (math.isnan(float_val) or math.isinf(float_val)):
                      result = abs(float_val) * 2
                      result = min(max(result, -1e6), 1e6)  # ç¯„å›²åˆ¶é™

                  # æ•´æ•°æ¼”ç®—
                  if int_val != 0:
                      division_result = 100 / int_val

              except (ZeroDivisionError, OverflowError, ValueError) as e:
                  pass  # æœŸå¾…ã•ã‚Œã‚‹ä¾‹å¤–
              except Exception as e:
                  print(f"Unexpected exception in numeric processing: {e}")

          import math
          atheris.Setup(sys.argv, TestStringProcessing)
          atheris.Setup(sys.argv, TestNumericProcessing)
          atheris.Fuzz()
          EOF

          # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒ•ã‚¡ã‚ºãƒ†ã‚¹ãƒˆ
          cat > fuzz_tests/fuzz_data_structures.py << 'EOF'
          import atheris
          import sys
          import pandas as pd
          import numpy as np
          sys.path.insert(0, 'src')

          def TestDataFrameOperations(data):
              """DataFrameæ“ä½œã®ãƒ•ã‚¡ã‚ºãƒ†ã‚¹ãƒˆ"""
              fdp = atheris.FuzzedDataProvider(data)

              try:
                  # ãƒ©ãƒ³ãƒ€ãƒ ãªDataFrameã‚’ç”Ÿæˆ
                  rows = fdp.ConsumeIntInRange(1, 100)
                  cols = fdp.ConsumeIntInRange(1, 10)

                  # ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ‡ãƒ¼ã‚¿ã§DataFrameã‚’ä½œæˆ
                  df_data = {}
                  for i in range(cols):
                      col_name = f"col_{i}"
                      if fdp.ConsumeBool():
                          # æ•°å€¤ã‚«ãƒ©ãƒ 
                          df_data[col_name] = [fdp.ConsumeFloat() for _ in range(rows)]
                      else:
                          # æ–‡å­—åˆ—ã‚«ãƒ©ãƒ 
                          df_data[col_name] = [fdp.ConsumeUnicodeNoSurrogates(20) for _ in range(rows)]

                  df = pd.DataFrame(df_data)

                  # åŸºæœ¬æ“ä½œã®ãƒ†ã‚¹ãƒˆ
                  try:
                      _ = df.describe()
                      _ = df.head()
                      _ = df.shape
                      _ = df.dtypes

                      # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ“ä½œ
                      numeric_cols = df.select_dtypes(include=[np.number]).columns
                      if len(numeric_cols) > 0:
                          col = fdp.PickValueInList(numeric_cols.tolist())
                          median_val = df[col].median()
                          if not pd.isna(median_val):
                              filtered_df = df[df[col] > median_val]

                  except Exception as e:
                      print(f"DataFrame operation failed: {e}")

              except Exception as e:
                  print(f"Unexpected exception in DataFrame fuzzing: {e}")

          atheris.Setup(sys.argv, TestDataFrameOperations)
          atheris.Fuzz()
          EOF

          echo "## ğŸ¯ Fuzz Testing Results" > fuzz_report.md
          echo "" >> fuzz_report.md

          # ãƒ•ã‚¡ã‚ºãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œï¼ˆçŸ­æ™‚é–“ã§ï¼‰
          timeout 60s python fuzz_tests/fuzz_string_processing.py -atheris_runs=1000 > fuzz_string_output.txt 2>&1 || true
          timeout 60s python fuzz_tests/fuzz_data_structures.py -atheris_runs=500 > fuzz_data_output.txt 2>&1 || true

          echo "### String Processing Fuzz Test" >> fuzz_report.md
          if grep -q "Unexpected exception" fuzz_string_output.txt; then
            echo "âš ï¸ **Potential issues found**" >> fuzz_report.md
            echo '```' >> fuzz_report.md
            grep "Unexpected exception" fuzz_string_output.txt | head -10 >> fuzz_report.md
            echo '```' >> fuzz_report.md
          else
            echo "âœ… **No issues found in string processing**" >> fuzz_report.md
          fi
          echo "" >> fuzz_report.md

          echo "### Data Structure Fuzz Test" >> fuzz_report.md
          if grep -q "Unexpected exception\|failed" fuzz_data_output.txt; then
            echo "âš ï¸ **Potential issues found**" >> fuzz_report.md
            echo '```' >> fuzz_report.md
            grep -E "Unexpected exception|failed" fuzz_data_output.txt | head -10 >> fuzz_report.md
            echo '```' >> fuzz_report.md
          else
            echo "âœ… **No issues found in data structure operations**" >> fuzz_report.md
          fi

      - name: Upload fuzz testing results
        uses: actions/upload-artifact@v4
        with:
          name: fuzz-testing-results
          path: |
            fuzz_report.md
            fuzz_tests/
            fuzz_*_output.txt

  chaos-engineering:
    runs-on: ubuntu-latest
    if: github.event.inputs.test_type == 'chaos' || github.event.inputs.test_type == 'all' || github.event.inputs.test_type == ''
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest psutil

      - name: Create chaos engineering tests
        run: |
          mkdir -p chaos_tests

          cat > chaos_tests/test_system_resilience.py << 'EOF'
          import pytest
          import time
          import threading
          import random
          import os
          import sys
          import tempfile
          import sqlite3
          import psutil
          sys.path.insert(0, 'src')

          class ChaosTest:
              """ã‚·ã‚¹ãƒ†ãƒ éšœå®³ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ"""

              def test_memory_pressure(self):
                  """ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""
                  try:
                      # ãƒ¡ãƒ¢ãƒªåœ§è¿«ã‚’æ¨¡æ“¬
                      memory_hogger = []
                      initial_memory = psutil.virtual_memory().percent

                      # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªã®20%ã¾ã§ä½¿ç”¨ï¼ˆå®‰å…¨ãªç¯„å›²ã§ï¼‰
                      target_memory = min(initial_memory + 20, 80)  # 80%ã‚’è¶…ãˆãªã„ã‚ˆã†åˆ¶é™

                      while psutil.virtual_memory().percent < target_memory:
                          # 1MBãšã¤ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»
                          memory_hogger.append(b'x' * 1024 * 1024)
                          if len(memory_hogger) > 100:  # 100MBåˆ¶é™
                              break

                      # ãƒ¡ãƒ¢ãƒªåœ§è¿«çŠ¶æ…‹ã§ã‚·ã‚¹ãƒ†ãƒ ãŒå‹•ä½œã™ã‚‹ã‹ãƒ†ã‚¹ãƒˆ
                      try:
                          from day_trade.ml.feature_pipeline import FeaturePipeline
                          pipeline = FeaturePipeline()

                          import pandas as pd
                          test_data = pd.DataFrame({
                              'timestamp': pd.date_range('2024-01-01', periods=50, freq='1min'),
                              'price': [100 + i for i in range(50)],
                              'volume': [1000 + i * 10 for i in range(50)]
                          })

                          features = pipeline.batch_generate_features(test_data, batch_size=10)
                          assert features is not None or True  # ã‚·ã‚¹ãƒ†ãƒ ãŒå¿œç­”ã™ã‚‹ã“ã¨

                      except ImportError:
                          pass  # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                      except Exception as e:
                          print(f"Memory pressure test warning: {e}")

                      # ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾
                      del memory_hogger

                  except Exception as e:
                      pytest.fail(f"Memory pressure test failed: {e}")

              def test_concurrent_access(self):
                  """åŒæ™‚ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã®å‹•ä½œãƒ†ã‚¹ãƒˆ"""
                  results = []
                  errors = []

                  def worker_task(worker_id):
                      """ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯"""
                      try:
                          # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹ã®æ¨¡æ“¬
                          with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as tmp:
                              conn = sqlite3.connect(tmp.name, timeout=1.0)
                              cursor = conn.cursor()

                              cursor.execute('''CREATE TABLE IF NOT EXISTS test
                                              (id INTEGER PRIMARY KEY, worker_id INTEGER, data TEXT)''')

                              # åŒæ™‚æ›¸ãè¾¼ã¿
                              for i in range(10):
                                  cursor.execute('INSERT INTO test (worker_id, data) VALUES (?, ?)',
                                               (worker_id, f'data_{i}'))
                                  time.sleep(0.01)  # çŸ­ã„é…å»¶

                              conn.commit()
                              conn.close()

                              results.append(f"Worker {worker_id} completed")
                              os.unlink(tmp.name)

                      except Exception as e:
                          errors.append(f"Worker {worker_id} error: {e}")

                  # è¤‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã§åŒæ™‚å®Ÿè¡Œ
                  threads = []
                  for i in range(5):
                      thread = threading.Thread(target=worker_task, args=(i,))
                      threads.append(thread)
                      thread.start()

                  # ã™ã¹ã¦ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®å®Œäº†ã‚’å¾…æ©Ÿ
                  for thread in threads:
                      thread.join(timeout=10)

                  # çµæœã®æ¤œè¨¼
                  print(f"Completed workers: {len(results)}")
                  print(f"Errors: {len(errors)}")

                  if errors:
                      print("Errors encountered:")
                      for error in errors:
                          print(f"  {error}")

                  # å°‘ãªãã¨ã‚‚ä¸€éƒ¨ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæˆåŠŸã™ã‚‹ã“ã¨
                  assert len(results) > 0, "At least some workers should succeed"

              def test_disk_space_simulation(self):
                  """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ã®æ¨¡æ“¬ãƒ†ã‚¹ãƒˆ"""
                  try:
                      # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã®å‹•ä½œãƒ†ã‚¹ãƒˆ
                      with tempfile.TemporaryDirectory() as tmpdir:

                          # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿æ¨¡æ“¬
                          config_path = os.path.join(tmpdir, 'test_config.json')

                          import json
                          test_config = {
                              "database": {"url": "sqlite:///test.db"},
                              "cache_size": 100
                          }

                          with open(config_path, 'w') as f:
                              json.dump(test_config, f)

                          # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ã“ã¨
                          with open(config_path, 'r') as f:
                              loaded_config = json.load(f)

                          assert loaded_config == test_config

                          # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã®æ¨¡æ“¬ï¼ˆåˆ¶é™ã‚ã‚Šï¼‰
                          large_file_path = os.path.join(tmpdir, 'large_file.txt')
                          with open(large_file_path, 'w') as f:
                              for i in range(1000):  # é©åº¦ãªã‚µã‚¤ã‚ºã«åˆ¶é™
                                  f.write(f'Line {i}: test data\n')

                          # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨
                          assert os.path.exists(large_file_path)
                          assert os.path.getsize(large_file_path) > 0

                  except Exception as e:
                      pytest.fail(f"Disk space simulation failed: {e}")

              def test_network_timeout_simulation(self):
                  """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®æ¨¡æ“¬ãƒ†ã‚¹ãƒˆ"""
                  import socket
                  import requests
                  from unittest.mock import patch, Mock

                  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’æ¨¡æ“¬
                  def mock_request_timeout(*args, **kwargs):
                      time.sleep(0.1)  # çŸ­ã„é…å»¶
                      raise requests.exceptions.Timeout("Simulated timeout")

                  try:
                      with patch('requests.get', side_effect=mock_request_timeout):

                          # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†ã®ãƒ†ã‚¹ãƒˆ
                          try:
                              import requests
                              response = requests.get('https://httpbin.org/delay/5', timeout=0.05)
                          except requests.exceptions.Timeout:
                              # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä¾‹å¤–ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹ã“ã¨
                              pass
                          except Exception as e:
                              print(f"Network timeout handling: {e}")

                  except Exception as e:
                      print(f"Network simulation error: {e}")
          EOF

          # ã‚«ã‚ªã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
          echo "## âš¡ Chaos Engineering Results" > chaos_report.md
          echo "" >> chaos_report.md

          if pytest chaos_tests/ -v -s --tb=short > chaos_output.txt 2>&1; then
            echo "âœ… **System resilience tests passed**" >> chaos_report.md
            echo "" >> chaos_report.md
          else
            echo "âš ï¸ **Some resilience tests failed**" >> chaos_report.md
            echo "" >> chaos_report.md
          fi

          echo "### Test Results" >> chaos_report.md
          echo '```' >> chaos_report.md
          grep -E "(PASSED|FAILED|ERROR)" chaos_output.txt >> chaos_report.md
          echo '```' >> chaos_report.md

          echo "" >> chaos_report.md
          echo "### Detailed Output" >> chaos_report.md
          echo '```' >> chaos_report.md
          tail -20 chaos_output.txt >> chaos_report.md
          echo '```' >> chaos_report.md

      - name: Upload chaos testing results
        uses: actions/upload-artifact@v4
        with:
          name: chaos-engineering-results
          path: |
            chaos_report.md
            chaos_tests/
            chaos_output.txt

  test-summary:
    runs-on: ubuntu-latest
    needs: [mutation-testing, property-based-testing, fuzz-testing, chaos-engineering]
    if: always()
    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4

      - name: Create comprehensive test summary
        run: |
          echo "# ğŸ§ª Advanced Automated Testing Report" > test_summary.md
          echo "" >> test_summary.md
          echo "**å®Ÿè¡Œæ—¥æ™‚**: $(date -u)" >> test_summary.md
          echo "**ã‚³ãƒŸãƒƒãƒˆ**: ${GITHUB_SHA:0:7}" >> test_summary.md
          echo "" >> test_summary.md

          # å„ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’çµ±åˆ
          for report_dir in *-results/; do
            if [ -d "$report_dir" ]; then
              echo "## ğŸ“‹ $(basename "$report_dir" -results)" >> test_summary.md
              echo "" >> test_summary.md

              for file in "$report_dir"*.md; do
                if [ -f "$file" ]; then
                  cat "$file" >> test_summary.md
                  echo "" >> test_summary.md
                fi
              done
            fi
          done

          echo "---" >> test_summary.md
          echo "" >> test_summary.md
          echo "## ğŸ“Š Test Summary" >> test_summary.md
          echo "" >> test_summary.md

          # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
          MUTATION_SUCCESS=false
          PROPERTY_SUCCESS=false
          FUZZ_SUCCESS=false
          CHAOS_SUCCESS=false

          if grep -q "âœ…" test_summary.md; then
            echo "### âœ… Successful Tests" >> test_summary.md
            grep "âœ…" test_summary.md | head -10 >> test_summary.md
          fi

          if grep -q "âš ï¸\|âŒ" test_summary.md; then
            echo "" >> test_summary.md
            echo "### âš ï¸ Issues Found" >> test_summary.md
            grep -E "âš ï¸|âŒ" test_summary.md | head -10 >> test_summary.md
          fi

          echo "" >> test_summary.md
          echo "### ğŸ¯ Recommendations" >> test_summary.md
          echo "" >> test_summary.md

          if grep -q "Critical\|critical\|ğŸš¨" test_summary.md; then
            echo "ğŸš¨ **Critical Issues Detected**" >> test_summary.md
            echo "- Immediate investigation and fixing required" >> test_summary.md
            echo "- Consider rolling back recent changes" >> test_summary.md
          elif grep -q "âš ï¸" test_summary.md; then
            echo "âš ï¸ **Warnings Detected**" >> test_summary.md
            echo "- Review and address issues before release" >> test_summary.md
            echo "- Consider additional testing for affected areas" >> test_summary.md
          else
            echo "âœ… **All Tests Passed**" >> test_summary.md
            echo "- System shows good resilience and quality" >> test_summary.md
            echo "- Continue with current development practices" >> test_summary.md
          fi

          echo "" >> test_summary.md
          echo "*This report was automatically generated*" >> test_summary.md

      - name: Comment test summary on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              const summary = fs.readFileSync('test_summary.md', 'utf8');

              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ğŸ§ª Advanced Testing Results\n\n${summary}\n\n---\n*Automated testing report - ${new Date().toISOString()}*`
              });
            } catch (error) {
              console.error('Error posting test summary:', error);
            }

      - name: Create testing issue if critical problems found
        if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              const summary = fs.readFileSync('test_summary.md', 'utf8');

              // é‡è¦ãªå•é¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
              const hasCriticalIssues = summary.includes('ğŸš¨') ||
                                       summary.includes('Critical') ||
                                       summary.includes('FAILED');

              if (hasCriticalIssues) {
                await github.rest.issues.create({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  title: `ğŸ§ª Advanced Testing ã§é‡è¦ãªå•é¡Œã‚’æ¤œå‡º - ${new Date().toISOString().split('T')[0]}`,
                  body: summary,
                  labels: ['testing', 'high-priority', 'automated'],
                  assignees: ['kaenozu']
                });
              }
            } catch (error) {
              console.error('Error creating testing issue:', error);
            }

      - name: Upload comprehensive test report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: test_summary.md
