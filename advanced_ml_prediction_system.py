#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Advanced ML Prediction System - é«˜åº¦æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 

äºˆæ¸¬ç²¾åº¦æ”¹å–„ã®ãŸã‚ã®åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ 
Issue #800-2-1å®Ÿè£…ï¼šäºˆæ¸¬ç²¾åº¦æ”¹å–„è¨ˆç”»
"""

import asyncio
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
import json
import sqlite3
from pathlib import Path
import joblib # Added for model saving/loading

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# æŠ€è¡“æŒ‡æ¨™ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import talib
    TALIB_AVAILABLE = True
except ImportError:
    TALIB_AVAILABLE = False
    print("WARNING: talib not available - using fallback technical indicators")

# Windowsç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–
import sys
import os
os.environ['PYTHONIOENCODING'] = 'utf-8'

if sys.platform == 'win32':
    try:
        sys.stdout.reconfigure(encoding='utf-8')
        sys.stderr.reconfigure(encoding='utf-8')
    except:
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

class ModelType(Enum):
    """ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—"""
    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    LOGISTIC_REGRESSION = "logistic_regression"
    SVM = "svm"
    NEURAL_NETWORK = "neural_network"
    ENSEMBLE_VOTING = "ensemble_voting"

class ScalerType(Enum):
    """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—"""
    STANDARD = "standard"
    ROBUST = "robust"
    MINMAX = "minmax"
    NONE = "none"

class FeatureSelectionType(Enum):
    """ç‰¹å¾´é¸æŠã‚¿ã‚¤ãƒ—"""
    K_BEST_F = "k_best_f"
    K_BEST_MUTUAL = "k_best_mutual"
    NONE = "none"

@dataclass
class ModelConfig:
    """ãƒ¢ãƒ‡ãƒ«è¨­å®š"""
    model_type: ModelType
    scaler_type: ScalerType
    feature_selection: FeatureSelectionType
    feature_selection_k: int = 50
    hyperparameters: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ModelPerformance:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½"""
    model_type: ModelType
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    roc_auc: float
    cross_val_score: float
    feature_importance: Dict[str, float] = field(default_factory=dict)

@dataclass
class PredictionResult:
    """äºˆæ¸¬çµæœ"""
    symbol: str
    prediction: int  # 0: ä¸‹é™, 1: ä¸Šæ˜‡
    confidence: float
    model_consensus: Dict[str, int]  # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
    feature_values: Dict[str, float]
    timestamp: datetime

class AdvancedFeatureEngineering:
    """é«˜åº¦ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    async def create_advanced_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """é«˜åº¦ç‰¹å¾´é‡ä½œæˆ"""

        if len(data) < 100:
            raise ValueError("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆæœ€ä½100ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆå¿…è¦ï¼‰")

        features = pd.DataFrame(index=data.index)

        # åŸºæœ¬ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿
        high = data['High'].values
        low = data['Low'].values
        close = data['Close'].values
        volume = data['Volume'].values
        open_price = data['Open'].values

        try:
            # 1. ãƒˆãƒ¬ãƒ³ãƒ‰æŒ‡æ¨™ï¼ˆ16ç¨®é¡ï¼‰
            features['sma_5'] = talib.SMA(close, timeperiod=5)
            features['sma_10'] = talib.SMA(close, timeperiod=10)
            features['sma_20'] = talib.SMA(close, timeperiod=20)
            features['sma_50'] = talib.SMA(close, timeperiod=50)
            features['ema_5'] = talib.EMA(close, timeperiod=5)
            features['ema_10'] = talib.EMA(close, timeperiod=10)
            features['ema_20'] = talib.EMA(close, timeperiod=20)
            features['wma_10'] = talib.WMA(close, timeperiod=10)
            features['tema_10'] = talib.TEMA(close, timeperiod=10)
            features['dema_10'] = talib.DEMA(close, timeperiod=10)
            features['kama_10'] = talib.KAMA(close, timeperiod=10)
            features['trima_10'] = talib.TRIMA(close, timeperiod=10)
            features['t3_10'] = talib.T3(close, timeperiod=10)
            features['mama'], features['fama'] = talib.MAMA(close, fastlimit=0.5, slowlimit=0.05)
            features['ht_trendline'] = talib.HT_TRENDLINE(close)

            # 2. ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ æŒ‡æ¨™ï¼ˆ20ç¨®é¡ï¼‰
            features['rsi_14'] = talib.RSI(close, timeperiod=14)
            features['rsi_7'] = talib.RSI(close, timeperiod=7)
            features['rsi_21'] = talib.RSI(close, timeperiod=21)
            features['mfi_14'] = talib.MFI(high, low, close, volume, timeperiod=14)
            features['williams_r'] = talib.WILLR(high, low, close, timeperiod=14)
            features['cci_14'] = talib.CCI(high, low, close, timeperiod=14)
            features['stoch_k'], features['stoch_d'] = talib.STOCH(high, low, close)
            features['stochf_k'], features['stochf_d'] = talib.STOCHF(high, low, close)
            features['stochrsi_k'], features['stochrsi_d'] = talib.STOCHRSI(close)
            features['ultimate_osc'] = talib.ULTOSC(high, low, close)
            features['roc_10'] = talib.ROC(close, timeperiod=10)
            features['roc_5'] = talib.ROC(close, timeperiod=5)
            features['rocp_10'] = talib.ROCP(close, timeperiod=10)
            features['rocr_10'] = talib.ROCR(close, timeperiod=10)
            features['trix_14'] = talib.TRIX(close, timeperiod=14)
            features['apo'] = talib.APO(close)
            features['ppo'] = talib.PPO(close)
            features['cmo_14'] = talib.CMO(close, timeperiod=14)
            features['dx_14'] = talib.DX(high, low, close, timeperiod=14)
            features['adx_14'] = talib.ADX(high, low, close, timeperiod=14)

            # 3. ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£æŒ‡æ¨™ï¼ˆ8ç¨®é¡ï¼‰
            features['atr_14'] = talib.ATR(high, low, close, timeperiod=14)
            features['atr_7'] = talib.ATR(high, low, close, timeperiod=7)
            features['natr_14'] = talib.NATR(high, low, close, timeperiod=14)
            features['trange'] = talib.TRANGE(high, low, close)
            features['ht_dcperiod'] = talib.HT_DCPERIOD(close)
            features['ht_dcphase'] = talib.HT_DCPHASE(close)
            features['inphase'], features['quadrature'] = talib.HT_PHASOR(close)
            features['ht_sine'], features['ht_leadsine'] = talib.HT_SINE(close)

            # 4. ãƒœãƒªãƒ¥ãƒ¼ãƒ æŒ‡æ¨™ï¼ˆ10ç¨®é¡ï¼‰
            features['ad'] = talib.AD(high, low, close, volume)
            features['adosc'] = talib.ADOSC(high, low, close, volume)
            features['obv'] = talib.OBV(close, volume)
            features['cmf'] = features['ad'] / volume  # Chaikin Money Flow
            features['volume_sma_10'] = talib.SMA(volume, timeperiod=10)
            features['volume_ratio'] = volume / features['volume_sma_10']
            features['price_volume'] = close * volume
            features['vwap'] = (features['price_volume'].rolling(20).sum() /
                              pd.Series(volume).rolling(20).sum())
            features['volume_oscillator'] = (talib.SMA(volume, 5) - talib.SMA(volume, 10)) / talib.SMA(volume, 10) * 100
            features['ease_of_movement'] = ((high + low) / 2 - (high.shift(1) + low.shift(1)) / 2) / (volume / ((high - low) * 1000000))

            # 5. ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤æŒ‡æ¨™ï¼ˆ12ç¨®é¡ï¼‰
            upperband, middleband, lowerband = talib.BBANDS(close, timeperiod=20)
            features['bb_upper'] = upperband
            features['bb_middle'] = middleband
            features['bb_lower'] = lowerband
            features['bb_width'] = (upperband - lowerband) / middleband
            features['bb_position'] = (close - lowerband) / (upperband - lowerband)

            features['sar'] = talib.SAR(high, low)
            features['sar_ext'] = talib.SAREXT(high, low)

            macd, macdsignal, macdhist = talib.MACD(close)
            features['macd'] = macd
            features['macd_signal'] = macdsignal
            features['macd_hist'] = macdhist
            features['macd_ratio'] = macd / macdsignal

            # 6. ä¾¡æ ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æŒ‡æ¨™ï¼ˆ10ç¨®é¡ï¼‰
            features['cdl_doji'] = talib.CDLDOJI(open_price, high, low, close)
            features['cdl_hammer'] = talib.CDLHAMMER(open_price, high, low, close)
            features['cdl_engulfing'] = talib.CDLENGULFING(open_price, high, low, close)
            features['cdl_harami'] = talib.CDLHARAMI(open_price, high, low, close)
            features['cdl_spinning_top'] = talib.CDLSPINNINGTOP(open_price, high, low, close)
            features['cdl_marubozu'] = talib.CDLMARUBOZU(open_price, high, low, close)
            features['cdl_shooting_star'] = talib.CDLSHOOTINGSTAR(open_price, high, low, close)
            features['cdl_hanging_man'] = talib.CDLHANGINGMAN(open_price, high, low, close)
            features['cdl_morning_star'] = talib.CDLMORNINGSTAR(open_price, high, low, close)
            features['cdl_evening_star'] = talib.CDLEVENINGSTAR(open_price, high, low, close)

            # 7. çµ±è¨ˆçš„æŒ‡æ¨™ï¼ˆ8ç¨®é¡ï¼‰
            features['returns'] = close.pct_change()
            features['log_returns'] = np.log(close / close.shift(1))
            features['volatility_5'] = features['returns'].rolling(5).std()
            features['volatility_20'] = features['returns'].rolling(20).std()
            features['skewness_20'] = features['returns'].rolling(20).skew()
            features['kurtosis_20'] = features['returns'].rolling(20).kurt()
            features['var_95'] = features['returns'].rolling(20).quantile(0.05)
            features['sharpe_ratio_20'] = features['returns'].rolling(20).mean() / features['volatility_20']

            # 8. ã‚«ã‚¹ã‚¿ãƒ è¤‡åˆæŒ‡æ¨™ï¼ˆ6ç¨®é¡ï¼‰
            features['price_position'] = (close - talib.SMA(close, 20)) / talib.SMA(close, 20)
            features['volume_price_trend'] = features['volume_ratio'] * features['returns']
            features['momentum_divergence'] = features['rsi_14'] - (close / close.shift(14) - 1) * 100
            features['trend_strength'] = abs(features['sma_5'] - features['sma_20']) / features['sma_20']
            features['volatility_breakout'] = features['atr_14'] / close
            features['composite_momentum'] = (features['rsi_14'] / 100 + features['stoch_k'] / 100 +
                                            (features['williams_r'] + 100) / 100) / 3

        except Exception as e:
            self.logger.error(f"ç‰¹å¾´é‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            raise

        # NaNå€¤ã‚’å‰æ–¹å‘ã«è£œé–“
        features = features.fillna(method='ffill').fillna(method='bfill')

        # æœ€åˆã®100è¡Œã‚’å‰Šé™¤ï¼ˆæŒ‡æ¨™è¨ˆç®—ã®ãŸã‚ï¼‰
        features = features.iloc[100:].copy()

        self.logger.info(f"é«˜åº¦ç‰¹å¾´é‡ä½œæˆå®Œäº†: {features.shape[1]}ç‰¹å¾´é‡")
        return features

class AdvancedMLPredictionSystem:
    """é«˜åº¦æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.feature_engineering = AdvancedFeatureEngineering()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = Path("ml_models_data/advanced_ml_predictions.db")
        self.db_path.parent.mkdir(exist_ok=True)

        # ãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.model_configs = self._create_model_configs()

        # è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
        self.trained_models: Dict[str, Any] = {}

        self.logger.info("Advanced ML prediction system initialized")

    def _create_model_configs(self) -> List[ModelConfig]:
        """ãƒ¢ãƒ‡ãƒ«è¨­å®šä½œæˆ"""

        configs = [
            # Random Forest é…ç½®
            ModelConfig(
                model_type=ModelType.RANDOM_FOREST,
                scaler_type=ScalerType.NONE,
                feature_selection=FeatureSelectionType.K_BEST_F,
                feature_selection_k=40,
                hyperparameters={
                    'n_estimators': 200,
                    'max_depth': 15,
                    'min_samples_split': 5,
                    'min_samples_leaf': 2,
                    'random_state': 42
                }
            ),

            # Gradient Boosting é…ç½®
            ModelConfig(
                model_type=ModelType.GRADIENT_BOOSTING,
                scaler_type=ScalerType.STANDARD,
                feature_selection=FeatureSelectionType.K_BEST_MUTUAL,
                feature_selection_k=35,
                hyperparameters={
                    'n_estimators': 150,
                    'learning_rate': 0.1,
                    'max_depth': 8,
                    'min_samples_split': 4,
                    'random_state': 42
                }
            ),

            # Logistic Regression é…ç½®
            ModelConfig(
                model_type=ModelType.LOGISTIC_REGRESSION,
                scaler_type=ScalerType.STANDARD,
                feature_selection=FeatureSelectionType.K_BEST_F,
                feature_selection_k=30,
                hyperparameters={
                    'C': 1.0,
                    'penalty': 'l2',
                    'max_iter': 1000,
                    'random_state': 42
                }
            ),

            # SVM é…ç½®
            ModelConfig(
                model_type=ModelType.SVM,
                scaler_type=ScalerType.ROBUST,
                feature_selection=FeatureSelectionType.K_BEST_F,
                feature_selection_k=25,
                hyperparameters={
                    'C': 1.0,
                    'kernel': 'rbf',
                    'gamma': 'scale',
                    'probability': True,
                    'random_state': 42
                }
            ),

            # Neural Network é…ç½®
            ModelConfig(
                model_type=ModelType.NEURAL_NETWORK,
                scaler_type=ScalerType.MINMAX,
                feature_selection=FeatureSelectionType.K_BEST_MUTUAL,
                feature_selection_k=45,
                hyperparameters={
                    'hidden_layer_sizes': (100, 50),
                    'activation': 'relu',
                    'solver': 'adam',
                    'alpha': 0.0001,
                    'learning_rate': 'adaptive',
                    'max_iter': 500,
                    'random_state': 42
                }
            )
        ]

        return configs

    async def train_advanced_models(self, symbol: str, period: str = "6mo", hyperparameters: Optional[Dict[ModelType, Dict[str, Any]]] = None) -> Dict[ModelType, ModelPerformance]:
        """é«˜åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""

        self.logger.info(f"é«˜åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹: {symbol}")

        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, period)

            if data is None or len(data) < 150:
                raise ValueError("è¨“ç·´ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

            # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
            features = await self.feature_engineering.create_advanced_features(data)

            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆï¼ˆç¿Œæ—¥ã®ä¸Šæ˜‡/ä¸‹é™ï¼‰
            targets = self._create_targets(data.iloc[100:]['Close'])  # ç‰¹å¾´é‡ã¨åŒã˜æœŸé–“

            # ãƒ‡ãƒ¼ã‚¿åŒæœŸ
            min_length = min(len(features), len(targets))
            features = features.iloc[:min_length]
            targets = targets[:min_length]

            if len(features) < 50:
                raise ValueError("è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

            # å„ãƒ¢ãƒ‡ãƒ«è¨“ç·´
            if hyperparameters is None:
                hyperparameters = {}
            performances = {}

            for config in self.model_configs:
                try:
                    # ã“ã“ã§ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’å‘¼ã³å‡ºã™
                    optimized_params = hyperparameters.get(config.model_type)

                    performance = await self._train_single_model(
                        features, targets, config, symbol, optimized_params
                    )
                    performances[config.model_type] = performance

                except Exception as e:
                    self.logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´å¤±æ•— {config.model_type.value}: {e}")
                    continue

            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            if len(performances) >= 3:
                ensemble_performance = await self._create_ensemble_model(
                    features, targets, symbol
                )
                performances[ModelType.ENSEMBLE_VOTING] = ensemble_performance

            # çµæœä¿å­˜
            await self._save_model_performances(symbol, performances)

            self.logger.info(f"é«˜åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†: {len(performances)}ãƒ¢ãƒ‡ãƒ«")
            return performances

        except Exception as e:
            self.logger.error(f"é«˜åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _create_targets(self, prices: pd.Series) -> np.ndarray:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ"""

        # ç¿Œæ—¥ã®ä¾¡æ ¼å¤‰å‹•ç‡
        returns = prices.pct_change().shift(-1)  # ç¿Œæ—¥ã®ãƒªã‚¿ãƒ¼ãƒ³

        # é–¾å€¤ä»¥ä¸Šã®ä¸Šæ˜‡ã‚’1ã€ãã‚Œä»¥å¤–ã‚’0
        threshold = 0.005  # 0.5%ä»¥ä¸Šã®ä¸Šæ˜‡
        targets = (returns > threshold).astype(int)

        return targets.values[:-1]  # æœ€å¾Œã®è¦ç´ ï¼ˆNaNï¼‰ã‚’é™¤å»

    async def _train_single_model(self, features: pd.DataFrame, targets: np.ndarray,
                                config: ModelConfig, symbol: str, hyperparameters: Optional[Dict[str, Any]] = None) -> ModelPerformance:
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""

        X = features.copy()
        y = targets.copy()

        # ç‰¹å¾´é¸æŠ
        if config.feature_selection != FeatureSelectionType.NONE:
            X = self._apply_feature_selection(X, y, config)

        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        if config.scaler_type != ScalerType.NONE:
            X = self._apply_scaling(X, config.scaler_type)

        # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
        model = self._create_model(config)

        # è¨“ç·´
        model.fit(X, y)

        # æ€§èƒ½è©•ä¾¡
        predictions = model.predict(X)
        prediction_proba = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else predictions

        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        cv_scores = cross_val_score(model, X, y, cv=5)

        # ç‰¹å¾´é‡è¦åº¦
        feature_importance = {}
        if hasattr(model, 'feature_importances_'):
            for i, importance in enumerate(model.feature_importances_):
                feature_importance[X.columns[i]] = float(importance)

        performance = ModelPerformance(
            model_type=config.model_type,
            accuracy=accuracy_score(y, predictions),
            precision=precision_score(y, predictions, average='weighted', zero_division=0),
            recall=recall_score(y, predictions, average='weighted', zero_division=0),
            f1_score=f1_score(y, predictions, average='weighted', zero_division=0),
            roc_auc=roc_auc_score(y, prediction_proba) if len(np.unique(y)) > 1 else 0.5,
            cross_val_score=float(cv_scores.mean()),
            feature_importance=feature_importance
        )

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_key = f"{symbol}_{config.model_type.value}"
        self.trained_models[model_key] = {
            'model': model,
            'config': config,
            'feature_columns': X.columns.tolist(),
            'performance': performance
        }
        self.save_model(model, model_key) # Save the model

        return performance

    def _apply_feature_selection(self, X: pd.DataFrame, y: np.ndarray,
                                config: ModelConfig) -> pd.DataFrame:
        """ç‰¹å¾´é¸æŠé©ç”¨"""

        k = min(config.feature_selection_k, X.shape[1])

        if config.feature_selection == FeatureSelectionType.K_BEST_F:
            selector = SelectKBest(score_func=f_classif, k=k)
        elif config.feature_selection == FeatureSelectionType.K_BEST_MUTUAL:
            selector = SelectKBest(score_func=mutual_info_classif, k=k)
        else:
            return X

        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()]

        return pd.DataFrame(X_selected, columns=selected_features, index=X.index)

    def _apply_scaling(self, X: pd.DataFrame, scaler_type: ScalerType) -> pd.DataFrame:
        """ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é©ç”¨"""

        if scaler_type == ScalerType.STANDARD:
            scaler = StandardScaler()
        elif scaler_type == ScalerType.ROBUST:
            scaler = RobustScaler()
        elif scaler_type == ScalerType.MINMAX:
            scaler = MinMaxScaler()
        else:
            return X

        X_scaled = scaler.fit_transform(X)
        return pd.DataFrame(X_scaled, columns=X.columns, index=X.index)

    def _create_model(self, config: ModelConfig):
        """ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""

        params = config.hyperparameters.copy()
        if hyperparameters:
            params.update(hyperparameters)

        if config.model_type == ModelType.RANDOM_FOREST:
            return RandomForestClassifier(**params)
        elif config.model_type == ModelType.GRADIENT_BOOSTING:
            return GradientBoostingClassifier(**params)
        elif config.model_type == ModelType.LOGISTIC_REGRESSION:
            return LogisticRegression(**params)
        elif config.model_type == ModelType.SVM:
            return SVC(**params)
        elif config.model_type == ModelType.NEURAL_NETWORK:
            return MLPClassifier(**params)
        else:
            raise ValueError(f"æœªã‚µãƒãƒ¼ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {config.model_type}")

    async def _create_ensemble_model(self, features: pd.DataFrame, targets: np.ndarray,
                                   symbol: str) -> ModelPerformance:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ä½œæˆ"""

        # æœ€é«˜æ€§èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        best_models = []
        for model_key, model_data in self.trained_models.items():
            if symbol in model_key and model_data['performance'].accuracy > 0.55:
                best_models.append((
                    model_data['config'].model_type.value,
                    model_data['model']
                ))

        if len(best_models) < 2:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨
            best_models = [
                (model_data['config'].model_type.value, model_data['model'])
                for model_key, model_data in self.trained_models.items()
                if symbol in model_key
            ]

        # Voting Classifierä½œæˆ
        voting_classifier = VotingClassifier(
            estimators=best_models,
            voting='soft' if all(hasattr(model, 'predict_proba') for _, model in best_models) else 'hard'
        )

        # ç‰¹å¾´é¸æŠã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆRandom Forestã®è¨­å®šã‚’ä½¿ç”¨ï¼‰
        X = features.copy()
        y = targets.copy()

        # æœ€ã‚‚åŸºæœ¬çš„ãªå‰å‡¦ç†ã®ã¿é©ç”¨
        k = min(50, X.shape[1])
        selector = SelectKBest(score_func=f_classif, k=k)
        X_selected = selector.fit_transform(X, y)
        selected_features = X.columns[selector.get_support()]
        X = pd.DataFrame(X_selected, columns=selected_features, index=X.index)

        # è¨“ç·´
        voting_classifier.fit(X, y)

        # æ€§èƒ½è©•ä¾¡
        predictions = voting_classifier.predict(X)
        prediction_proba = voting_classifier.predict_proba(X)[:, 1] if hasattr(voting_classifier, 'predict_proba') else predictions

        cv_scores = cross_val_score(voting_classifier, X, y, cv=3)  # é«˜é€ŸåŒ–ã®ãŸã‚CV=3

        performance = ModelPerformance(
            model_type=ModelType.ENSEMBLE_VOTING,
            accuracy=accuracy_score(y, predictions),
            precision=precision_score(y, predictions, average='weighted', zero_division=0),
            recall=recall_score(y, predictions, average='weighted', zero_division=0),
            f1_score=f1_score(y, predictions, average='weighted', zero_division=0),
            roc_auc=roc_auc_score(y, prediction_proba) if len(np.unique(y)) > 1 else 0.5,
            cross_val_score=float(cv_scores.mean())
        )

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_key = f"{symbol}_{ModelType.ENSEMBLE_VOTING.value}"
        self.trained_models[model_key] = {
            'model': voting_classifier,
            'config': None,
            'feature_columns': X.columns.tolist(),
            'performance': performance
        }
        self.save_model(voting_classifier, model_key) # Save the model

        return performance

    async def _save_model_performances(self, symbol: str,
                                     performances: Dict[ModelType, ModelPerformance]):
        """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ä¿å­˜"""

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()

                # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
                cursor.execute('''
                    CREATE TABLE IF NOT EXISTS advanced_model_performances (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        symbol TEXT NOT NULL,
                        model_type TEXT NOT NULL,
                        accuracy REAL,
                        precision_score REAL,
                        recall_score REAL,
                        f1_score REAL,
                        roc_auc REAL,
                        cross_val_score REAL,
                        feature_importance TEXT,
                        created_at TEXT,
                        UNIQUE(symbol, model_type)
                    )
                ''')

                # ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥
                for model_type, performance in performances.items():
                    cursor.execute('''
                        INSERT OR REPLACE INTO advanced_model_performances
                        (symbol, model_type, accuracy, precision_score, recall_score,
                         f1_score, roc_auc, cross_val_score, feature_importance, created_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        symbol,
                        model_type.value,
                        performance.accuracy,
                        performance.precision,
                        performance.recall,
                        performance.f1_score,
                        performance.roc_auc,
                        performance.cross_val_score,
                        json.dumps(performance.feature_importance),
                        datetime.now().isoformat()
                    ))

                conn.commit()

        except Exception as e:
            self.logger.error(f"æ€§èƒ½ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def save_model(self, model, model_key: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
        model_path = self.db_path.parent / "trained_advanced_models" / f"{model_key}.joblib"
        model_path.parent.mkdir(exist_ok=True)
        try:
            joblib.dump(model, model_path)
            self.logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")
        except Exception as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ {model_key}: {e}")

    def load_model(self, model_key: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
        model_path = self.db_path.parent / "trained_advanced_models" / f"{model_key}.joblib"
        if model_path.exists():
            try:
                model = joblib.load(model_path)
                self.logger.info(f"ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {model_path}")
                return model
            except Exception as e:
                self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ {model_key}: {e}")
        return None

    async def predict_with_advanced_models(self, symbol: str) -> PredictionResult:
        """é«˜åº¦ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äºˆæ¸¬"""

        try:
            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿å–å¾—
            from real_data_provider_v2 import real_data_provider
            data = await real_data_provider.get_stock_data(symbol, "2mo")

            # ç‰¹å¾´é‡ä½œæˆ
            features = await self.feature_engineering.create_advanced_features(data)
            latest_features = features.iloc[-1:].copy()

            # å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
            model_predictions = {}
            confidences = []

            for model_key, model_data in self.trained_models.items():
                if symbol in model_key:
                    try:
                        model = model_data['model']
                        feature_columns = model_data['feature_columns']

                        # ç‰¹å¾´é‡ã‚’åˆã‚ã›ã‚‹
                        X = latest_features[feature_columns].copy()

                        # äºˆæ¸¬
                        prediction = model.predict(X)[0]
                        confidence = model.predict_proba(X)[0].max() if hasattr(model, 'predict_proba') else 0.7

                        model_type = model_data['config'].model_type.value if model_data['config'] else ModelType.ENSEMBLE_VOTING.value
                        model_predictions[model_type] = int(prediction)
                        confidences.append(confidence)

                    except Exception as e:
                        self.logger.warning(f"äºˆæ¸¬å¤±æ•— {model_key}: {e}")
                        continue

            # æœ€çµ‚äºˆæ¸¬ï¼ˆå¤šæ•°æ±ºï¼‰
            if model_predictions:
                final_prediction = int(np.mean(list(model_predictions.values())) >= 0.5)
                final_confidence = float(np.mean(confidences))
            else:
                final_prediction = 1  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬
                final_confidence = 0.5

            # ç‰¹å¾´é‡å€¤
            feature_values = {col: float(val) for col, val in latest_features.iloc[0].items()}

            return PredictionResult(
                symbol=symbol,
                prediction=final_prediction,
                confidence=final_confidence,
                model_consensus=model_predictions,
                feature_values=feature_values,
                timestamp=datetime.now()
            )

        except Exception as e:
            self.logger.error(f"äºˆæ¸¬ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬ã‚’è¿”ã™
            return PredictionResult(
                symbol=symbol,
                prediction=1,
                confidence=0.5,
                model_consensus={},
                feature_values={},
                timestamp=datetime.now()
            )

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
advanced_ml_system = AdvancedMLPredictionSystem()

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
async def test_advanced_ml_system():
    """é«˜åº¦MLç³»ç»Ÿæµ‹è¯•"""

    print("=== é«˜åº¦æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")

    test_symbol = "7203"

    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    print(f"\nğŸ¤– {test_symbol} é«˜åº¦ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
    performances = await advanced_ml_system.train_advanced_models(test_symbol, "4mo")

    print(f"\nğŸ“Š è¨“ç·´çµæœ:")
    for model_type, performance in performances.items():
        print(f"  {model_type.value}: ç²¾åº¦{performance.accuracy:.3f} F1{performance.f1_score:.3f}")

    # äºˆæ¸¬å®Ÿè¡Œ
    print(f"\nğŸ”® äºˆæ¸¬å®Ÿè¡Œ...")
    prediction = await advanced_ml_system.predict_with_advanced_models(test_symbol)

    print(f"\nğŸ“ˆ äºˆæ¸¬çµæœ:")
    print(f"  äºˆæ¸¬: {'ä¸Šæ˜‡' if prediction.prediction else 'ä¸‹é™'}")
    print(f"  ä¿¡é ¼åº¦: {prediction.confidence:.3f}")
    print(f"  ãƒ¢ãƒ‡ãƒ«åˆæ„: {prediction.model_consensus}")

    print(f"\nâœ… é«˜åº¦æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    asyncio.run(test_advanced_ml_system())