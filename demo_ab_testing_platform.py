#!/usr/bin/env python3
"""
A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆçµ±åˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

Issue #733å¯¾å¿œ: MLãƒ¢ãƒ‡ãƒ«A/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å®Œå…¨ãƒ‡ãƒ¢
å®Ÿéš›ã®æ ªå¼å–å¼•ã‚·ã‚¹ãƒ†ãƒ ã§ã®ãƒ¢ãƒ‡ãƒ«æ”¹å–„ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
import sys

import numpy as np

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.day_trade.ml.ml_experimentation_platform import (
    MLExperimentationPlatform, ModelVersion
)
from src.day_trade.utils.logging_config import get_context_logger

logger = get_context_logger(__name__)


class StockPredictionSimulator:
    """æ ªä¾¡äºˆæ¸¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿"""

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.symbols = ["7203", "8306", "9984", "6758", "4689", "1605", "2914", "8058", "3382", "9437"]
        self.base_prices = {symbol: 1000 + hash(symbol) % 500 for symbol in self.symbols}
        self.market_volatility = 0.02  # 2%ã®ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£

        # ãƒ¢ãƒ‡ãƒ«æ€§èƒ½è¨­å®š
        self.model_performance = {
            "RandomForest_v1": {"accuracy": 0.75, "bias": 0.0, "latency": 65.0},
            "RandomForest_v2": {"accuracy": 0.78, "bias": 0.5, "latency": 55.0},
            "XGBoost_v1": {"accuracy": 0.80, "bias": 1.0, "latency": 45.0},
            "LSTM_v1": {"accuracy": 0.82, "bias": 1.5, "latency": 85.0},
        }

    def generate_market_data(self, symbol: str, timestamp: datetime) -> dict:
        """å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        base_price = self.base_prices[symbol]

        # æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ä¾¡æ ¼å¤‰å‹•
        time_factor = np.sin((timestamp.hour + timestamp.minute / 60) * np.pi / 12)
        daily_trend = np.random.normal(0, self.market_volatility)

        current_price = base_price * (1 + daily_trend + time_factor * 0.01)

        return {
            "symbol": symbol,
            "current_price": current_price,
            "timestamp": timestamp,
            "volatility": abs(daily_trend),
            "volume": np.random.uniform(10000, 100000)
        }

    def simulate_model_prediction(self, model_name: str, market_data: dict) -> dict:
        """ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        if model_name not in self.model_performance:
            model_name = "RandomForest_v1"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        perf = self.model_performance[model_name]
        current_price = market_data["current_price"]

        # å®Ÿéš›ã®ä¾¡æ ¼å¤‰å‹•ï¼ˆæœªæ¥ã®å€¤ï¼‰
        true_change = np.random.normal(0, 0.01)  # 1%ã®æ¨™æº–çš„ãªå¤‰å‹•
        actual_future_price = current_price * (1 + true_change)

        # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ï¼ˆç²¾åº¦ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’è€ƒæ…®ï¼‰
        prediction_noise = np.random.normal(0, 1 - perf["accuracy"])
        prediction_change = true_change + perf["bias"] * 0.001 + prediction_noise * 0.02
        predicted_price = current_price * (1 + prediction_change)

        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        latency = np.random.normal(perf["latency"], 10)

        return {
            "prediction": predicted_price,
            "actual_value": actual_future_price,
            "latency_ms": max(10, latency),
            "confidence": perf["accuracy"] + np.random.uniform(-0.05, 0.05),
            "model_name": model_name
        }


async def run_comprehensive_ab_test_demo():
    """åŒ…æ‹¬çš„A/Bãƒ†ã‚¹ãƒˆãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""

    print("=" * 80)
    print("ğŸš€ ML A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  åŒ…æ‹¬çš„ãƒ‡ãƒ¢")
    print("   Issue #733: MLãƒ¢ãƒ‡ãƒ«ç”¨A/Bãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    print("=" * 80)

    # 1. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆæœŸåŒ–
    print("\nğŸ“‹ 1. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆæœŸåŒ–")
    print("-" * 50)

    platform = MLExperimentationPlatform("data/ab_testing_demo")
    simulator = StockPredictionSimulator()

    print("âœ… MLå®Ÿé¨“ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ åˆæœŸåŒ–å®Œäº†")
    print("âœ… æ ªä¾¡äºˆæ¸¬ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–å®Œäº†")

    # 2. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã®è¨­å®š
    print("\nğŸ§ª 2. ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã®è¨­å®š")
    print("-" * 50)

    experiments = [
        {
            "name": "RandomForest v1 vs v2 Performance Comparison",
            "control_model": {"model_type": "RandomForest_v1", "version": "1.0", "n_estimators": 100},
            "test_model": {"model_type": "RandomForest_v2", "version": "2.0", "n_estimators": 120},
            "traffic_split": 0.3,
            "duration_hours": 0.1  # ãƒ‡ãƒ¢ç”¨ã«çŸ­æ™‚é–“
        },
        {
            "name": "RandomForest vs XGBoost Model Comparison",
            "control_model": {"model_type": "RandomForest_v2", "version": "2.0", "n_estimators": 120},
            "test_model": {"model_type": "XGBoost_v1", "version": "1.0", "max_depth": 6},
            "traffic_split": 0.4,
            "duration_hours": 0.1
        },
        {
            "name": "Traditional ML vs Deep Learning Comparison",
            "control_model": {"model_type": "XGBoost_v1", "version": "1.0", "max_depth": 6},
            "test_model": {"model_type": "LSTM_v1", "version": "1.0", "hidden_units": 128},
            "traffic_split": 0.2,  # ä¿å®ˆçš„ãªãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯é…åˆ†
            "duration_hours": 0.1
        }
    ]

    experiment_ids = []

    for exp_config in experiments:
        experiment_id = await platform.create_model_comparison_experiment(
            experiment_name=exp_config["name"],
            control_model=exp_config["control_model"],
            test_model=exp_config["test_model"],
            traffic_split=exp_config["traffic_split"],
            experiment_duration_hours=exp_config["duration_hours"]
        )

        if experiment_id:
            experiment_ids.append(experiment_id)
            print(f"âœ… å®Ÿé¨“ä½œæˆæˆåŠŸ: {exp_config['name'][:50]}...")
            print(f"   - å®Ÿé¨“ID: {experiment_id}")
            print(f"   - ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯åˆ†å‰²: {exp_config['traffic_split']*100:.1f}%")
        else:
            print(f"âŒ å®Ÿé¨“ä½œæˆå¤±æ•—: {exp_config['name']}")

    print(f"\nğŸ“Š ä½œæˆã•ã‚ŒãŸå®Ÿé¨“æ•°: {len(experiment_ids)}")

    # 3. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ»è¨˜éŒ²
    print("\nğŸ“ˆ 3. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆãƒ»è¨˜éŒ²")
    print("-" * 50)

    prediction_count = 0
    start_time = time.time()

    # å„å®Ÿé¨“ã§äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    for i, experiment_id in enumerate(experiment_ids):
        exp_config = experiments[i]
        control_model = exp_config["control_model"]["model_type"]
        test_model = exp_config["test_model"]["model_type"]

        print(f"\nğŸ”¬ å®Ÿé¨“ {i+1}: {control_model} vs {test_model}")

        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆäºˆæ¸¬ã®å®Ÿè¡Œ
        for prediction_round in range(100):  # å„å®Ÿé¨“100å›ã®äºˆæ¸¬
            symbol = np.random.choice(simulator.symbols)
            current_time = datetime.now() + timedelta(minutes=prediction_round)

            # å¸‚å ´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
            market_data = simulator.generate_market_data(symbol, current_time)

            # å®Ÿé¨“ã‚°ãƒ«ãƒ¼ãƒ—ã®å‰²ã‚Šå½“ã¦ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«ä¸€æ™‚çš„ãªäºˆæ¸¬
            temp_prediction = simulator.simulate_model_prediction(control_model, market_data)

            # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚’é€šã˜ã¦äºˆæ¸¬ã‚’è¨˜éŒ²
            success = await platform.record_model_prediction(
                experiment_id=experiment_id,
                symbol=symbol,
                prediction=temp_prediction["prediction"],
                actual_value=temp_prediction["actual_value"],
                latency_ms=temp_prediction["latency_ms"],
                metadata={
                    "confidence": temp_prediction["confidence"],
                    "model_name": temp_prediction["model_name"],
                    "market_volatility": market_data["volatility"],
                    "volume": market_data["volume"]
                }
            )

            if success:
                prediction_count += 1

            # é€²æ—è¡¨ç¤ºï¼ˆ10å›ã”ã¨ï¼‰
            if prediction_round % 25 == 0 and prediction_round > 0:
                print(f"   ğŸ“Š {prediction_round} äºˆæ¸¬å®Œäº†...")

        print(f"   âœ… å®Ÿé¨“ {i+1} ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: 100 äºˆæ¸¬")

    elapsed_time = time.time() - start_time
    print(f"\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚µãƒãƒªãƒ¼:")
    print(f"   - ç·äºˆæ¸¬å›æ•°: {prediction_count}")
    print(f"   - å®Ÿè¡Œæ™‚é–“: {elapsed_time:.2f}ç§’")
    print(f"   - å¹³å‡äºˆæ¸¬é€Ÿåº¦: {prediction_count/elapsed_time:.1f} äºˆæ¸¬/ç§’")

    # 4. å®Ÿé¨“çµæœã®åˆ†æ
    print("\nğŸ” 4. å®Ÿé¨“çµæœã®åˆ†æ")
    print("-" * 50)

    analysis_results = []

    for i, experiment_id in enumerate(experiment_ids):
        exp_config = experiments[i]

        print(f"\nğŸ“ˆ å®Ÿé¨“åˆ†æ {i+1}: {exp_config['name'][:60]}...")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã®å®Ÿè¡Œ
        performance_comparison = await platform.analyze_experiment_performance(experiment_id)

        if performance_comparison:
            analysis_results.append(performance_comparison)

            print(f"   âœ… åˆ†æå®Œäº†")
            print(f"   ğŸ“Š æ¨å¥¨äº‹é …: {performance_comparison.recommendation}")
            print(f"   ğŸ¯ ä¿¡é ¼åº¦: {performance_comparison.confidence_score:.3f}")

            # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            if performance_comparison.metric_comparisons:
                print("   ğŸ“‹ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ:")
                for metric, comparison in performance_comparison.metric_comparisons.items():
                    change_pct = comparison.get("improvement_percentage", 0)
                    if abs(change_pct) > 1:  # 1%ä»¥ä¸Šã®å¤‰åŒ–ã®ã¿è¡¨ç¤º
                        direction = "â†—ï¸" if change_pct > 0 else "â†˜ï¸"
                        print(f"      {metric}: {direction} {change_pct:+.1f}%")
        else:
            print(f"   âŒ åˆ†æå¤±æ•—")

    print(f"\nğŸ“Š åˆ†æå®Œäº†: {len(analysis_results)} / {len(experiment_ids)} å®Ÿé¨“")

    # 5. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ¨å¥¨äº‹é …
    print("\nğŸš€ 5. ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ¨å¥¨äº‹é …")
    print("-" * 50)

    deployment_decisions = []

    for i, experiment_id in enumerate(experiment_ids):
        exp_config = experiments[i]

        print(f"\nğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåˆ¤å®š {i+1}: {exp_config['name'][:50]}...")

        # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨­å®š
        deployment_context = {
            'daily_request_volume': np.random.uniform(10000, 500000),
            'recent_deployments_count': np.random.randint(0, 3),
            'model_complexity_score': 0.3 + i * 0.2  # å®Ÿé¨“é †ã§è¤‡é›‘ã•ãŒå¢—åŠ 
        }

        # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ¨å¥¨ã®å–å¾—
        deployment_decision = await platform.get_deployment_recommendation(
            experiment_id, deployment_context
        )

        if deployment_decision:
            deployment_decisions.append(deployment_decision)

            print(f"   ğŸ“‹ åˆ¤å®šçµæœ: {'ğŸŸ¢ ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨' if deployment_decision.deploy_recommended else 'ğŸŸ¡ ãƒ‡ãƒ—ãƒ­ã‚¤è¦‹é€ã‚Š'}")
            print(f"   ğŸ¯ ä¿¡é ¼åº¦: {deployment_decision.confidence_level:.3f}")
            print(f"   ğŸš€ æ¨å¥¨æˆ¦ç•¥: {deployment_decision.deployment_strategy.value}")
            print(f"   ğŸ“ åˆ¤å®šç†ç”±:")
            for reason in deployment_decision.reasons[:3]:  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º
                print(f"      â€¢ {reason}")

            # ãƒªã‚¹ã‚¯è©•ä¾¡ã®è¡¨ç¤º
            overall_risk = deployment_decision.risk_assessment.get('overall_risk', 0.5)
            risk_level = "ğŸŸ¢ ä½" if overall_risk < 0.3 else "ğŸŸ¡ ä¸­" if overall_risk < 0.7 else "ğŸ”´ é«˜"
            print(f"   âš ï¸  å…¨ä½“ãƒªã‚¹ã‚¯: {risk_level} ({overall_risk:.3f})")
        else:
            print(f"   âŒ åˆ¤å®šå¤±æ•—")

    # 6. è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    print("\nğŸ¤– 6. è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ")
    print("-" * 50)

    deployed_count = 0

    for i, decision in enumerate(deployment_decisions):
        if decision.deploy_recommended:
            exp_config = experiments[i]
            test_model = exp_config["test_model"]

            print(f"\nğŸš€ è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ {deployed_count + 1}:")
            print(f"   ğŸ“¦ ãƒ¢ãƒ‡ãƒ«: {test_model['model_type']} v{test_model.get('version', '1.0')}")

            # ãƒ‡ãƒ—ãƒ­ã‚¤ç”¨ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ä½œæˆ
            model_version = ModelVersion(
                version_id=f"{test_model['model_type']}_v{test_model.get('version', '1.0')}",
                name=f"{test_model['model_type']} Version {test_model.get('version', '1.0')}",
                description=f"A/Bãƒ†ã‚¹ãƒˆã§é¸æŠã•ã‚ŒãŸ{test_model['model_type']}ãƒ¢ãƒ‡ãƒ«",
                model_path=f"/models/{test_model['model_type'].lower()}_v{test_model.get('version', '1.0')}.joblib",
                config=test_model,
                performance_metrics={
                    "accuracy": 0.8 + np.random.uniform(-0.05, 0.05),
                    "latency_ms": 50 + np.random.uniform(-15, 15),
                    "throughput": 1000 + np.random.uniform(-200, 200)
                }
            )

            # è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®Ÿè¡Œ
            deployment_id = await platform.execute_auto_deployment(decision, model_version)

            if deployment_id:
                print(f"   âœ… ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆé–‹å§‹: {deployment_id}")
                deployed_count += 1

                # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆçŠ¶æ…‹ã‚’ç¢ºèª
                await asyncio.sleep(1)

                deployment_state = platform.deployment_manager.get_deployment_status(deployment_id)
                if deployment_state:
                    print(f"   ğŸ“Š ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆçŠ¶æ…‹: {deployment_state.status.value}")
                    print(f"   ğŸ¯ ç¾åœ¨ã®ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯: {deployment_state.current_traffic_percentage*100:.1f}%")
            else:
                print(f"   âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå¤±æ•—")
        else:
            print(f"\nâ¸ï¸  ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¹ã‚­ãƒƒãƒ— {i+1}: æ¨å¥¨ã•ã‚Œã¦ã„ã¾ã›ã‚“")

    print(f"\nğŸ“Š ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚µãƒãƒªãƒ¼: {deployed_count} / {len(deployment_decisions)} ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œ")

    # 7. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çŠ¶æ…‹ã®æœ€çµ‚ç¢ºèª
    print("\nğŸ“‹ 7. ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çŠ¶æ…‹ã‚µãƒãƒªãƒ¼")
    print("-" * 50)

    active_experiments = platform.get_active_experiments()
    active_deployments = platform.get_active_deployments()
    experiment_pairs = platform.get_experiment_deployment_pairs()

    print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"   ğŸ§ª ä½œæˆå®Ÿé¨“æ•°: {len(experiment_ids)}")
    print(f"   ğŸ“ˆ åˆ†æå®Œäº†æ•°: {len(analysis_results)}")
    print(f"   ğŸ¯ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåˆ¤å®šæ•°: {len(deployment_decisions)}")
    print(f"   ğŸš€ å®Ÿè¡Œãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ•°: {deployed_count}")
    print(f"   ğŸ”— å®Ÿé¨“-ãƒ‡ãƒ—ãƒ­ã‚¤ãƒšã‚¢æ•°: {len(experiment_pairs)}")

    # æˆåŠŸç‡ã®è¨ˆç®—
    experiment_success_rate = len(experiment_ids) / len(experiments) * 100
    analysis_success_rate = len(analysis_results) / len(experiment_ids) * 100 if experiment_ids else 0
    deployment_rate = deployed_count / len(deployment_decisions) * 100 if deployment_decisions else 0

    print(f"\nâœ… æˆåŠŸç‡:")
    print(f"   ğŸ§ª å®Ÿé¨“ä½œæˆæˆåŠŸç‡: {experiment_success_rate:.1f}%")
    print(f"   ğŸ“ˆ åˆ†ææˆåŠŸç‡: {analysis_success_rate:.1f}%")
    print(f"   ğŸš€ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®Ÿè¡Œç‡: {deployment_rate:.1f}%")

    # æ¨å¥¨äº‹é …ã®è¦ç´„
    print(f"\nğŸ¯ æ¨å¥¨äº‹é …ã‚µãƒãƒªãƒ¼:")
    deploy_recommended_count = sum(1 for d in deployment_decisions if d.deploy_recommended)
    print(f"   âœ… ãƒ‡ãƒ—ãƒ­ã‚¤æ¨å¥¨: {deploy_recommended_count} / {len(deployment_decisions)}")

    if deployment_decisions:
        avg_confidence = np.mean([d.confidence_level for d in deployment_decisions])
        print(f"   ğŸ“Š å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")

        strategy_counts = {}
        for d in deployment_decisions:
            strategy = d.deployment_strategy.value
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1

        print(f"   ğŸš€ æ¨å¥¨æˆ¦ç•¥åˆ†å¸ƒ:")
        for strategy, count in strategy_counts.items():
            print(f"      {strategy}: {count} å›")

    print("\n" + "=" * 80)
    print("ğŸ‰ ML A/Bãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  ãƒ‡ãƒ¢å®Œäº†!")
    print("   Issue #733å®Ÿè£…ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹MLãƒ¢ãƒ‡ãƒ«æ”¹å–„ãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸã€‚")
    print("=" * 80)


async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        await run_comprehensive_ab_test_demo()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ãƒ‡ãƒ¢ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\n\nâŒ ãƒ‡ãƒ¢å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())